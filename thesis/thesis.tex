% Copyright (C) 2014-2020 by Thomas Auzinger <thomas@auzinger.name>

\documentclass[draft,final]{vutinfth} % Remove option 'final' to obtain debug information.

% Load packages to allow in- and output of non-ASCII characters.
\usepackage{lmodern}        % Use an extension of the original Computer Modern font to minimize the use of bitmapped letters.
\usepackage[T1]{fontenc}    % Determines font encoding of the output. Font packages have to be included before this line.
\usepackage[utf8]{inputenc} % Determines encoding of the input. All input files have to use UTF8 encoding.

% Extended LaTeX functionality is enables by including packages with \usepackage{...}.
\usepackage{amsmath}    % Extended typesetting of mathematical expression.
\usepackage{amssymb}    % Provides a multitude of mathematical symbols.
\usepackage{mathtools}  % Further extensions of mathematical typesetting.
\usepackage{microtype}  % Small-scale typographic enhancements.
\usepackage[inline]{enumitem} % User control over the layout of lists (itemize, enumerate, description).
\usepackage{multirow}   % Allows table elements to span several rows.
\usepackage{booktabs}   % Improves the typesettings of tables.
\usepackage{subcaption} % Allows the use of subfigures and enables their referencing.
\usepackage[ruled,linesnumbered,algochapter]{algorithm2e} % Enables the writing of pseudo code.
\usepackage[usenames,dvipsnames,table]{xcolor} % Allows the definition and use of colors. This package has to be included before tikz.
\usepackage{nag}       % Issues warnings when best practices in writing LaTeX documents are violated.
\usepackage{todonotes} % Provides tooltip-like todo notes.
\usepackage{hyperref}  % Enables cross linking in the electronic document version. This package has to be included second to last.
\usepackage[acronym,toc]{glossaries} % Enables the generation of glossaries and lists fo acronyms. This package has to be included last.
\usepackage{float}
\usepackage{amsfonts,amsthm, graphicx, trfsigns, physics, xparse, mleftright}

% Define convenience functions to use the author name and the thesis title in the PDF document properties.
\newcommand{\authorname}{Hannes Brantner} % The author name without titles.
\newcommand{\thesistitle}{Comparing Machine Learning Models using Long-Term Dependency and Physical System Benchmarks} % The title of the thesis. The English version should be used, if it exists.

% Set PDF document properties
\hypersetup{
    pdfpagelayout   = TwoPageRight,           % How the document is shown in PDF viewers (optional).
    linkbordercolor = {Melon},                % The color of the borders of boxes around crosslinks (optional).
    pdfauthor       = {\authorname},          % The author's name in the document properties (optional).
    pdftitle        = {\thesistitle},         % The document's title in the document properties (optional).
    pdfsubject      = {Subject},              % The document's subject in the document properties (optional).
    pdfkeywords     = {a, list, of, keywords} % The document's keywords in the document properties (optional).
}

\setpnumwidth{2.5em}        % Avoid overfull hboxes in the table of contents (see memoir manual).
\setsecnumdepth{subsection} % Enumerate subsections.

\nonzeroparskip             % Create space between paragraphs (optional).
\setlength{\parindent}{0pt} % Remove paragraph identation (optional).

\makeindex      % Use an optional index.
\makeglossaries % Use an optional glossary.
%\glstocfalse   % Remove the glossaries from the table of contents.

% Set persons with 4 arguments:
%  {title before name}{name}{title after name}{gender}
%  where both titles are optional (i.e. can be given as empty brackets {}).
\setauthor{}{\authorname}{BSc}{male}
\setadvisor{Dipl.-Ing. Dr.rer.nat.}{Radu Grosu}{BSc}{male}

% For bachelor and master theses:
\setfirstassistant{Dipl.-Ing. Dr.}{Ramin Hasani}{BSc}{male}
%\setsecondassistant{Pretitle}{Forename Surname}{Posttitle}{male}
%\setthirdassistant{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations:
%\setfirstreviewer{Pretitle}{Forename Surname}{Posttitle}{male}
%\setsecondreviewer{Pretitle}{Forename Surname}{Posttitle}{male}

% For dissertations at the PhD School and optionally for dissertations:
%\setsecondadvisor{Pretitle}{Forename Surname}{Posttitle}{male} % Comment to remove.

% Required data.
\setregnumber{01614466}
\setdate{31}{04}{2021} % Set date with 3 arguments: {day}{month}{year}.
\settitle{\thesistitle}{\thesistitle} % Sets English and German version of the title (both can be English or German). If your title contains commas, enclose it with additional curvy brackets (i.e., {{your title}}) or define it as a macro as done with \thesistitle.
%\setsubtitle{Optional Subtitle of the Thesis}{Optionaler Untertitel der Arbeit} % Sets English and German version of the subtitle (both can be English or German).

% Select the thesis type: bachelor / master / doctor / phd-school.
% Bachelor:
%\setthesis{bachelor}
%
% Master:
\setthesis{master}
\setmasterdegree{dipl.} % dipl. / rer.nat. / rer.soc.oec. / master
%
% Doctor:
%\setthesis{doctor}
%\setdoctordegree{rer.soc.oec.}% rer.nat. / techn. / rer.soc.oec.
%
% Doctor at the PhD School
%\setthesis{phd-school} % Deactivate non-English title pages (see below)

% For bachelor and master:
\setcurriculum{Computer Engineering}{Technische Informatik} % Sets the English and German name of the curriculum.

% For dissertations at the PhD School:
%\setfirstreviewerdata{Affiliation, Country}
%\setsecondreviewerdata{Affiliation, Country}


\begin{document}

    \frontmatter % Switches to roman numbering.
% The structure of the thesis has to conform to the guidelines at
%  https://informatics.tuwien.ac.at/study-services

%\addtitlepage{naustrian} % German title page (not for dissertations at the PhD School).
    \addtitlepage{english} % English title page.

    \AddStatementPage

%\begin{danksagung*}
%\todo{Ihr Text hier.}
%\end{danksagung*}

    \begin{acknowledgements*}
        At first, I have to thank Ramin for providing me great support throughout my work on the thesis.
        He cared about me and was always pointing me to state-of-the-art literature, as he wanted to push me forward.
        I also have to thank Prof. Grosu for participating in numerous online meetings and for sharing his in-depth knowledge in the machine learning domain.
        Furthermore, I want to thank Mathias Lechner for giving me first-class support on questions I had regarding various machine learning models.
        I have to point out that he was always willing to help me and provided his responses incredibly fast.
        Last but not least, I have to thank my parents for providing me with mental and financial support throughout my whole study journey.
    \end{acknowledgements*}

%\begin{kurzfassung}
%\todo{Ihr Text hier.}
%\end{kurzfassung}

    \begin{abstract}
        The diversity of machine learning models has rapidly increased in recent years as research in the machine learning domain flourishes.
        This thesis tries to give an overview of machine learning models that are capable of dealing with regularly sampled time-series data without specifying a given history length that should be taken into account by the model.
        Therefore, all models presented in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
        Furthermore, new machine learning models are introduced that try to improve on the given transformer and unitary recurrent neural network \cite{EfficientUnitaryRNNs} architecture.
        After the introduction of all models, they are all benchmarked against five benchmarks and compared thoroughly.
        These benchmarks try to determine the model's capabilities to capture long-term dependencies and the ability to model physical systems.
        Moreover, a time-continuous memory cell is introduced that is capable of storing a data bit over a large number of time steps without losing the stored information.
        This memory cell is built using the LTC network \cite{LTCNetworks} architecture.
    \end{abstract}

% Select the language of the thesis, e.g., english or naustrian.
    \selectlanguage{english}

% Add a table of contents (toc).
    \tableofcontents % Starred version, i.e., \tableofcontents*, removes the self-entry.

% Switch to arabic numbering and start the enumeration of chapters in the table of content.
    \mainmatter

    \chapter{Introduction}

    \section{Machine Learning Terms}
    A machine learning model is a mathematical parametrized function that gets an input and produces an output.
    For example, the machine learning model GPT-3 proposed in \cite{GPT-3} has 175 billion scalar parameters.
    This thesis will use imitation learning to optimally set the parameters of machine learning models.
    This means that for each input, there is an associative expected output provided that the model should return by applying its function to the input.
    Of course when the model's function is applied to the input with the initial state of the model's parameters, the returned model output will differ from the desired output in almost all cases.
    The measure that quantifies this error between model output and expected output is called a loss function and has a scalar return value.
    A sample loss function can be constructed as easy as computing the mean of all squared errors between the model output and the expected output.
    The model output is also often denoted as the prediction of the model.
    For each input sample, the loss function describes the error the model makes by applying its function and this error is only dependent on the parameters of the model.
    In the general case, a computer scientist wants to find the global minimum of that function with respect to all machine learning model parameters.
    As this is a problem that cannot be solved analytically in most cases, it is approximated by using gradient descent \cite{GradientDescent}.
    This method incrementally changes each parameter depending on the gradient of the loss function with respect to each parameter in lock step.
    By denoting the loss function with $L$, the learning rate with $\alpha$, the old whole parameter set with $p$, the old single scalar parameter with $p_i$ and the new single scalar parameter with $p_i'$, the formula to update the individual parameters $p_i$ in a single gradient descent step can be given as follows \cite{GradientDescent}:
    \begin{align} 
        \label{gradient_descent_update}
        \forall p_i:~p_i' &= p_i - \alpha * \frac{\partial{L}}{\partial{p_i}}(p)
    \end{align}
    It is essential to note that the model as well as the loss measure must be deterministic functions for the gradient to exist.
    This update rule ensures that if the loss function increases with increasing $p_i$, therefore if the computed gradient of the loss function is larger than zero, a decrease of the parameter will happen that leads to a decreasing loss function result.
    The opposite case holds as well and this is why there is a minus sign in \ref{gradient_descent_update}.
    The learning rate $\alpha$ determines how large in magnitude the update to the parameters should be at each gradient descent step.
    A too small learning rate will lead to slow convergence, a too large learning rate will lead to divergence.
    Therefore, a too large learning rate is far more dangerous than a too small one.
    Convergence means that the parameter updates have led to a local minimum of the loss function.
    There are no guarantees that this is the global minimum. Divergence means that the loss function diverges towards infinity.
    A local minimum or convergence can be reached by applying the gradient descent update rule to as many inputs as needed to set the loss function derivative to nearly zero. 
    As at the differentiation of the loss function, which can be represented as a computational graph with lots of nested functions, with respect to the individual model parameters involves lots of applications of the chain rule, the machine learning term for repeatedly applying the chain rule is backpropagation.
    If these nested functions correspond to applying the same machine learning model function across multiple input time steps as done in recurrent neural networks, then this backpropagation procedure can also be called backpropagation through time as introduced in \cite{GradientDescent}.
    The chain rule for differentiating $z(y(x_0))$ with respect to $x$ for $x=x_0$ where $z$ and $y$ are both functions in a single variable is given by:
    \begin{align}
    \label{chain_rule}
    \frac{dz}{dx} \Bigr\rvert_{x=x_0} = \frac{dz}{dy} \Bigr\rvert_{y=y(x_0)} * \frac{dy}{dx} \Bigr\rvert_{x=x_0}
    \end{align}
    This reveals that a machine learning framework has to compute all partial derivatives of all functions present in the above mentioned computational graph.
    Furthermore, it must keep track of the so called activations which are denoted by $y(x_0)$ in the above formula \ref{chain_rule}, as otherwise the gradient of the loss function with respect to the parameters cannot be computed.
    As this can use lots of memory, reversible layers were introduced by \cite{ReversibleLayer} where intermediate activations can be computed from the output vector of that layer which makes storing intermediate activations obsolete.

    \section{Problem Statement}
    As the sheer amount of different machine learning models can be overwhelming, the task was to fix a distinct application domain and compare the most influential machine learning models in this domain with suitable benchmarks.
    Benchmarks are just large input data sets with associative expected outputs.
    Additionally, ideas for possible improvements in existing architectures should be implemented and benchmarked against the already existing ones.
    All benchmarked models should be implemented in the same machine learning framework and the benchmark suite should be extensible and reusable for other machine learning research projects.
    The whole implementation work done for this thesis should be made accessible for everyone by open-sourcing all the code.
    As mentioned in the abstract, all the models covered in this thesis are either derivatives of the recurrent neural network or the transformer \cite{Transformer} architecture.
    The benchmarks used in this thesis either test the models for their capabilities to capture long-term dependencies or their ability to model physical systems.
    
    \section{How to better model Physical Systems} \label{physical_systems}
    Physical systems are guided by differential equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the state derivative function $f$ and the output function $h$, both of which depend on the absolute time $t$, as follows:
    \begin{align} 
        \label{physical_system_equations_state}
        \dot x(t) &= f(x(t),u(t),t) \\
        \label{physical_system_equations_output}
        y(t) &= h(x(t),u(t),t)
    \end{align}
    This form of system description is applicable to all physical systems in our daily surroundings, most of them are even time-invariant. 
    This means the functions $f$ and $h$ do not depend on the absolute time $t$.
    For example, a mechanical pendulum will now approximately behave the same as in one year, as its dynamics do not depend on the absolute time $t$.
    The system description given in \ref{physical_system_equations_state} and \ref{physical_system_equations_output} proposes, that machine learning models that are built in a similar fashion and whose state is also determined by a differential equation, should be pretty capable of modelling the input-output relation of physical systems.
    When the benchmarked models are introduced in more detail, it can be seen that all continuous-time machine learning models use a comparable structure in terms of parameterizing the state derivative and the output function.
    
    \section{Sampled Physical Systems} \label{sampled_physical_systems}
    As the evaluation of the current state $x$ at point in time $t'$ with initial state $x_0$ given the dynamics from \ref{physical_systems} can be computationally very expensive or even infeasible, sampling was introduced to avoid solving a complex differential equation.
    Therefore, the whole system is only observed at equidistant successive time instants, values belonging to this time instant are denoted with a subscript index $k \in \mathbb{Z}$, and the system is now called discrete.
    Discrete systems are guided by difference equations. The relation between system state $x$, system input $u$ and system output $y$ is given by the next state function $f$ and the output function $h$, both of which depend on the time instant $k$, as follows:
    \begin{align} 
        \label{discrete_system_equations_state}
        x_{k+1} &= f(x_k,u_k,k) \\
        \label{discrete_system_equations_output}
        y_k &= h(x_k,u_k,k)
    \end{align} 
    It must be noted that $x$ and $y$ are time-series in discrete systems and no more functions like in the case of continuous-time physical systems.
    This slightly off-topic explanation is necessary, as vanilla recurrent neural networks are built using the same principle. 
    The system equations \ref{discrete_system_equations_state} and \ref{discrete_system_equations_output} require a regularly (equidistantly) sampled input $x$.
    A similar argument as before in \ref{physical_systems} proposes now that a machine learning model with a similar structure, which gets a regularly sampled input of a physical system, should also be pretty capable of modelling the input-output relation of this sampled physical system.
    The corresponding machine learning models are then called discrete-time machine learning models.
    
    \section{Why capturing Long-Term Dependencies is difficult} \label{long_term_difficult}
    The difficulty will be outlined solely on the example of vanilla recurrent neural networks (RNNs).
    How transformer-based and advanced RNN architectures tackle the problem will be discussed later.
    Vanilla recurrent neural networks are discrete-time machine learning models. 
    Its dynamics are given in a similar fashion to the equations that govern sampled physical systems \ref{sampled_physical_systems}.
    The current state vector $h_{t}$ and the next input vector $x_{t+1}$ determine the next state vector $h_{t+1}$ and output vector $y_{t+1}$ deterministically.
    In this model all the past inputs are implicitly encoded in the current state vector.
    This entails a big challenge for computer scientists, as computers only allow states of finite size and finite precision, unlike our physical environment, which results in an information bottleneck in the state vector.
    The next state of a vanilla recurrent neural network $h_{t+1}$ and its output $y_{t}$ is typically computed by equations like the two proposed in \cite[p. 2]{UnitaryRNNs} by using a non-linear bias-parametrized activation function $\sigma$, three matrices ($W$, $V$ and $U$) and the output bias vector $b_o$:
    \begin{align}
        \label{rnn_state_update}
        h_{t+1} &= \sigma(W*h_t + V*x_{t+1}) \\
        \label{rnn_output}
        y_{t} &= U*h_{t} + b_o
    \end{align}
    Without the time shift on the input in the next state equation \ref{rnn_state_update}, the equations are pretty similar to the ones describing sampled physical systems.
    The following inequality from \cite[p. 2]{UnitaryRNNs} using norms shows the relation between the loss derivative, a recent state $h_T$ and a state from the distant past $h_t$ where $T \gg t$.
    The notation is kept similar to the examples before. A subscript $2$ after a vector norm denotes the Euclidean norm and a subscript $2,ind$ after a matrix norm denotes the spectral norm:
    \begin{align}
        \label{gradient_formula}
        \left\Vert \pdv{L}{h_t} \right\Vert_2 \leq \left\Vert \pdv{L}{h_T} \right\Vert_2 * \left\Vert W \right\Vert_{2,ind}^T * \prod^{T-1}_{k=t} \left\Vert diag(\sigma'(W*h_k + V*x_{k+1})) \right\Vert_{2,ind}
    \end{align}
    This inequality contains all essential parts to understand why capturing long-term dependencies with vanilla recurrent neural networks is difficult.
    Some problems that machine learning tries to solve require incorporating input data from the distant past to make good predictions in the present.
    As these inputs are implicitly encoded in the states of the distant past, $\left\Vert \pdv{L}{h_t} \right\Vert_2$ should not decay to zero or grow unboundedly to effectively tune the parameters using the gradient descent update rule shown above in \ref{gradient_descent_update}.
    This ensures that distant past inputs influence the loss function reasonably and makes it feasible to incorporate the knowledge to minimize the loss function.
    As known the spectral norm of the diagonal matrix in \ref{gradient_formula} is just the largest magnitude out of all diagonal entries.
    Therefore, if the norm of the diagonal matrix is close to zero over multiple time steps $k$, also the desired loss gradient will decay towards zero.
    Otherwise, if the norm of the diagonal matrix is much larger than one over multiple time steps $k$, the desired loss gradient may grow unboundedly.
    Using this knowledge it is now clear that a suitable activation function must have a derivative of one in almost all cases to counteract the above described problems.
    A good fit would be a rectified linear unit (relu) activation function with an added bias term.
    The relu activation function with a bias $b$ can simply be discribed by the function $max(0,x+b)$. The $max$ function should be applied element-wise.
    As the requirements for the activation function candidates are clear now, the next thing to discuss is the norm of the matrix $W$.
    If $\left\Vert W \right\Vert_{2,ind} > 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ may grow unboundedly, making it difficult to apply the gradient descent technique to optimize parameters.
    If $\left\Vert W \right\Vert_{2,ind} < 1$, $\left\Vert \pdv{L}{h_t} \right\Vert$ will decay to $0$, making it impossible to apply the gradient descent technique to optimize parameters.
    These problems are identical to the problems regarding the norm of the diagonal matrix and also have the same implications.
    The first case is calles the exploding gradient problem and the second case is called the vanishing gradient problem for given reasons.
    Both phenomena are explained in more detail in \cite{LongTermDependenciesGradientDescent}.

    \section{Aim of the Work}
    This work should help to objectively compare various machine learning models used to process regularly sampled time-series data.
    It should outline the weaknesses and strengths of the benchmarked models and determine their primary domain of use.
    Moreover, as there are many models benchmarked, their relative expressivity across various application domains can be compared reasonably well.
    Another aim is to provide an overview of what architectures are currently available and how they can be implemented.
    Furthermore, the implemented benchmark suite should be reusable for future projects in the machine learning domain.

    \section{Methodological Approach}
    The first part of this thesis was to determine the most influential models for processing time-series data.
    Some of the models that were benchmarked against each other in this thesis were taken from \cite{ODELSTM}, even though this paper focuses primarily on irregularly sampled time-series.
    The other models were implemented according to the following architectures: Long Short-Term Memory \cite{LSTM}, Differentiable Neural Computer \cite{DNC}, Unitary Recurrent Neural Network \cite{EfficientUnitaryRNNs}, Transformer \cite{Transformer} and Neural Circuit Policies \cite{NCP}.
    These nine models are then complemented by five models that were newly introduced.
    All these models are benchmarked against each other.
    Additionally, a time-continuous memory cell architecture should be introduced.
    This architecture must have its own benchmark test and should not be benchmarked against all other fully-fledged machine learning models as it is only a proof-of-concept implementation.
    All mentioned models should be implemented in the machine learning framework Tensorflow \cite{Tensorflow}.
    After the implementation of all models, an extensible benchmark suite had to be implemented to compare all implemented models.
    A basic benchmark framework should be implemented, which automatically trains a given model and saves all relevant information regarding the training process including generating plots to visualize the data.
    All that should be needed to implement a new benchmark is to specify the input, the expected output data, the loss function and the required output vector size of the model.
    The benchmarks regarding person activity classification, sequential MNIST classification and kinematic physics simulation were taken from \cite{ODELSTM} and were modified slightly to be compatible with the benchmark framework.
    The other two benchmark regarding the copying memory and the adding problem were taken from \cite{UnitaryRNNs}, but were also slightly modified to fit the benchmark framework's needs.
    The sixth benchmark that had to be implemented was the cell benchmark that should check if the memory cell is able to store information over a large number of time steps.
    When this step is also done, all benchmarks should be run on all applicable models and then the results should be thoroughly compared to filter out the strengths and weaknesses of the diverse models.
    Only after that a summary should be written to concisely summarize the most important discoveries and fallacies that were made.

    \section{State of the Art}
    The whole field of sequence modeling started with recurrent neural networks.
    More and more modern machine learning architectures exploit the fact that continuous-time models are very well suited for tasks related to dynamical physical systems as explained in \ref{physical_systems}.
    A few examples for such models would be the CT-GRU \cite{CTGRU}, the LTC network \cite{LTCNetworks} and the ODE-LSTM architecture \cite{ODELSTM}.
    But there are also some older architectures that exploit continuous-time dynamics in machine learning models like the CT-RNN architecture \cite{CTRNN}.
    The other problem described in the previous chapters is the hard task of capturing lon-term dependencies in time-series.
    One solution for the problem was proposed in \cite{UnitaryRNNs}, which introduced the Unitary RNN architecture.
    This architecture in principle just uses the vanilla RNN architecture described above, but with the difference that the matrix $W$ fulfills $\left\Vert W \right\Vert_{2,ind} = 1$ to tackle the vanishing and exploding gradient problem.
    This idea was later refined by \cite{EfficientUnitaryRNNs}.
    The vanishing gradient problem was also tackled by the LSTM architecture \cite{LSTM} using a mechanism called gating. 
    This mechanism changes the next state computation of the vanilla RNN.
    Another possible mitigation to the vanishing gradient problem is the transformer architecture proposed in \cite{Transformer} using a mechanism called attention.
    In principle the transformer architecture model has access to all past inputs at a single time-step and directs its attention to the inputs most relevant for solving the required task.
    This eliminates the need to backpropagate the error through multiple time-steps, which keeps the number of backpropagation steps low. 
    
%\todo{Enter your text here.}

    \chapter{Models}

    \section{Model Factory}
    As all the benchmarks require variants of the same models with different output vector sizes, a model factory function was implemented that is capable of producing an output tensor given the model's name, the output vector size and the input tensor tuple.
    This function was called \texttt{get\_model\_output\_by\_name} and can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    This mechanism of creating the output tensor including the internal computational graph of the Tensorflow library \cite{Tensorflow} is called the Functional API.
    Most of the models were parametrized such that they have roughly $20000$ trainable parameters in the Walker Benchmark \ref{walker} as this benchmark features the largest input and output vector size of all benchmarks. 
    The exceptions of the parameter cound are the Unitary RNN model \ref{urnn}, the NCP model \ref{ncp}, the Unitary NCP model \ref{uncp}, the Recurrent Network Augmented Transformer \ref{rnaut} and the Recurrent Network Attention Transformer \ref{rnatt}.
    All these models' computational graphs lead to high computation cost during backpropagation which leads to training durations up to a whole day for a single benchmark. 
    This was unnacceptable and therefore their parameter count was reduced, such that at least some results can be presented for these models.

    \section{LSTM} \label{lstm}
    The LSTM (Long Short-Term Memory) recurrent neural network architecture is a discrete-time machine learning model as introduced in \ref{sampled_physical_systems}.
    The model not only has an ordinary (hidden) state vector, but also a cell state vector which should store information over a longer time horizon than the hidden state vector.
    This thesis uses the open-source LSTM implementation provided by the Keras library \cite{Keras} which is based on the original LSTM paper \cite{LSTM} as well on its successor paper \cite{LSTM_forget} that introduces a forget mechanism for the LSTM.
    The function the LSTM model is applying to its inputs to produce the outputs is given as follows with inputs denoted as $x_t$ and outputs which equals the hidden states denoted as $h_t$ \cite[p. 4-8]{LSTM_forget}:
    \begin{align}
    \label{forget_gate} f_t &= sigmoid(W_f*x_t + U_f*h_{t-1} + b_f) \\
    \label{input_gate} i_t &= sigmoid(W_i*x_t + U_i*h_{t-1} + b_i) \\
    \label{output_gate} o_t &= sigmoid(W_o*x_t + U_o*h_{t-1} + b_o) \\
    \label{cell_input} \tilde{c}_t &= tanh(W_c*x_t + U_c*h_{t-1} + b_c) \\
    \label{cell_state} c_t &= f_t * c_{t-1} + i_t * \tilde{c}_t \\
    \label{hidden_state} h_t &= o_t * tanh(c_t)
    \end{align}
    The term $f_t$ \ref{forget_gate} is the forget gate's activation vector, $i_t$ \ref{input_gate} is the input gate's activation vector, $o_t$ \ref{output_gate} is the output gate's activation vector, $\tilde{c}_t$ \ref{cell_input} is the cell input activation vector, $c_t$ \ref{cell_state} is the cell state vector and $h_t$ \ref{hidden_state} is the hidden state vector or also called output vector of the LSTM model.
    The initial hidden state $h_0$ and the initial cell state $c_0$ are picked to the all-zero vector.
    Matrices are denoted with capital letters and vectors are denoted with lower case letters. 
    The LSTM model has a configurable state size. The multiplication sign between two vectors denotes a scalar product and it denotes matrix multiplication between matrices and vectors.
    This convention is used throughout this thesis.
    Dimensions of matrices are picked such that the resulting vector has the required state size which is configurable.
    The bias vectors denoted with $b$ also have the required state dimension.
    The matrices denoted by $W$ map the input vector in each time step and the matrices denoted by $U$ map the hidden state vector at each time step to a resulting vector.
    The structure of the model allows it to capture long-term dependencies as by setting $f_t$ equal to one and $i_t$ equal to zero in some common vector indices $i$, only the previous cell state is used to build the next cell state in these next cell state vector entries.
    This will lead to $\frac{\partial{c_{t,i}}}{\partial{c_{t-1,i}}} = 1$, as this clearly approximates the identity function for a specific index $i$ in the cell state vector.
    Backpropagation to activations in the distant past is feasible using this model function as gradients are not vanishing or exploding when the parameters of the model are learnt properly.
    This mechanism is called the constant error carrousel as described in \cite[p. 7]{LSTM}.
    LSTMs can incorporate this mechanism to store essential information from the distant past, which may be useful to make accurate predictions in the future.
    Furthermore, the model can also decide to forget the previous cell state completely, if the current input vector makes the stored cell state obsolete in the corresponding application.
    This is done by learning to set the forget gate's activation vector close to zero and the cell input activation vector is then used to fill the cell state again if the input gate's activation vector is set accordingly.
    The output gate's activation vector determines which portion of the cell state is used to build the hidden state or output vector of the LSTM model. 
    Throughout the thesis an LSTM model with a fixed state vector size of $64$ was used. As mentioned in the benchmark framework section, each model must support an arbitrary output vector size.
    This is accomplished by postprocessing the hidden state outputs of the LSTM by a dense layer with the required amount of output neurons and without an activation function.
    The output $y$ of a dense layer without an activation function and input vector $x$ can simply be given by: $y = W*x + b$.
    In this notation $W$ is a matrix such that it maps the input vector $x$ to the required output size and $b$ is just a bias vector like in the functions describing the LSTM model.
    Training the LSTM model from the Keras library is fast as it uses an optimized cuDNN \cite{cuDNN} implementation.
    The LSTM model implementation used in this thesis is exposed under the \texttt{get\_lstm\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.

    \section{GRU} \label{gru}
    The GRU (Gated Recurrent Unit) recurrent neural network architecture is a discrete-time machine learning model as introduced in \ref{sampled_physical_systems}.
    The model has only a single ordinary hidden state vector.
    This thesis uses the open-source GRU implementation provided by the Keras library \cite{Keras} which is based on the original GRU paper \cite{GRU}.
    The GRU model tries to simplify the LSTM architecture by removing the output gate for example without sacrificing expressivity.
    This leads to a smaller parameter count of a GRU model with the same hidden state vector size as an LSTM model.
    The function the GRU model is applying to its inputs to produce the outputs is given as follows with inputs denoted as $x_t$ and outputs which equals the hidden states denoted as $h_t$ \cite[p. 4]{GRU}:
    \begin{align}
    \label{update_gate} z_t &= sigmoid(W_z*x_t + U_z*h_{t-1} + b_z) \\
    \label{reset_gate} r_t &= sigmoid(W_r*x_t + U_r*h_{t-1} + b_r) \\
    \label{candidate_activation_vector} \tilde{h}_t &= tanh(W_h*x_t + U_h*(r_t * h_{t-1}) + b_h) \\
    \label{output_vector} h_t &= (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t \\
    \end{align}
    The term $z_t$ \ref{update_gate} is the update gate vector, $r_t$ \ref{reset_gate} is reset gate vector, $\tilde{h}_t$ \ref{candidate_activation_vector} is the candidate activation vector and $h_t$ is the hidden state or output vector of the GRU model.
    The initial hidden state $h_0$ is picked to the all-zero vector.
    The notation of operations, matrices and vectors stays the same as for the LSTM architecture \ref{lstm}.
    Subtraction in the output vector equation \ref{output_vector} is meant element-wise and the $1$ should denote the all-one vector.
    As in the LSTM architecture the hidden state vector size is configurable and all matrices map their inputs to a vector of the corresponding hidden state vector size.
    The structure of the model allows it to capture long-term dependencies as by setting $z_t$ equal to zero for some vector entries, only the previous hidden state vector is used to build the next hidden state vector for these indices $i$.
    This will lead to $\frac{\partial{h_{t,i}}}{\partial{h_{t-1,i}}} = 1$, as this clearly approximates the identity function for a specific index $i$ in the hidden state vector.
    Backpropagation to activations in the distant past is feasible using this model function as gradients are not vanishing or exploding when the parameters of the model are learnt properly.
    As also mentioned in \cite[p. 5]{GRU}, the LSTM architecture does not expose its full cell state in the output vector as the cell state is further processes using the output gate.
    The GRU architecture however exposes its full cell state at each time step as it does not have an output gate as mentioned before.
    Another key difference between the LSTM and GRU architecture is that the LSTM architecture controls the portions of the previous cell state and the portions of the cell input activation that add up to the next step cell state separately using the forget gate's activation vector and the input gate's activation vector \ref{cell_state}.
    The GRU model simplifies this mechanism by providing just a single update gate vector $z$.
    The other vector controlling the portion from the previous hidden state that is added together to build the next step hidden state vector is then determined by subtracting $z$ from the all-one vector \ref{output_vector}.
    This is feasible as the sigmoid activation function produces only outputs lying in the interval $[0,1]$. 
    Furthermore, also the reset mechanism works differently in the GRU architecture as the reset vector only operates on the previous step hidden state vector when computing the next state candidate activation vector.
    Throughout the thesis a GRU model with a fixed state vector size of $80$ was used. As mentioned in the benchmark framework section, each model must support an arbitrary output vector size.
    This is accomplished by postprocessing the hidden state outputs with a dense layer just like in the case of the LSTM architecture \ref{lstm}.
    Training the GRU model from the Keras library is fast as it uses an optimized cuDNN \cite{cuDNN} implementation.
    The LSTM model implementation used in this thesis is exposed under the \texttt{get\_gru\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
 
    \section{CT-RNN} \label{ctrnn}
    The CT-RNN (continuous-time recurrent neural network) was first proposed in \cite{CTRNN} and is a continuous-time machine learning model \ref{physical_systems}.
    This thesis uses an implementation taken from the repository of the paper \cite{ODELSTM} which can be found under the url \url{https://github.com/mlech26l/ode-lstms}.
    The CT-RNN has a configurable hidden state vector size and its output vector is equal to its hidden state vector at each time step.
    The hidden state vector is parametrized as follows \cite[p. 2]{CTRNN} with the same notation as introduced in \ref{lstm}:
    \begin{align}
    \label{paper_ct_rnn_function}
    \dot h(t) &= -\frac{h(t)}{\tau} + W * sigmoid(h(t)) + i(t)
    \end{align}
    The division between $h(t)$ and the vector $\tau$ is understood element-wise. The vector $\tau$ is also called the time constant as it is the time constant of the exponential decay of the hidden state vector over time.
    As the input has not in general the same dimension as the hidden state vector, the input is preprocessed by mapping it to the right dimension with a matrix multiplication.
    Furthermore, the implementation used for benchmarking has a tanh activation function as it is applied at a different position in the formula to allow for negative activations.
    There is also an additional bias vector $b$ and scaling vector $\alpha$ introduced whose multiplication is to understood element-wise.
    The derivative in the CT-RNN implementation used for benchmarking is given by:
    \begin{align}
    \label{used_ct_rnn_function}
    \dot h(t) &= -\frac{h(t)}{\tau} + \alpha * tanh(W_h * h(t) + W_i * i(t) + b)
    \end{align}
    The idea of parameterizing the derivative or change of the hidden vector or activation rather than computing a completely new hidden vector or activation was extensively reused in recent research.
    For example ResNets \cite{ResNet} used the idea in a discrete-time model and Neural ODEs \cite{NeuralODEs} reused it in a continuous-time model which features a similar model function as the CT-RNN.
    In discrete-time models residual connections are added which help backpropagation in a deep machine learning architecture as they are just representing the identity function which is easily differentiable.
    For more information on residual connections consult the corresponding paper \cite{ResNet}.
    As the benchmark input samples are only regularly sampled vectors and not a function $i(t)$ as needed by the CT-RNN model function \ref{used_ct_rnn_function}, each input sample is held constantly for $1$ time unit to form the input function.
    This mechanism is used for all continuous-time models throughout this thesis.
    Therefore, the input function is defined on the interval $[0,T]$ where $T$ is the input sequence length.
    The output of the CT-RNN after consuming the whole input function $i(t)$ from time $0$ to time $T$ is then given by the hidden state vector $h(T)$ at time $T$.
    There is also the possibility to evaluate the hidden state at intermediate time points, for example at $T-1$, which equals $h(T-1)$.
    With this mechanism any continuous-time model can also map an input vector sequence to an output vector sequence.
    If additional timing information is available about the input vectors, for example the time interval between two input vector samples, it can be used as time to hold this specific input constantly in the input function.
    This leads to an irregularly sampled time series where time-continuous models are exceptionally well suited as machine learning models, as discrete-time models \ref{sampled_physical_systems} implicitly model a regularly sampled continuous-time system.
    This statement was also shown to be valid by \cite{ODELSTM}.
    The initial state of the CT-RNN is given by $h(0)$, which is picked to the all-zero vector.
    To compute the final hidden state $h(T)$, the ODE (ordinary differential equation) from \ref{used_ct_rnn_function} must be solved given the initial condition $h(0)$.
    This can be done by incorporating ODE solvers which simply compute $h(T)$ by approximately integrating $\dot h(t)$ with guarantees on the error bound.
    Then $h(T)$ is given by $h(0) + \int_0^T{\dot h(t)}~dt$. 
    In all continuous-time models implementations the ODE solver is called at each time step computing the next step hidden state $h(t+1)$ as $h(t) + \int_t^{t+1}{\dot h(t)}~dt$.
    Examples for ODE solvers are the explicit Euler method, the RK4 (Runge-Kutta $4^{th}$ order) method or the Dormand-Prince method.
    The Dormand-Prince method is the default ODE solver used in the \texttt{ode45} solver of MATLAB \cite{MATLAB}.
    All of these are members of explicit methods and the Runge-Kutta methods to solve ODEs. Explicit methods calculate the state at a later time only from the state at the current time.
    There are also implicit methods, which find a solution by solving an equation involving both the state at the current time and the state at the next time.
    Implicit methods are primarily used for stiff ODEs, which are characterized that small numerical deviations or errors can lead to a huge change in the output.
    For the CT-RNN implementation, the RK4 method was used to solve the ODE. The hidden state vector size was picked to $128$ and the number of unfolds was set to $3$.
    The number of unfolds determines how often an ODE solver is called on a single input sample. This means that instead of integrating the whole interval of length $1$ at each time step, the ODE solver integrates an interval of length $\frac{1}{3}$ three times, which yields more accurate results.
    Computing the loss gradient with respect to the model parameters is still possible for continuous-time models as the ODE solvers are just functions themselves which can be differentiated.
    The ODE solver can also be run as a black-box without knowing its internal operations as shown in \cite{NeuralODEs}.
    Then the gradients with respect to the functions applied by the ODE solver can be computed by the adjoint sensitivity method \cite{AdjointSensitivityMethod}.
    As pointed out by \cite[p. 3]{LTCNetworks} this memory-efficient procedure however comes with numerical errors as it forgets the forward-time computational trajectories.
    The CT-RNN model implementation used in this thesis is exposed under the \texttt{get\_ct\_rnn\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/ct_rnn.py}.

    \section{CT-GRU} \label{ctgru}
    The CT-GRU (continuous-time gated recurrent unit) recurrent neural network architecture is a continuous-time machine learning model firstly introduced in \cite{CTGRU}.
    The implementation of the CT-GRU architecture used in this thesis was taken from the repository of \cite{ODELSTM}.
    It shares many concepts with the GRU architecture \ref{gru}, but the update gate \ref{update_gate} and reset gate \ref{reset_gate} operate on multiple hidden state vectors stored across various time scales.
    This is done because some information may become obsolete very quickly, whereas some other information may also be very important in the longer term.
    These rates of information decay are referred to as time scales.
    The time scales are represented using time constants and the amount of time scales was fixed to $8$ in this thesis.
    Therefore, the update gate is then called storage scale and the reset gate is then called retrieval scale as they operate not only on a single hidden vector, but across hidden vectors stored across multiple time scales.
    They can be thought of as multi-dimensional gates.
    As the amount of time scales is fixed, input data that matches a certain time scale not present in the fixed set must be approximated using a combination of the available time scales.
    This is indeed possible with small error when the time scale to approximate is in a certain range as pointed out in \cite[p. 5-6]{CTGRU}.
    Then the half life of the combination of exponentials approximately matches the half life of the corresponding exponential to the correct time scale.
    A good match for time constants $\tau_i$ representing the various time scales is the set of constants where $\tau_0=1$ and $\tau_{i+1} = \sqrt{10} * \tau_{i}$.
    This set was also used in the benchmarked implementation.
    The explicit time input called $\Delta t_k$ of this model was not used as interval to integrate an ODE, but instead as time duration of exponential decay between two input vectors.
    As all benchmarks do not provide time inputs and the input vectors of the benchmarks are regularly sampled, $\Delta t_k$ was set to constant $1$.
    The function the GRU model is applying to its input vectors to produce the output vectors or hidden state vectors is given as follows with inputs denoted as $x_k$ and outputs which equals the hidden states denoted as $h_k$ \cite[p. 7]{CTGRU}:
    \begin{align}
    \label{retrieval_scale}
    \ln{\tau_k^R} &= W^R*x_k + U^R*h_{k-1} + b^R \\
    \label{retrieval_weighting}
    r_{ki} &= softmax_i(-(\ln{\tau_k^R-\ln{\tau_i}})^2) \\
    \label{relevant_event_signals}
    q_k &= tanh(W^Q*x_k + U^Q*(\sum_i {r_{ki} *_{ew} \tilde{h}_{k-1,i}}) + b^Q) \\
    \label{storage_scale}
    \ln{\tau_k^S} &= W^S*x_k + U^S*h_{k-1} + b^S \\
    \label{storage_weighting}
    s_{ki} &= softmax_i(-(\ln{\tau_k^S-\ln{\tau_i}})^2) \\
    \label{state_update}
    \tilde{h}_{ki} &= [(1 - s_{ki}) *_{ew} \tilde{h}_{k-1,i} + s_{ki} *_{ew} q_k] * e^{-\frac{\Delta t_k}{\tau_i}} \\
    \label{state_output}
    h_k &= \sum_i{\tilde{h}_{ki}}
    \end{align}
    Multiplication which are meant to be understood element-wise are denoted with a subscript $ew$, otherwise the notation is kept the same as in the previous models.
    The equations \ref{retrieval_scale} and \ref{retrieval_weighting} determine the retrieval scale and compute the weighting for each time scale.
    The equations \ref{storage_scale} and \ref{storage_weighting} determine the storage scale and compute the weighting for each time scale.
    The retrival scale vector $r_{ki}$ is the multi-dimensional equivalent to the reset vector of the GRU architecture and the storage scale vector $s_{ki}$ is the multi-dimensional equivalent to the update vector of the GRU architecture.
    Equation \ref{relevant_event_signals} describes how the next candidate hidden state vector $s_k$ is computed.
    Finally, equation \ref{state_update} describes how the hidden state for each time scale is updated and equation \ref{state_output} describes how the output vector $h_k$ is computed out of the multi-dimensional hidden state vector.
    It can be said that the CT-GRU architecture is a GRU model with multi-dimensional state and exponential decay of its state between input vector observations with different time constants.
    Most of the features discussed for the GRU model are also applicable for the CT-GRU architecture.
    It should also be able to learn long-term dependencies as time scales featuring a large time constant have little decay on their corresponding hidden state and then simply the argument used in the GRU architecture \ref{gru} can also be applied here.
    As other models, the CT-GRU has a configurable hidden state vector size, which was picked to $32$ throughout this thesis.
    The CT-GRU model implementation used in this thesis is exposed under the \texttt{get\_ct\_gru\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/ct_gru.py}.

    \section{ODE-LSTM} \label{odelstm}
    The ODE-LSTM recurrent neural network architecture is a continuous-time machine learning model firstly introduced in \cite{ODELSTM}.
    The implementation of the ODE-LSTM architecture used in this thesis was taken from the repository of its original paper \cite{ODELSTM}.
    The idea behind this model is to combine the abilitiy of the LSTM architecture to capture long-term dependencies and the ability of CT-RNNs to accurately model dynamical physical systems, even if an irregularly sampled time-series is provided to the model as input.
    As in this thesis, only regularly sampled time-series are used, the continous time model is always fed with the time input $1$ as mentioned in the \ref{ctrnn} section. 
    This should be no problem as the ability to model dynamical physical systems generalizes to any time input very well.
    Similar to the LSTM architecture, the ODE-LSTM has two state vectors: one hidden state vector $h_i$ and one cell state vector $c_i$. Both vectors are initialized to the all-zero vector.
    The function the ODE-LSTM model is applying to its input vectors to produce the output vectors or hidden state vectors is given as follows with inputs denoted as $x_i$ and outputs denoted as $h_i$ \cite[p. 5]{ODELSTM}:
    \begin{align}
    (c_i,h_i') &= LSTM(x_i, (c_{i-1}, h_{i-1})) \\
    h_i &= CTRNN(h_i', (h_{i-1}))
    \end{align}
    The function $LSTM$ denotes one model function step of the LSTM model \ref{lstm} starting from the given state $(c_{i-1}, h_{i-1})$ for input $x_i$.
    The function $CTRNN$ denotes one model function step of the CT-RNN model \ref{ctrnn} starting from the given state $(h_{i-1})$ for input $x_i$, the input is set to $1$ for each time step.
    Implementation-wise, the CTRNN model function call was done to the implementation as described in \ref{ctrnn}.
    The LSTM model function was implemented from scratch and no library modules were used. 
    As only the hidden state vector of the LSTM architecture is postprocessed by the CT-RNN model, the cell state stays untouched, which enables the architecture to learn long-term dependencies by using the same argument as in \ref{lstm}.
    By the postprocessing of the hidden state vector which controls the LSTM's gates, the gating dynamics become dependent on the time input as well \cite[p. 4]{ODELSTM}.
    Of course also the ODE-LSTM architecture has a configurable hidden state vector size which was picked to $64$.
    The CT-RNN was initialized by the same hidden vector size, the number of unfolds were set to $4$ and the explicit Euler method was used as an ODE solver.
    The ODE-LSTM model implementation used in this thesis is exposed under the \texttt{get\_ode\_lstm\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/ode_lstm.py}.

    \section{Neural Circuit Policies (NCP)} \label{ncp}
    Neural Circuit Policies were used in the paper \cite{NCP} which shows the high expressivity of the architecture in the domain of autonomous driving.
    The architecture is a subset of all LTC (Liquid Time-Constant) Networks that were introduced in \cite{LTCFormulas} and further discussed in \cite{LTCNetworks}.
    An LTC Network consists of biologically inspired neurons with leakage that are interconnected using chemical synapses with non-linear activations.
    LTC Networks model the cell membrane as integrator and are therefore a continous-time machine learning model.
    Neural Circuit Policies were derived from the neuron interconnection structure of the Caenorhabditis elegans nematode \cite[p. 3]{NCP} which trims the space of all possible LTC networks.
    The state of each neuron $i$ with incoming chemical synapses from neurons $j$ is given as its potential $V_i$ and the ODE that describes the dynamics of a single neuron's potential is given by \cite[p. 1-2]{LTCFormulas}:
    \begin{align}
    \label{ltc_formula}
    \dot{V_i}(t) &= \frac{1}{C_i} * (I_{leak,i} + \sum_j{I_{syn,ji}}) \\
    \label{leakage_current}
    I_{leak,i} &= G_{leak,i} * (E_{leak,i} - V_i(t)) \\
    \label{synaptic_current}
    I_{syn,ji} &= [G_{syn,ji} * sigmoid(\sigma_{ji}*(V_j(t)-\mu_{ji}))] * (E_{ji} - V_i(t))
    \end{align}
    By reordering terms in equation \ref{ltc_formula}, it can be shown that the time constant $\tau$ as used in the equation \ref{paper_ct_rnn_function} in the CT-RNN architecture is varying with time.
    The capacitance of a neuron $i$ is denoted as $C_i$ and the whole equation will be more familiar when the capacitance is brought to the left hand side which yields $C_i * \dot{V_i}(t) = I_{leak,i} + \sum_j{I_{syn,ji}}$.
    This is just the differential equation describing the behavior of an electrical conductance.
    All the currents on the right hand side as given in the leakage current equation \ref{leakage_current} and in the chemical synaptic current equation \ref{synaptic_current} are written according to Ohm's law $I = \frac{U}{R}$.
    By using the conductance $G$ instead of the resistance $R$ which is just the reciprocal value, the equation yields $I = G * U$, exactly the form both current equations are using.
    As the voltage $U$ is given as the potential difference, all terms in equation \ref{leakage_current} and \ref{synaptic_current} should be clear now.
    Worth mentioning is the non-linear conductance for chemical synaptic currents given as $G_{syn,ji} * sigmoid(\sigma_{ji}*(V_j(t)-\mu_{ji}))$, where the parameter $G_{syn,ji}$ controls the maximum conductance, the parameter $\mu_{ji}$ controls the mean conductance potential and the parameter $\sigma_{ji}$ controls the steepness of the transition between conductance and non-conductance.
    Note that the non-linear synaptic conductance is only influenced by the presynaptic neuron potential $V_j(t)$.
    The potentials given by the capital letter $E$ control the targeted potentials for the neuron $i$, therefore if the neuron has reached this potential the corresponding currents will vanish. 
    The NCP architecture builds its output vector by determining output neurons in the same amount as the output vector size.
    These neurons are called motor neurons and their vectorized potentials then build the output vector.
    The input vector entries are fed to neurons as currents by using the chemical synaptic current equation as described in \ref{synaptic_current} and by setting the presynaptic potential equal to the input vector entry.
    Furthermore, before the input vector is provided to the NCP model and before the output vector is returned from the NCP model, an affine transformation is applied to the input and output vector by mapping both vectors with a dense layer as described in the LSTM section \ref{lstm}.
    Additionally to motor neurons, NCP models also have inter and command neurons.
    Inter neurons receive input vector entries as chemical synaptic currents and command neurons are the only neuron type where recurrent connections are allowed.
    Command neurons also are the only neuron type which has synaptic connections to motor neurons.
    Therefore, the input vector entries are processed using the inter neurons, which feed the processed information to the command neurons that control the motor neurons and therefore the output vector entries.
    The procedure to create the synaptic wiring is described in detail in \cite[p. 3]{NCP} and will not be covered in this thesis.
    The NCP implementation used for benchmarking uses the implementation provided in the repository of the paper \cite{NCP} located under the url \url{https://github.com/mlech26l/keras-ncp}.
    It was configured with $9$ inter neurons and $7$ command neurons. The amount of motor neurons was picked according to the required output vector size.
    There were two incoming synapses from input vector entries to inter neurons and two incoming synapses from inter neurons to command neurons.
    Each motor neuron receives two incoming synapses from command neurons and there were $14$ recurrent synapses in all command neurons.
    The time input to solve the ODE was set to $1$ per time step and the ODE was solved using the Fused Solver proposed in \cite{LTCNetworks} that fuses explicit and implicit Euler methods.
    Per time step the ODE was unrolled $6$ times, as there are at least $3$ unrolls necessary until the currents from the input vector reach the command neurons via synapses in each time step.
    The initial potential of all neurons was picked to $0$.
    The NCP model implementation used in this thesis is exposed under the \texttt{get\_neural\_circuit\_policies\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/neural_circuit_policies.py}.

    \section{Unitary RNN} \label{urnn}
    The Unitary RNN architecture was first introduced in \cite{UnitaryRNNs} and later refined in \cite{EfficientUnitaryRNNs} and is a discrete-time machine learning model.
    It uses the same vanilla recurrent neural network model function as discrussed in section \ref{long_term_difficult}.
    The Unitary RNN implementation used in this thesis is a modified version of the implementetion provided with the original paper which can be found under the url \url{https://github.com/jingli9111/EUNN-tensorflow/blob/master/eunn.py}.
    The next hidden state vector of a Unitary RNN $h_{t+1}$ which also equals its output vector is computed as given in \cite[p. 2]{EfficientUnitaryRNNs} by using a non-linear bias-parametrized activation function $\sigma$ and two matrices $W$ and $V$:
    \begin{align}
    \label{urnn_state}
    h_{t+1} &= \sigma(W*h_t + V*x_{t+1})
    \end{align}
    The bias-parametrized activation function $\sigma$ was set to the modrelu function firstly introduced in \cite[p. 4]{UnitaryRNNs}.
    The modrelu function applied to a complex vector $z$ is defined as follows for each vector entry $z_i$: $moderelu(z_i) = max(0, |z_i|+b_i) * \frac{z_i}{|z_i|}$ with a real-valued bias parameter $b_i$ per vector entry.
    The initial hidden state vector $h_0$ was picked to the all-zero vector.
    The difference with this model is that it does not use real parameters which is the standard in machine learning.
    It uses complex parameters which are represented by two single-precision floating-point parameters each. 
    The parameter count for each model however is always given in terms of single-precision floating-point parameters.
    The matrices $W$ and $V$ are parametrized as complex matrices. 
    Matrix $V$ does not have to follow any particular restrictions, therefore it can be simply parametrized by two real matrices $V_{real}$ and $V_{imag}$ for the real and imaginary part. 
    As explained in detail in section \ref{long_term_difficult}, a matrix $W$ that fulfills $\left\Vert W \right\Vert_{2,ind} = 1$ and a suitable activation function $\sigma$ would solve the vanishing and exploding gradient problem for the vanilla recurrent neural network architecture and exactly this was done in the case of Unitary RNNs. 
    Unitary matrices $W$ fulfill the requirement $\left\Vert W \right\Vert_{2,ind} = 1$, as all eigenvalues of unitary matrices have a magnitude of $1$ from which follows that $1$ is always the largest singular value as unitary matrices are square. 
    As the spectral norm is just the largest singular value, it is proven that unitary matrices fulfill the proposed requirement.
    The difficulty now is to parametrize unitary matrices efficiently as they are only a subset of all complex matrices and therefore cannot be as simply parametrized as the matrix $V$.
    The method to parametrize unitary matrices as used in \cite[p. 3]{EfficientUnitaryRNNs} was proposed by \cite{UnitaryMatrixParametrization} and is called square decomposition method. 
    The core statement is that any unitary matrix of dimension $N \times N$ can be represented by matrix multiplications involving a diagonal matrix $D$ and rotational matrices $R_{ij}$ as follows:
    \begin{align}
    W &= D \prod_{i=2}^N \prod_{j=1}^{i-1} R_{ij}.
    \end{align}
    The diagonal matrix $D$ has only the entries $e^{iw_j}$ on its diagonal which results in $N$ parameters $w_j$.
    The matrices $R_{ij}$ which are parameterized by two real parameters $\theta_{ij}$ and $\phi_{ij}$ are defined as $N$-dimensional identity matrices whose four entries at positions given as $(row,column)$ are replaced with given entries as follows:
    \begin{align}
    \begin{bmatrix} 
    (i,i) & (i,j) \\
    (j,i) & (j,j) 
    \end{bmatrix} \mapsto 
    \begin{bmatrix} 
    e^{i\phi_{ij}}\cos (\theta_{ij})  & -e^{i\phi_{ij}}\sin (\theta_{ij}) \\
    \sin (\theta_{ij}) & \cos (\theta_{ij}) 
    \end{bmatrix}
    \end{align}
    By reordering and grouping rotational matrices as shown in \cite[p. 4]{EfficientUnitaryRNNs}, the unitary matrix $W$ with even capacity $L$ can also be written as:
    \begin{align}
    W = D * F_A^{(1)} * F_B^{(2)} * F_A^{(3)} * F_B^{(4)} * \ldots * F_B^{(L)}
    \end{align}
    Whenever the capacity $L$ matches the dimension $N$ of the unitary matrix $W$, this expression spans the entire space of all unitary matrices. 
    Whenever the capacity $L$ is smaller than the dimension $N$ of the unitary matrix $W$, this expression spans a subspace of the space of all unitary matrices. 
    The matrices $F_A^{(l)}$ and $F_B^{(l)}$ are constructed as follows where superscript $(l)$ denotes different instances of the same type of rotational matrices when the subscript matches:
    \begin{align}
    F_A^{(l)} &= R_{1,2}^{(l)} * R_{3,4}^{(l)} * R_{5,6}^{(l)} * \ldots * R_{N/2-1,N/2}^{(l)} \\
    F_B^{(l)} &= R_{2,3}^{(l)} * R_{4,5}^{(l)} * R_{6,7}^{(l)} * \ldots * R_{N/2-2,N/2-1}^{(l)}
    \end{align}
    Furthermore, each matrix $F$ of the above two types is a general rotational matrix and its mapping performend on a vector $x$ can also be written as \cite[p. 4]{EfficientUnitaryRNNs}:
    \begin{align}
    F*x = v_1 *_{ew} x + v_2 *_{ew} permute(x)
    \end{align}
    The vectors $v_1$ and $v_2$ are computable from the parameters $\theta_{ij}$ and $\phi_{ij}$ that are used to parameterize the rotational matrices $R_{ij}$ that build the matrix $F$.
    The permutation given by the function $permute$ is fixed and set only at the first instantiation of the machine learning model. 
    The formula used to generate both vectors $v_1$ and $v_2$ is given under \cite[p. 4]{EfficientUnitaryRNNs}.
    As this way of applying the mapping of the $F$ matrices to the input vector avoids matrix multiplications and just uses element-wise multiplications and permutation operations, it is an efficient way to parameterize unitary matrices.
    As the output vector of this machine learning model is complex, the real part of the output was used for further processing as this was also done in benchmarks from the official repository of the paper \cite{EfficientUnitaryRNNs} which can be found under the url \url{https://github.com/jingli9111/EUNN-tensorflow/blob/master/copying_task.py}.
    Also this model has a configurable hidden vector size which must be even and was picked to $128$ throughout the thesis.
    The capacity $L$ was always set to $16$, therefore the matrix $W$ is parameterized as a partial-space unitary matrix.
    As the output vector size has the be variable, the real part of the output vector of the model was then fed to a dense layer to achieve the right output vector dimension.
    The Unitary RNN model implementation used in this thesis is exposed under the \texttt{get\_unitary\_rnn\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/unitary_rnn.py}.
    
    \section{Matrix Exponential Unitary RNN} \label{meurnn}
    This machine learning model is an original contribution and a variant of the Unitary RNN \ref{urnn} architecture.
    It is therefore also a discrete-time recurrent neural network model with the same model function as specified in \ref{urnn_state} and augments the architecture in various ways.
    Only the differences between the two architectures will be listed.
    First, the option to use a trainable initial hidden state vector was added to the architecture which is initialized to the all-zero vector.
    Furthermore, there was an option added to use an augmented input for the model.
    This augmented input consists of the concatenation of the regular input vector $x_k$ per time step and its 1D discrete Fourier transform as given by $FFT(x_k)$.
    As problems in the signal and system theory domain are either easier to solve in the time or the frequency domain, this feature may help to make better predictions in some tasks.
    Moreover, the DFT matrix used to convert a time-domain vector to the frequency domain is also a unitary matrix, which preserves the energy of the input vector, and is therefore a good fit for this architecture.
    Both described features are disabled during benchmarking of this model, as they showed no substantial decrease of the final test loss.
    Another difference to the Unitary RNN architecture \ref{urnn} is the construction of the output vector of the required size.
    As the imaginary part of the hidden state vector may also convey useful information, the approach from \cite[p. 4]{UnitaryRNNs} was used in the implementation to construct the output vector.
    With this method the final output vector is constructed by passing a concatenated vector consisting of the real and imaginary part of the hidden state vector which is now solely real through a dense layer to get the right output vector dimension.
    The last difference is the parametrization of the unitary matrix $W$ used in the model function equation \ref{urnn_state}.
    As presented in section \ref{urnn} the parametrization is quite involved and therefore the new way of parameterizing the unitary matrix is using an approximated matrix exponential.
    As any unitary matrix $W$ of dimension $N \times N$ can be written as the matrix exponential of a skew-Hermitian matrix $A$ of dimension $N \times N$ as $W=e^A$, the problem is reduced to parameterizing a skew-Hermitian matrix $A$.
    This matrix exponential is the matrix generalization of $|e^j|=1$ in the scalar case where $j$ is any imaginary number.
    The approximated matrix exponential implementation used for this model is exposed under the function \texttt{tf.linalg.expm} in the Tensorflow library \cite{Tensorflow} which uses Pad approximation as desribed in \cite{expm}.
    The fundamental idea is that $e^A = (e^{2^{-s}A})^{2^s} \approx (r_m(2^{-s}A))^{2^s}$ where $r_m(X)$ is the $[m/m]$ Pad approximant to $e^X$ and the non-negative integers $m$ and $s$ are to be chosen \cite[p. 1]{expm}.
    An approximation is needed as the matrix exponential $e^A$ is defined by an infinite sum as follows:
    \begin{align}
    \label{expm_definition}
    e^A = \sum_{k=0}^{\infty} \frac{A^k}{k!}
    \end{align}
    A skew-Hermitian matrix $A$ fulfills $A^H = -A$ which implies that the individual matrix entries fulfill $a_{ij} = -\overline{a_{ji}}$. 
    This further implies that the diagonal entries of $A$ are purely imaginary.
    Therefore, the square skew-Hermitian matrix $A$ can be parameterized by only a lower triangular matrix $T$ with complex entries as all other entries follow by symmetry.
    The diagonal entries in this matrix $T$ can be parametrized with a real parameters, therefore saving $N$ parameters, but this optimization was not applied in the implementation.
    The skew-Hermitian matrix $A$ can easily be constructed by the triangular matrix $T$ by the following formula fulfilling all symmetry requirements:
    \begin{align}
    \label{skew_hermitian}
    A = T - T^H
    \end{align}
    As in this formula \ref{skew_hermitian} only the diagonal entries overlap after the transposition, the diagonal entries will be purely imaginary as the real parts will cancel themselves.
    All other entries follow the predescribed symmetry.
    In this model's implementation the matrix $T$ was parameterized by a vector $v$ of size $N*(N+1)/2$ which equals the number of all non-zero elements in $T$.
    This vector $v$ was then converted to a triangular matrix by filling a triangular matrix with all the values from $T$.
    With this method any lower triangular matrix $T$ can be constructed, from which any skew-hermitian matrix $A$ can be constructed, from which any unitary matrix $W$ can be computed by using the matrix exponential.
    This parameterization allows to parameterize the full-space of unitary matrices. 
    If a partial-space parametrization is favored to reduce the parameter count of the model, there is a capacity measure $c$ available in the model's implementation which should fulfill $0 \leq c \leq 1$.
    With this only the first $\lfloor c*N*(N+1)/2 \rfloor$ entries of the vector $v$ will be trainable and the remaining entries will be filled up with zeros.
    The model used for benchmarking had a hidden vector size of $128$ and the capacity measure $c$ set to $1$, therefore the full space of unitary matrices was parameterizable.
    The Matrix Exponential Unitary RNN model implementation used in this thesis is exposed under the \texttt{get\_matrix\_exponential\_unitary\_rnn\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/matrix_exponential_unitary_rnn.py}.

    \section{Unitary NCP} \label{uncp}
    The Unitary NCP model is a novel discrete-time machine learning model that combines the Unitary RNN model \ref{urnn} and the Neural Circuit Policies model \ref{ncp}, just like the ODE-LSTM model \ref{odelstm} combines the LSTM \ref{lstm} and the CT-RNN \ref{ctrnn} architecture.
    This combination however is not as tightly coupled as the ODE-LSTM architecture. 
    This architecture uses a Unitary RNN to preprocess all input vectors of the input sequence $x_k$ to an intermediate sequence by storing the real part of the hidden state vector at each time step without feeding it through a dense layer afterwards.
    Then this intermediate sequence is fed to the Neural Circuit Policies model, which just treats it as its regular input sequence and maps it to the output vector sequence $o_k$.
    The Unitary NCP model function is given as follows where $x_k$ is the input vector at time step $k$, $h_{k,unitary}$ is the hidden state vector of the Unitary RNN model, $h_{k,ncp}$ is the state vector of the Neural Circuit Policies model and $o_k$ is the output vector at time step $k$:
    \begin{align}
    h_{k+1,unitary} &= UnitaryRNN(x_{k+1}, h_{k,unitary}) \\
    (h_{k+1,ncp}, o_{k+1}) &= NCP(\Re{h_{k+1,unitary}}, h_{k,ncp})
    \end{align}
    The $UnitaryRNN$ function is just a pointer to the corresponding model function described in \ref{urnn_state}.
    How the NCP model maps the input sequence to an output sequence as meant by the function $NCP$ is described in detail in section \ref{ncp}.
    The architecture should combine the great expressiveness of the NCP model with the ability to capture long-term dependencies of the Unitary RNN model.
    The Unitary RNN was configured with a hidden state vector size of $32$ and the capacity was set to $4$. 
    The NCP model uses $4$ inter and command neurons and no recurrent command synapses, as memory-related tasks should be handled by the Unitary RNN.
    For details on both architectures, consult their individual sections.
    The Unitary NCP model implementation used in this thesis is exposed under the \texttt{get\_unitary\_ncp\_output} function defined in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/model_factory.py}.
    The in-detail implementation is provided in the file \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/models/unitary_ncp.py}.

    \section{Transformer} \label{transformer}

    \section{Recurrent Network Augmented Transformer} \label{rnaut}

    \section{Recurrent Network Attention Transformer} \label{rnatt}

    \section{Memory Augmented Transformer} \label{mat}

    \section{Differentiable Neural Computer} \label{dnc}
    This model defines a memory-augmented neural network architecture, that consists of a controller, read and write heads and obviously an external memory that is not parameterized by the neural network parameter.
    Furthermore, the external memory was structured in rows, where each memory row has a specific length.
    The architecture was taken from \cite{DNC} and is an enhancement to the Neural Turing Machine firstly introduced in \cite{NTM}.
    The Neural Turing Machine introduced differentiable read and write functions that allow to access the memory by context or by location in both read and write mode.
    The access by context was implemented by comparing the cosine distance of an emitted key vector to all memory row contents and by applying the softmax function to that distance vector, which yields a weight vector.
    The access by location was implemented by adding a possibility to interleave the previous step weights with the current step content-based weights and adding a "blurry" shift operation on top of it.
    These weight vectors are then normalized using a softmax function that takes each argument to the power of an emitted number to sharpen the weights.
    Some improvements of the Differentiable Neural Computer include a memory management system that is able to allocate and free memory in the external memory to avoid overwriting of important information and a memory use link matrix that allows the model to track its memory operations through time.

    \section{Memory Cell} \label{memory_cell}
    The Memory Cell is a simple RNN consisting of two LTC neurons as described in \cite{LTCNetworks} and used in \cite{NCP}.
    However, the leakage term was removed from the ordinary differential equation describing the state dynamics, as a fading potential would mean losing information stored in the memory cell over time.
    It has a 2-dimensional input, the input voltage for each of the two neurons delivered by a synapse, and a 1-dimensional output, which is just the potential of the first neuron.
    This model was implemented to tune state dynamics and parameters for the suggested memory cell architecture.
    Each cell has three incoming synapses:
    \begin{itemize}
        \item{}
        an excitatory synapse that delivers the input voltage over a synapse to the neuron
        \item{}
        a recurrent excitatory synapse that connects each neuron with itself, which is useful to maintain an excitation in a single neuron
        \item{}
        an inhibitory synapse from the other neuron, to create mutual exclusive excitation
    \end{itemize}
    This simple memory cell should now be able to store information over a long time horizon, the input vectors are provided in the right way.
    Recent results have shown that this architecture is not capable of repeatedly storing information and I am not sure in what range the input values shall lie.
    As described in \cite{LTCNetworks}, the synaptic current is computed by multiplying a non-linear conductance with a potential difference.
    The potential difference is just a parameter $E_{ij}$ minus the potential of the postsynaptic neuron $V_j$.
    However, if the potentials are not bounded, this would mean even an excitatory synapse can deliver a negative current or analogously an inhibitory synapse can deliver a positive current.
    The bounded dynamics of LTC networks is no longer valid, as the leakage term was removed from the state dynamics equation.

    \chapter{Benchmarks}

    \section{Benchmark Framework}
    \subsection{Setup}
    A single code base to run and evaluate the diverse set of benchmarks and models was inevitable. 
    Otherwise, the whole project would have been unmanageable.
    As the implementation of all models occurred in the \texttt{Python} programming language \cite{Python3} using the framework Tensorflow \cite{Tensorflow}, also the benchmark framework used the same set of tools.  
    Therefore, a benchmark base class was created in the file \texttt{benchmark.py}, which is available under the URL \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/benchmark.py}.
    The creation of a new benchmark is as easy as subclassing the benchmark base class \texttt{Benchmark}.
    For instructions how to call the newly created class, please consulate the \texttt{README.md} file given under the URL \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/README.md}.
    After subclassing the base class, the new class has to correctly call the superclass constructor and overwrite the abstract method \texttt{get\_data\_and\_output\_size}.
    Furthermore, the new benchmark's name should be added to the \texttt{BENCHMARK\_NAMES} list constant.
    The superclass constructor only has two arguments: \texttt{name} and \texttt{parser\_configs}. 
    The first argument is just the name of the new benchmark passed as a string.
    The second argument should be a tuple of individual parser configs. 
    A parser config is itself a tuple consisting of the argument name, the argument default value and the argument type.
    This argument determines which values should be settable and usable when calling the benchmark from the command line.
    There are at least three parser configs required that set the loss name, the loss config and the metric name.
    A sample \texttt{parser\_configs} argument would be: \texttt{(('-{}-loss\_name', 'SparseCategoricalCrossentropy', str), ('-{}-loss\_config', {'from\_logits': True}, dict), ('-{}-metric\_name', 'SparseCategoricalAccuracy', str))}.
    If loss config or metric name is not applicable to the benchmark, simply set the default loss config to \texttt{\{\}} or the default metric name to \texttt{''}.
    Furthermore, if the benchmark needs additional parameters, just extend the \texttt{parser\_configs} parameter to also include the desired command line arguments.
    This feature is used by all individual benchmark implementations. 
    After calling the superclass constructor all command line arguments configured through \texttt{parser\_configs} will be available by their names as properties of \texttt{self.args} without the double hyphen.
    For example the loss name can be accessed by \texttt{self.args.loss\_name}.
    If some parameters were set through the command line, they will have the corresponding value, otherwise the configured default values will be applied.
    After that the benchmark base class will create paths for some required directories. 
    There are five directories required during benchmark execution: a saved model directory (will be created to save the models together with their best weights during training), a TensorBoard directory (will be created to save TensorBoard logs for eventual later evaluation), a supplementary data directory (already present in the repo to pass input data to the benchmark), a result directory (will be created to save csv files with relevant information about the training process) and a visualization directory (will be created to save visualizations created after each training of a model).
    All these paths start in the root folder of the repository called \texttt{NeuralNetworkArena}. 
    The structure how these paths continue is the same for all five kinds of folders.
    For the next step in path creation, the individual name for the required folder kind will be appended to the root folder.
    These names can be passed as a command argument when calling the individual benchmark classes. 
    For a more in detail description of these command line parameters just call an implemented benchmark class with the \texttt{-{}-h} command line parameter as described in the \texttt{README.md} file.
    Then the name of the individual benchmarks is added to the path, such that each benchmark has its own five subfolders.
    Then the benchmark base class calls its \texttt{get\_data\_and\_output\_size} method that should have been implemented by the subclass.
    The function should return a tuple of inputs, a tuple of expected outputs and an output vector size of the machine learning model.
    The input and output tuple should only contain numpy arrays \cite{numpy}. The output tuple must have size exactly one.
    The input tuple must have size at least one.
    The benchmark base class has also support for time inputs to the models.
    Please make sure that the time input is the last entry in the input tuple.
    There is also the command line argument called \texttt{use\_time\_input}. 
    If you want to use time input, then make sure you have this argument set to true.
    Otherwise, if the input tuple has a dimension larger than one, the last entry will be discarded from the input tuple, as it is assumed to be the time input.
    The benchmark suite works currently only for benchmarks which provide time-series input data and only expect a model output after the last input data in the time-series.
    For people familiar with the Tensorflow framework \cite{Tensorflow} this is equivalent to setting \texttt{return\_sequences=False} in an RNN model.
    All input arrays in the input tuple should have the shape \texttt{(SAMPLE\_AMOUNT, SEQUENCE\_LENGTH, INPUT\_DIMENSION)}.
    Of course the input dimension can vary between different inputs. Time data should have an input dimension of one. 
    The single output array present in the output tuple should have the shape \texttt{(SAMPLE\_AMOUNT, OUTPUT\_DIMENSION)}.
    The sample amount should match between input and output data to be valid input to the benchmark framework.
    All the constraints on the shapes will be checked by the framework and then all individual samples are shuffled such that corresponding input and output data are at the same indices in their arrays.
    Then tensors are created with the same shape as the inputs in the input tuple excluding the first dimension that denotes the sample amount.
    These are required to later use the Functional API of the Tensorflow framework \cite{Tensorflow}.
    They are created by specifying a fixed batch size, which helps the machine learning framework to better optimize the computational graph for the corresponding model.
    The default batch size is set to $128$ and can be changed by a command line parameter.
    After that, the whole samples are divided into test, validation and training samples. The amount of test and validation samples can be set via command line parameters, which default to $10\%$ each. 
    It is ensured that each individual sample set is exactly divisible by the batch size, as the computational graph was optimized by only allowing inputs of a fixed batch size as described above.
    After all the setup work is done, the folder paths to the result, the saved model and the TensorBoard directory will be augmented with the model name that is currently under test and which was passed via a command line parameter.
    Then the TensorBoard directory for that model will be deleted, as each training run creates a significant amount of log files.
    After that, the TensorBoard, the result, the saved model and visualization directory will be created if they do not already exist. 
    Then it will be checked if the passed model name is present in the list constant \texttt{MODEL\_ARGUMENTS} in the file \texttt{model\_factory.py}.
    When this check is passed, the benchmark framework either loads a saved model with the corresponding model name or it creates a new one using the model output functions in the predescribed model factory depending on the command line parameter \texttt{use\_saved\_model}.
    These output functions get an output vector size and the tensor inputs and create an output tensor that contains all the information about the operations in between.
    By knowing the input and the output tensors, the Tensorflow \cite{Tensorflow} Functional API can be incorporated to create a machine learning model.
    If the model is newly created and not loaded from a saved one, the model is also compiled using a customizable optimizer, learning rate, loss, loss config and metric.
    These can be changed by command line parameters.
    The default optimizer and learning rate used throughout all benchmarks in this thesis are the Adam optimizer \cite{Adam} and a learning rate of $10^{-3}$.
    The three remaining parameters also discussed in the previous subsection must be passed such that it is conforming with the requirements of the functions \texttt{tf.keras.optimizers.get} and \texttt{tf.keras.losses.get}.
    A debug mode can also be enabled via the command line which puts the newly created model in eager execution mode making it easier to debug the model. Furthermore, the model will be called on a single batch of inputs without invoking the model's \texttt{fit} method. This happens only in debug mode.
    In any case a model ready to train should now have been constructed and all the models characteristics including input and output shape will then be printed to the command line enabling to check if all the dimensions match the expectations.
    \subsection{Training}
    After printing available information of the model to the command line a unix timestamp is retrieved from the system to keep track of the total training duration.
    Then the training is ultimately started by invoking the model's \texttt{fit} method. This method takes the training and validation sample set, the batch size, the number of epochs and a tuple of callbacks as arguments.
    The number of epochs can also be configured in the command line, but throughout the thesis it is left to the default value of $128$.
    The \texttt{fit} method calls the machine learning model function for each batch of inputs in the training sample set. After that the model is validated on the validation sample set.
    This means the loss function is computed on the validation data, which is data that the model is not trained on.
    Validating the model should help to determine how well the model will perform on actual test data, which is also data that the model is not trained on. 
    If the loss function results for training and validation data are similar, it is said that the model generalizes well.
    When the validation step is finished the training loop proceeds with the next epoch and starts the same cycle again by providing the first batch of inputs from the training sample set.
    This cycle is repeated as often as the set value of the epochs.
    The callbacks are invoked after each completed epoch. There were five callbacks added: a ModelCheckpoint callback (saves the model with the best validation loss), an EarlyStopping callback (terminates training if the validation loss has not improved for a configurable number of epochs), a TerminateOnNan callback (terminates the training when a nan loss is encountered), a ReduceLROnPlateau callback (multiplies the learning rate by a configurable factor after no improvement of the validation loss for a configurable number of epochs) and a TensorBoard callback (saves TensorBoard log data for eventual later inspection).
    The default number of epochs used in this thesis for the EarlyStopping callback is $5$.
    Another important callback is the TerminateOnNan callback, which terminates the training loop if the loss evaluates to nan.
    This can for example happen when the loss function diverges towards infinity, therefore if the exploding gradient problem appears. 
    It may also be the case that there is a division through zero somewhere in the computational graph, which may also lead to a nan loss.
    The term nan just stands for \texttt{not a number}.
    As all benchmarked models are trained until convergence in this thesis, the ReduceLROnPlateau callback is especially important.
    The corresponding default parameters are a learning rate factor of $10^{-1}$ and a default number of epochs equal to $2$, both of which are used throughout all benchmark invocations.
    The EarlyStopping and the ReduceLROnPlateau do not see an improvement if the absolute change in the validation loss is less than $0.0001$.
    This minimum delta can also be configured via the command line, but this thesis uses the default value throughout all benchmarks.
    Furthermore, all these parameters are configurable by passing alternative values in the command line.
    After the training loop has terminated, another unix timestamp is taken to compute the total training duration.
    \subsection{Evaluation}
    The model is then evaluated using the parameters that led to the smallest validation loss during the whole training loop.
    Evaluation means that the model function is applied to the test sample set inputs and the resulting loss function result on that inputs is saved.
    The created model also provides an \texttt{evaluate} function, which takes the test sample set a batch size and another callback tuple as arguments.
    The only callback passed in the tuple is the TensorBoard callback already used in the \texttt{fit} method invocation.
    \subsection{Data Processing}
    The return values of the \texttt{fit} and \texttt{evaluate} method invocations now contain information about the means of the loss function results and of the metric function results on training, validation and test sample set.
    The means for the training and validation sample set are available for each training epoch together with the currently applied learning rate.
    All of that information is automatically accumulated in a single csv file per model for the training and the testing process. 
    The testing results of all models are also merged in a single csv containing all model results for a single benchmark.
    Data that was generated during training is automatically visualized by the benchmark base class and will be presented in a future chapter that discusses the benchmark results in more detail.
    Of course all generated files will be stored in their respective directories.

    \section{Activity Benchmark} \label{activity}
    As described in the benchmark base class, all benchmarks feature time-series data where the model output is only used after the last time step to compute the loss function.
    This benchmark uses a slightly modified person activity recognition dataset from the UCI repository \cite{UCI}.
    The mentioned dataset was distributed under the \url{https://archive.ics.uci.edu/ml/machine-learning-databases/00196/ConfLongDemo_JSI.txt}.
    The target function to learn is to map a sequence of measurements from four inertial sensors worn on the person's arms and feet to an activity classification.
    This benchmark should test a model's capability to model dynamical physical systems and understand what motion patterns belong to which class.
    The ability to capture long-term dependencies is not tested with this benchmark as the most recent input vectors should be enough to make good predictions.
    At each time step only the measurement of a single inertial sensor is presented as input to the model.
    The model can differ between the individual sensors as the modified dataset of person activity has a one-hot encoding to mark the sensor from which the current measurement is coming.
    All benchmarks feature an additional time input, where the time interval since the last input is passed on to the model if the feature is activated.
    However, this thesis has not used an additional time input for any benchmark.
    All the measurements used for this dataset were stored in the file \texttt{activity.csv} located in the supplementary data folder described in the benchmark framework section.
    The dataset is annotated with an activity classification for each time step, this benchmark however only requires the model to predict the classification corresponding to the last measurement data received.
    As the benchmark is a classification task a categorical cross-entropy loss was used that was computed from the output logits of the model.
    A categorical accuracy metric is used in this benchmark better judge how accurate the model predicts the activity class annotation corresponding to the last measurement input.
    Each model had an output vector size of seven, as there were seven different activity classes with their respective indices in brackets: lying ($0$), sitting on a chair ($1$), standing up ($2$), walking ($3$), falling ($4$), on all fours ($5$) and sitting on the ground ($6$).
    The processing of the UCI dataset was similarly done as in \cite{ODELSTM}.
    The benchmark had a configurable sequence length, maximum sample amount and sample distance.
    For this thesis, a sequence length of $64$, a maximum sample amount of $40000$ and a sample distance of $4$ was used.
    This means that each model gets a history of $64$ measurements before it has to predict the activity corresponding to the last measurement.
    The maximum sample amount should bound the number of samples and in the case of $40000$ and a sample distance of $4$, there were enough entries in the dataset file, so the benchmark was run with $40000$ samples in total.
    The sample distance is the indices offset in the dataset file between two drawn sample sequences.
    A model will get a sequence of $64$ input vectors of size seven that look like: $[0,0,0,1,4.3,1.8,0.9]$.
    The first four entries in that vector represent the one-hot encoding that describes from which one of the four sensors the measurement data was taken.
    The remaining three entries contain the x, y and z coordinate of the corresponding sensor.
    The required output vector has just one entry as it is just the index of the corresponding activity class with the mapping as described above.
    As this is a sparse class encoding, the framework has to extend this output value to a one-hot encoding to apply a cross-entropy loss between the extended one-hot encoding and the output vector of our model after a softmax function was applied.
    The softmax function is necessary to convert the so called output logits to an output probability for each class.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/activity_benchmark.py}.
    
    \section{Add Benchmark} \label{add}
    This benchmark uses a the same structure as the add benchmark introduced used in \cite{UnitaryRNNs}.
    Data for this benchmark is generated randomly at each instantiation of the benchmark.
    The target function to learn is to simply add two numbers that are marked in a much longer stream of numbers.
    At each time step a number together with a marker bit is presented as input vector to the model.
    As in the Activity Benchmark \ref{activity}, the sequence length and the sample amount are also configurable.
    For all models a sequence length of $100$ and a sample amount of $40000$ was used.
    As described above, the input vector has size two.
    The second entry is set to one only in one input vector of the first and last $50$ input vectors.
    Their distribution is uniformly across the whole first and second half of the time-series.
    In all other input vectors this second entry is set to zero. 
    The first entry of all input vectors is filled with random numbers taken independently and uniformly from the interval $[0,1)$.
    A single input vector out of the $100$ input vector each model gets during the benchmark looks like: $[0.5,1]$.
    In this example the random number is $0.5$ and it is marked, as the second entry is one.
    As described there are only two marked numbers and the expected output vector has size one and is simply the addition of both marked numbers.
    This benchmark simply uses the mean squared error loss function, as the smaller the mean square error is, the more similar the expected and the model output will be.
    Furthermore, there is no metric used in this benchmark.
    As this benchmark uses an increased sequence length of $100$ and as described the error signal is only provided after the last input vector, the model will be only able to learn this function when it is able to capture long-term dependencies.
    This means the model function must be designed in a way such that the gradient does not vanish or explode during backpropagation through the model's function.
    These problems was discussed in detail in chapter \ref{long_term_difficult}.
    When the model is not able to capture these long-term dependencies, therefore it is not able to store seen marked values in its state, the model will be forced to learn the naive memory-less strategy of always predicting one.
    This is the case as the expectation of each individual number out of the two marked ones is clearly $0.5$, as they were drawn uniformly from the given interval.
    An addition of both expectation values reveals the output of the memory-less strategy.
    As also pointed out in \cite[p. 6]{UnitaryRNNs}, this naive strategy will lead to a mean squared error of $\frac{1}{6}$.
    This can be verified as the mean squared error when constantly predicting the mean is equal to the variance of the distribution.
    As both random numbers were picked independently of each other, the variance of the distribution of the sum of both random numbers is just the sum of their individual variances.
    The distribution from which the random numbers are drawn has variance $\frac{1}{12}$.
    Therefore, adding this value to itself proves the mean square error of the memory-less strategy.
    For this benchmark, the model output vector size is simply one, as it should just contain the sum of both marked numbers.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/add_benchmark.py}.

    \section{Walker Benchmark} \label{walker}
    This benchmark evaluates how well a model can predict a dynamical, physical system's behavior and was taken from \cite{ODELSTM}.
    The training data is acquired simulation data of the \texttt{Walker2d-v2} OpenAI gym \cite{OpenAIGym} controlled by a pre-trained policy.
    The objective was to learn the kinematic simulation of the MuJoCo physics engine \cite{MuJoCo} in an auto-regressive fashion using imitation learning.
    To increase the task difficulty, the simulation data was acquired from different training stages of the pre-trained policy (between 500 and 1200 Proximal Policy Optimization iterations) and 1\% of actions were overwritten by random actions.
    Furthermore, the benchmark implements eventual frame-skips that would create an irregularly sampled time-series. 
    This feature was not used in this thesis as it covers only regularly sampled time-series.
    Only if the model understands the dynamics that are guided by differential equations, it will be able to produce accurate predictions.
    The ability to capture long-term dependencies is not tested with this benchmark as the most recent input vectors should be enough to make good predictions.
    The benchmark had a configurable sequence length, a maximum sample amount and a sample distance just like the Activity Benchmark \ref{activity}.
    Throughout the thesis a sample length of $64$, a maximum sample amount of $40000$ and a sample distance of $4$ was used.
    All parameters have the same meaning as before.
    As there were enough training data provided in \texttt{.npy} files by the creators of \cite{ODELSTM}, the benchmark had $40000$ different samples available that are partitioned in training, validation and test samples.
    The acquired simulation data can be downloaded from \url{https://pub.ist.ac.at/~mlechner/datasets/walker.zip}.
    The input sequence consists of input vectors of size $17$, which contains the current state of the physics engine at this specific time step.
    These values represent the angles of the joints and the absolute position of the bipedal robot.
    The function to learn for this benchmark is to predict the physics engine's state in the next time step by giving the machine learning model a history of the past $64$ physic engine's states.
    Therefore, the model output vector size was set to $17$ and the expected output data were also vectors of size $17$.
    As both vectors have the same size and the more similar they are, the better the prediction is, a mean squared error loss was used.
    There was no metric used for this benchmark.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/walker_benchmark.py}.

    \section{Memory Benchmark} \label{memory}
    This benchmark evaluates how well a model can capture long-term dependencies by letting the model recall past seen categories exactly.
    It is a slightly changed version of the copying memory problem described in \cite{UnitaryRNNs}.
    Input data of the benchmark input is randomly created at each invocation of the benchmark.
    There is a configurable memory length to test for, a length of the sequence to memorize, an amount of categories and an amount of samples that are randomly generated.
    Throughout the thesis a memory length of $100$, a sequence length of $1$, a category amount of $10$ and a sample amount of $40000$ were used.
    Each single input vector sequence is created by concatenating three sub sequences.
    The first sequence is the sequence to memorize of length $1$.
    It contains category indices sampled uniformly from $0$ to $9$.
    The second sequence is then just a sequence of the filler symbol $10$ repeated $100$ times.
    The third sequence is just the index of the category in the sequence to memorize that the model should recall, which is also sampled uniformly from all available indices in the sequence.
    This sequence is obviously of length $1$ and always filled with $0$ in the case of the predescribed setup.
    In total this makes up for a total sequence length of $102$ and a vector size of $1$ per time step. 
    The expected output category is encoded sparsely as in the Activity Benchmark \ref{activity} and contains a category index from $0$ to $9$ that matches the category at the index the model got at the last time step in the sequence to memorize.
    The output vector size of the model is $10$ and each output logit represents a  single category.
    As this is a classification problem, a categorical cross-entropy loss was used between the output logits of the model passed through a softmax function and the one-hot encoding extension of the sparsely encoded expected category index.
    To better visualize how good a model can actually recall the category, a categorical accuracy metric was added to this benchmark.
    It must be pointed out that a model is only capable of recalling the category seen in the first input vector if the gradient does not vanish or explode, as the error signal is only provided after the last time step.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/memory_benchmark.py}.

    \section{MNIST Benchmark} \label{mnist}
    This benchmark evaluates how well a model can capture long-term dependencies, as for correct classification the model needs to incorporate input vectors from the distant past, which will be described below.
    The idea to incorporate this benchmark was taken from \cite{ODELSTM}, which also features an event-based sequential MNIST classification problem.
    Input sequences for this benchmark were constructed from the MNIST dataset of the Keras framework \cite{Keras}.
    The MNIST dataset contains images of hand-drawn digits of size $28$ by $28$ pixels where each pixel is encoded by a single integer from $0$ to $255$.
    All images are in grey-scale and a higher integer represents a darker pixel.
    The images were vectorized to a vector of size $784$ and then split up to a sequence of vector chunks of size $8$, which results in an input sequence length of $98$.
    The expected output class index is just the digit the current image is representing.
    Furthermore, the benchmark has a configurable maximum amount of samples, which was set to $40000$.
    As the MNIST dataset had enough image samples, all specified $40000$ samples were used.
    A long-term memory of seen input chunks is indeed necessary to produce an accurate category prediction, as digits like $1$, $4$ and $9$ may be indistinguishable when only considering the most recent seen input chunks.
    This corresponds to classifying the image only based on a lower fraction of the image visible to the model, where the upper fraction was cut away.
    A model that yields accurate results must not suffer from the vanishing or exploding gradient problem, as only then the whole picture can be taken into account for classification. 
    The model output vector size was set to $10$, as each output logit should represent a single digit.
    As the expected output digit is encoded sparsely, the same procedure as in the Memory Benchmark \ref{memory} is applied to compute the categorical cross-entropy loss.
    The performance of the models was also measured by using a categorical accuracy metric, which produces a more human-interpretable result than the chosen loss function.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/mnist_benchmark.py}.

    \section{Cell Benchmark} \label{cell}
    This benchmark evaluates if the newly introduced memory cell architecture is able to store a single bit of information repeatedly including switching the memory state.
    Furthermore, it should be checked if the memory state vanishes or if it successfully persists over a long time horizon.
    This requires the ability to capture long-term dependencies as the input is provided sparsely to the model as described below.
    The benchmark has a configurable memory high symbol, memory low symbol, memory length, amount of cell switches and amount of samples that are generated at each invocation of the benchmark.
    The memory high and low symbol represent the expected output symbol when either memory state is active but the memory high symbol is also used as input symbol to sparsely activate a specific memory state, all other inputs are then set to the memory low symbol.
    The memory high symbol was picked to $1$, the memory low symbol was picked to $0$, the memory length was picked to $128$, the amount of cell switches was set to $2$ and the sample amount was set to $40000$.
    As the memory cell is a bistable memory element, there are two memory states that can be activated sparsely.
    The input vector at each time step has size $2$. If both entries are $0$, the current memory state should be kept.
    Otherwise, if a single entry is $1$ and the other entry is $0$, the corresponding memory state should be activated.
    The first part of the input sequence is constructed by activating any of the memory states sparsely as described above and the succeeding $127$ vectors are all-zero vectors.
    This sub sequence now has length $128$. The next sequence is built like the first one, but it activates the opposite cell state at the first time step, which corresponds to a cell switch.
    There are $2$ further sub sequences of this kind. The final input sequence is then the concatenation of all three sub sequences and has length $384$.
    In half of the samples either memory state is activated first in the concatenated sequence.
    The required model output vector is also given as a sequence of vectors of size $2$, therefore the error signal is provided at each time step.
    The output sequence can be easily built from the input sequence by continuing to set its entry to $1$ at the corresponding index until a new sparsely input is provided to the model.
    Therefore, the model may get the input sequence consisting of the following vectors: $[1,0],[0,0],[0,0],...,[0,1],[0,0],[0,0]$ and is required to produce the following vectors of the expected output sequence: $[1,0],[1,0],[1,0],...,[0,1],[0,1],[0,1]$.
    The sparse activation of the memory cell should lead to a permanent storage of the activation, until a new sparse input is provided to the model.
    As described above the model output vector size is $2$ and a mean squared error loss without a metric was used, as more similar vectors lead to a better prediction.
    The results of this benchmark are presented in a later chapter.
    The implementation of this benchmark can be found under \url{https://github.com/Oidlichtnwoada/NeuralNetworkArena/blob/master/experiments/benchmarks/cell_benchmark.py}.
    
    \chapter{Results}

    \chapter{Summary}
%\todo{Enter your text here.}

% Remove following line for the final thesis.
% \input{intro.tex} % A short introduction to LaTeX.

    \backmatter

% Use an optional list of figures.
    \listoffigures % Starred version, i.e., \listoffigures*, removes the toc entry.

% Use an optional list of tables.
    \cleardoublepage % Start list of tables on the next empty right hand page.
    \listoftables % Starred version, i.e., \listoftables*, removes the toc entry.

% Use an optional list of algorithms.
%\listofalgorithms
%\addcontentsline{toc}{chapter}{List of Algorithms}

% Add an index.
    \printindex

% Add a glossary.
    \printglossaries

% Add a bibliography.
    \bibliographystyle{alpha}
    \bibliography{thesis}

\end{document}