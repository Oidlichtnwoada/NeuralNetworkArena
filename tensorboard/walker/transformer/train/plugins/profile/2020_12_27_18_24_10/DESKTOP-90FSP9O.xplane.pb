
ƒ¥	/host:CPUΎF΄^μΤ
"μ ΧΚΛΐΩμΙ¥"8" "λΰ‘Α­ τΗ™"κ γΧι€φΦ‰™"ι ·ΉκΰίΦ™"ζΐ‚λΐ¬Νρ
"ε ΧΖπ Β"εΰ—™ΐΟ$"εΰή¶›ΐ"εΐΓΐ"ε ‹·ΐ"ε€’ΐ"ε θ’ΐ"εΰ΅θΐ"εΰΐ±  "εΰ”“΅ "ε Α“£ "εΐΆϋ£ "ε ιΦ¤ΐ"εΐΣΈ¦ΐ"ε ”§ΐ"ε Ήέ§ΐ"ε€Λ ¨ "εΰάγ¨ΐ"εΐ£Ώ©ΐΟ$"εΐχ ΐ"ε ‰δ "ε ¨­« "ε ’ή«ΐ"ε€¤΅¬ "εΐ¨ή¬ΐ"	ε€­›­"ε€—Μ­ΐ"εΰ¨® "ε€ Ζ®ΰ§"εΰ±‰― "ε€ήΨ―ΐ"ε€ύ΅°ΐ"εΰε° "εΐ ¨± "εΰΜχ± "ε€Δ®² "ε »ε²ΐ"ε€‚Α³€µ"εΐ»–΄ΐ"εΐΪί΄ "ε€ίµΐ"ε€ώεµ "εΐ‚£¶ΐ"εΐ΅μ¶ "ε€¦©·ΐ"ε€Ες· "ε ρΑΈ "ε ‹Ήΐ"ε€ΆΞΉ€µ"εΰθ©Ί "εΰΫΤ» "ε€Σ‹Ό "εΐΧΘΌΐ"εΰΞΌΐ"ε€Ζ¶½ΐ"εΐΚσ½ "ε€Ο°Ύΐ"ε€ξωΎ "εΰ΄ΥΏ "ε Ή’ΐ "εΰ½Οΐΐ"ε ΒΑ "εΰΖΙΑ "ε Λ†Β "ε€έΙΒ "	εΐα†Γ"εΐσϊΔ "εΐΗάΕΐ"	ε€²Ζ"εΰ’υΖ "ε€¬Η "εΐιΗΐ"ε  ¬Θΐ"εΰ¤ιΘΐ"ε ©¦Ιΐ"εΰ­γΙ "ε ² Κ "ε€ΔγΚΐ"εΐύΈΛ "εΰτοΛ "ε ω¬Μΐ"εΰ²‚Ν "εΐΔΕΝΐ"ε ΦΞ "ε υΡΞ "ε€‡•Ο "ε€ϊΏΠ "ε ρφΠΐ"εΐθ­Ρΐ"εΐ‡χΡΐ"ε€΄ "εΰχ "εΰΌΐΣ "ε ΑύΣ "εΐΈ΄Τ "	ε ΚχΤ"εΐφΖΥΐ"εΐ•Φΐ"ε€ΝΦΐ"εΰ«Χΐ"εΰΚΩΧΐ"ε Ο–Ψ "εΰΣΣΨ "ε€€£Ω Β"εΐΉψΩ "εΐ£©Ϊ "ε µμΪΐ"ε€Η―Ϋ "ε€±ΰΫΐ"εΐµάΐ"ε€ΊΪά "	ε€¤‹έ"εΰµΞέΐ"εΐΗ‘ήΐ"ε ΩΤήΐ"ε€λ—ίΐ"ε€αί€µ"εΐψΞΰΐ"ε€ύ‹αΐ"εΐΙαΐ"εΰψα "εΰ—Ιβ "εΐ©γΐ"ε ΔΙδ "εΐ»€ε "	ε€ΐ½ε"εΐΔϊεΐ"εΰπΙζΐ"ε€™ηΐ"ε€Όβηΐ"εΰΝ¥θΐ"εΰμξθ "εΐώ±ιΐ"εΰυθι "	εΐ‡¬κ"εΰ³ϋκΐ"εΐΕΎλΐ"ε€Κϋλΐ"εΐΞΈμ "ε ΰϋμΐ"εΰδΈνΐ"ε€άον "ε€Ζ ξ€µ"ε ςοξΐ"εΐι¦ο "ε ϋιο "εΰ¦π "ε „δπ€µ"ε £­ρΐ"ε€µπρΐ"ε€ΤΉς€µ"εΰ•σ "ε€’Μσΐ"εΐ–‰τ "ε€›Ζτ "εΰ¬‰υ "εΰ–Ίυ Β"ε€Γ‰φΐ"ε οΨφ "εΰόψ "ε Νψΐ"ε  –ωΰ§"ε Ώίω "ε€ΡΆϊΐ"εΰβεϊΐ"ε€Ϊϋ "εΐήΩϋ "εΰΥό "ε ΪΝό "εΰήύ "ε γΗύ "εΰη„ώ "ε€ί»ώ "εΐγψώΐ"ε€θµ "ε€¦Θ€ "εΐ…ΐ"ε Ϋ‘‚ΐ"εΰΙ‚ "ε ΞΌƒ "ε ν…„ΰ§"ε€΄α„ "ε Κα… "ε€ά¤† Β"ε τ† "εΰ±‡ "ε€ξ "εΐςΥ "εΰ¥‰€µ"ε Ψϊ‰ΐ"ε ¬ά "ε€Ύ‹ΐΟ$"εΰ„ϋ‹ "ε€±Κΐ"ε€Π“ "ε όβΐ"ε ¤¦ΐ"ε€¶ι "εΐΊ¦ "ε Μι "εΰ…Ώ‘ "ε€ύυ‘ "εΐ³’ΐ"εΰβ“ "εΰδ“ "ε †΅” "	ε€δ”"εΰ©§• "ε ®δ• "εΰ²΅– "ε ·ή–ΐ"εΰ»›— "ε€θκ—ΐ"εΰω­ΐ"ε ώκ "εΰ‚¨™ΐ"ε ‡ε™ΰ§"εΰ‹Ά "	ε ί"εΰ”› "εΰώΜ› "ε€φƒ "ε ΆΣΐ"εΐΞΆ "ε ΰεΐ"ε€ς¨ "ε ψ "ε€εΣ "ε€„  "ε€£ζ  "εΐ§£΅ "ε€¬ΰ΅ "ε ¬‘£ "εΰ°Ξ£ΐ"εΐΒ‘¤ "εΐαΪ¤ΰ§"εΐ€¤¥ΐ"ε€…α¥ "	εΐ‰¦"ε ›α¦ΐ"εΐΗ°§ΐ"ε Ωσ§ "εΰέ°¨ "ε€Υη¨ΐ"εΰζ© "εΰΠΫ©ΐ"ε Υ "	εΐΜΟ"εΰΓ†« "εΐΥΙ« "ε€Ϊ†¬ "εΐήΓ¬ΐ"ε€™­ "εΐΦ­ "	ε ®™®"εΰ²Φ®ΐ"εΐω±―ΐ"εΐϋ― "ε Ύ°ΐ"εΰ®ϋ°ΐ"εΐΐΎ±ΐ"ε ² "	ε ρΚ²"ε Ϋϋ² "εΰίΈ³ "	ε δυ³"εΰθ²΄ΐ"ε νο΄ "εΐδ¦µ "ε€ςέ¶ΐ"εΰΈΉ·ΐ"ε ½φ·ΐ"ε€ΟΉΈΐ"εΐΣφΈ "ε εΉΉ "εΐάπΉ "ε€α­Ί "εΰ§‰» "ε ¬Ζ»ΐ"εΰε›Ό "ε κΨΌ "εΐ–¨½ΐ"ε€›ε½ΰ§"εΐΤΊΎΐ"ε€ΩχΎ "ε …ΗΏΐ"	ε€—ΐ"εΐ›Ηΐΐ"ε ­Αΐ"ε ζΒ "	εΰ—Δ"ε ·ΘΗ χ6"ε θ…Κ€κ0"ε Ϋ°ΛΐΟ$"ε€«†Ν€µ"ε “Π "ε ϋ½Ρ "εΐΈΣ "ε€ΎΎΤ "εΰ„Υ€µ"ε€ζΦ "εΐκΎΦΐ"ε ±Χΐ"ε€ψυΧ Β"ε€—ΏΨ "εΐ›όΨΰϋs"ε€τΪ "ε λΡΪΐ"ε€ύ”Ϋ "εΰΨΫΐ"εΐ ›άΐ"ε€¥Ψά "εΐ©•έΐ"ε »Ψέ "ε€‚΄ήΐ"εΐ»‰ί "εΐΪί "ε€ίΰΐ"εΰπΰΐ"ε€Άαΐ"εΐ΅ία "εΐΐ¨β "ε€ϊύβ€µ"ε ¦Νγ "ε€Έδΐ"εΰΙΣδ "εΐΫ–εΐ"	ε νΩε"ε Χζ "εΐΞΑζ "ε ΰ„ηΐ"ε€ςΗηΐ"ε ιώη "εΰν»θ "ε€‹ι "εΐΘιΐ"ε °‹κΐ"ε ΟΤκ "εΰΣ‘λ "ε ΨΞλΐ"ε€κ‘μΐ"εΐξΞμ "εΐαωνΐ"εΰΙξ "ε ’†ο "εΐ‰½οΐ"	ε€ϊο"ε€­Γπΐ"εΐ±€ρ "ε€¶½ρ "ε€Υ†ςΐ"εΐάςΐ"εΐ­¥σΐ"ε Ώθσ "εΰΓ¥τ Β"ε€πττ "ε η«υΐ"ε€ωξυ€µ"εΐ²Δφ "ε Δ‡χΐ"εΰΘΔχ "ε Νψ€µ"ε μΚψ "ε ‹”ωΐ"ε€Χω "εΰ®ϊΐ"ε θοϊ "εΰμ¬ϋ "εΐώοϋΐ"ε ³ό "εΐ‡κόΐ"εΐ¦³ύ "ε€«πύΐ"εΰΌ³ώ "εΐΞφώ "ε€Σ³ΐ"εΐ‰€ "εΰƒΐ€ "ε€ϋφ€ΐ"ε °ΐ‚ΐ"ε€Βƒƒΐ"εΐΖΐƒ "ε Ψƒ„€µ"εΰ‘Ω„ "εΰ°Ά…ΐ"εΐΒε…ΐ"εΰξ΄† "εΰώ† "ε ’»‡ΐ"ε€¤ώ‡ "εΐ¨»ΐ"εΰς Β"ε ΩΗ‰ "εΐΠώ‰ "	ε —Ϊ"ε ‹‹ΐ"εΰ…Θ‹ΐ"ε … "εΐΌ "ε€†ωΐ"εΰ—Όΐ"ε ωΐ"εΐΘΘΰ§"εΐη‘ "εΰ“αΐ"εΰ²ΐ"ε μ "ε ‹Ι‘ΐ"	ε ’’"εΐΦα’ΐ"ε θ¤“ΐ"	ε€ϊη“"	ε ρ”"εΐξ” "ε€ΧΓ•ΐ"	εΐ™“—"εΐΈά— "εΰ―“ΐ"εΰΞά "ε Σ™™ΐ"ε ςβ™€µ"ε€„¦ "εΰ•ι "ε ΟΎ› "ε€α "εΰςΔ "ε χΐ"ε€‰Εΐ"εΰ "εΐ¬Λ "ε Ύΐ"εΰΒΛΐ"ε Η  "εΰΛΕ  "ε Π‚΅ "εΐΗΉ΅ "εΐζ‚Ά "ε€λΏΆ "ε —£ "εΐψφ£ΐ"ε€ύ³¤€µ"ε ©ƒ¥ΐ"ε ΘΜ¥ "εΰΜ‰¦ "ε€ωΨ¦ "εΐύ•§ "ε€‚Σ§ "εΐ†¨ΐ"εΰ²ί¨ "ε ·© "εΐ„ΐ"	ε Η"ε ς« "ε€―µ¬ "ε ¦μ¬ΐ"εΐ»­ΐ"ε€Χψ­ "εΐΫµ®ΐ"ε νψ® "εΐ™Θ― "ε «‹° "ε€½Ξ° "	εΐΑ‹±"ε ΣΞ± "εΰΧ‹² "ε€ΟΒ²ΐ"ε ϋ‘³ "εΰΞ³ "ε€χ…΄ΐ"εΰΙ΄ "εΐµ "ε€Ιµ€µ"εΐ£†¶ "ε€¨Γ¶ΐ"εΐ¬€· "ε€±½·ΐ"εΐµϊ·ΐ"ε€Ί·Έΐ"ε€Ω€Ή "ε€ψΙΉ "εΰ‰Ί "ε€ΔΊ "ε ­“» "	εΰ±Π»"εΐΓ“Ό "ε ΥΦΌΐ"εΰ¬½ "εΐ ο½ΐ"ε †”Ώΐ"εΰΡΏΐ"ε ΐΐ"εΰ“Λΐ "ε Α "εΰΕΑ "ε€”όΑ "ε€³ΕΒΐ"εΐ·‚Γΐ"ε€ΌΏΓ "εΐΐόΓΐΟ$"ε€ϊΡΔ "ε ¦΅Εΐ"	ε ΕκΕ"	εΐρΉΖ"εΰθπΖ "ε€ΰ§Η "ε ΧήΗΰ§"	ε φ§Θ"εΰϊδΘ "ε€ς›Ι "εΰƒίΙ "εΐ•ΆΚ "εΐ΄λΚ "εΐΣ΄Λΰ§"εΐςύΛ "εΐέςΡΐ"ε οµ "εΰ¨‹Σΐ"εΐΊΞΣ "ε€Ώ‹Τ "εΰΠΞΤ "ε Υ‹Υ "εΐΜΒΥΐ"εΐλ‹Φ "ε€πΘΦ "ε€’Χ "ε€‚½Ψΐ"ε€΅†Ω "	εΰ²ΙΩ"εΐω¤Ϊ "εΰΪΫ€µ"ε ΙϊΫ "εΐίϊάΐ"ηΐ›Ίλ ΅‚"0θΰδωΰΛΨσ"  "*EagerKernelExecute 0"9β€΄ΩΞ »±©" νιΟτίΛΊΖΣ" "νιΟτίΛΊΖΣ"  "νΐ»ξυ³ΰΒ€"ν€ΕΚω³ ¬O"ξ πΰφΏΐΒ„"ν ΘΐΐΉU"ς Ω΅Σΐΰ‹"ν€έ€θΐΐ"
νΰιΐ"ρΰµΠκΐΰ½ύ
"π ο¥λΐ€”Ιό
"ηΐΔ²ΟΖΰΖ["θ Ι ΡΖ€±Τ•"  "ο€άΨ”Ι ΉΙ"  " "σΰΞ Μΐζ·"ςΐπ†«Μΐρξ)"φ £›·Μ Πμ"υ χό·Μ€ο„"η€ΔΠΜ Β"θΰεΡΜ€ζΥ"  "τ€±ΈΜΐ„="  " "ς€³ΐέΜ —Δ"σ€ο΄„Νΰ“Λ"σ€ΈΙ‘Ν€ΒΆ"ξ „¦Ν Φ"νΐύδ£Ν χ6"σ ϊΝΰ¥Ώ"ξ §®ΝΐΊ™"ν€›½΄Ν€µ"σΰ›ΠΈΝΐ“
"ξ »ϋ»ΝΰκΓ"νΰωοΏΝ€µ"λ€®ΗΝΰΘ°ϋ
"ω€Ύ–ΛΝΰΎςτ
"ψ€¨ΗΛΝ€­―τ
"ηΐέφζΝΐΟ$"θΐόΏηΝΰΣΘΦ
"  "χ ­ΜθΝ «£
"  " "ν€ΫΩΘΨΰ½"νΐΌςΚΨ€κ0"
ν ΞµΛΨ"ξΰ†όήΨΰ¬ζ"ν ‹ξχΨ€µ"νΐα…ΩΐΉU"ν€Ωπ†Ωΐ·"ν€ήΔΆΩΐα"νΐη ¦Ω "ς€Λ«δΩ ήγΐ"όΰ΅¨τΩ ΅ΰ¬"ϋΰυ‰υΩΐ¥μ«"η€„ΤϊΩΰ‘C"θ ΟμϋΩ ›ύ£"  "ϊ€ΤΪύΩ€„Ή"  " ΰτ
tf_ComputeμΤ
"=€ΗΊέ"ΰαώ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ±Νΰ" ο"  "  "=ΐβΏϋ" ζ†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " Ζύ" κα"  "  "=ΐΪ‰€#ΐΛΙ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " υΖ#ΐξm"  "  "=ΰ·–ƒ#€ό¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΐ„#ΰΖ["  "  "=ΰμ…#ΐχη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€΄μ†#€I"  "  "=€‘Θ# ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ξΤ‹#€‰z"  "  "=ΐ£#€σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " µα#€I"  "  "=ΐκ#ΰΨΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	 Ί€‘#ΐ„="  "  "=€Φ’#€…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "
ΐ—”#ΐ·"  "  "=ΰ Έ–#ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€‚ —#ΰΖ["  "  "=ΰ²¬#ΐα" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ‹Λ™#ΐξm"  "  "=ΐΦγ#ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐυ¬›#ΰϋs"  "  "=€ΞΛ# €±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ‡΅#€I"  "  "=€«§#ΐΨ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΌκ#ΰ‘C"  "  "=ΐΈή# Λ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€½› #ΰ‘C"  "  "=ΐ«‰΅#ΰΨΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ•λΆ# ¬O"  "  "=ΐΖχ£#ΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€€Ν¤# ¬O"  "  "= –Ν¥#€σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€¨¦#€I"  "  "= Ύ§#ΰ„ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐψ§# αg"  "  "=€ψ–©#€¨Γ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€‘# χ6"  "  "=€‹«#€’τ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΓΰ«# –€"  "  "=€»Θ­#€±½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ©¶®#ΐ·"  "  "=ΐπΒ°#ΰε¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ·±# χ6"  "  "<€ώω±#ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€θ²#€κ0"  "  "=ΐ΅€³#ΐ·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΫΥ³#€I"  "  "=ΰΦΙ΄#ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " Ϋ†µ#€κ0"  "  "= ―θµ#ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ³¥¶#ΐ„="  "  "=€•·#ΰΟΥ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " Αά·#ΐΉU"  "  "=ΰ™ϋΈ#ΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΖΚΉ#ΰ‘C"  "  "= ‘γΊ# –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΰ‡»#ΐ„="  "  "< Ή¦Ό#ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ΐ°έΌ#€κ0"  "  "<€κ²½#ΐΉU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "!ΐΉΧ½#ΐΟ$"  "  "<ΰε¦Ύ#€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ""ΐΒΡΎ#ΰά*"  "  "<ΐαΏ#€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "# σέΏ#ΐΟ$"  "  "= ’§ΐ#ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "$ όΧΐ#ΰ‘C"  "  "<ΐέΏΑ#€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "% ΊκΑ#ΐΟ$"  "  "< Ω³Β#€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "&€¶ήΒ# Β"  "  "< —ΖΓ#ΐΉU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "'€τπΓ# Β"  "  "<ΐψ­Δ#ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "(€ύκΔ#ΰά*"  "  "<€΄Ε#ΐΉU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ")€†εΕ#ΰ§"  "  "= ύ›Ζ# –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "*ΰΩΖ# χ6"  "  "< »®Η# ¬O" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "+€ΩΗ#€µ"  "  "< Θ#€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ",ΰή΄Θ#€κ0"  "  "=€‹„Ι#ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "-ΰΗΙ#ΰά*"  "  "<€ώ®Κ#ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ".ΰΪΩΚ#ΰά*"  "  "< ”―Λ#€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "/€ρΩΛ# Β"  "  "<€£Μ#ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "0ΰμΝΜ# χ6"  "  "<ΐ³©Ν#ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "1€ƒΞΝ#€µ"  "  "=€ΖΟΨ&€ΕΉ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "2 όάί&ΰσ½"  "  "= Φ΅ι&€™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "3 ‡ίλ& €±"  "  "=€«–ξ&€Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "4ΐΞο& αg"  "  "= ΄Απ& €±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "5ΐΰρ&ΰ‘C"  "  "=ΰφς&€σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "6€£ΰς&€I"  "  "=ΰΤσ& †”
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "7ΐΨΪυ&€²"  "  "=ΰ“ώ&ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "8 π€&ΰ‘C"  "  "=ΐ†€'ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "9ΰύ·€'ΰ‘C"  "  "= μ¥' Λ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ":€³‚'€κ0"  "  "=ΐ΅ο‚'€±½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ";ΐυΠƒ' µΙ"  "  "<€ΆΡ…'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "<ΐ¦†'€κ0"  "  "<€ΰγ†'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "=€Κ”‡' χ6"  "  "<€φ‡'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "> •­'€κ0"  "  "< ι‰'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "?ΐΰΕ‰' χ6"  "  "< §΅'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "@ Ζκ'ΐΟ$"  "  "=€Ψ­‹'ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "A€χφ‹' χ6"  "  "=ΐ°Μ'ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "BΐΟ•'ΰά*"  "  "=ΰϋδ'ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Cΐ¨'€κ0"  "  "=€Ηύ'ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "D€±®' ¬O"  "  "=ΐ' –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "E€¤Ω'€κ0"  "  "<ΰκ΄‘' αg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "FΰΤε‘'ΐΟ$"  "  "< »’'ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "G€λε’'ΰά*"  "  "<ΰ±Α“'€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "H ζ“'ΰά*"  "  "<ΰΊ»”'ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "I€²ς”'ΰά*"  "  "< ήΑ•'€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "J€»μ•'ΐΟ$"  "  "<€Ϊµ–'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Kΰλψ–'ΐΟ$"  "  "=€Θ—' –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Lΐ…'€κ0"  "  "=€ΦΪ'ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "M Ν‘™' χ6"  "  "<ΐ®ω™'ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Nΐ'€κ0"  "  "<ΰΔω'€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "O ”›'ΰά*"  "  "=ΐΐν›' –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "P '€I"  "  "<ΰ‹†'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Qΐθ°'ΐΟ$"  "  "=ΐ‡ϊ'ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Rΐρ'€I"  "  "< ν'ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "S ΧΟ'ΰά*"  "  "<ΐƒ 'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "T ΰΙ ' Β"  "  "=ΐ™΅'ΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "U€‘Φ΅'ΐ„="  "  "< ς½Ά'ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "V άξΆ'ΰά*"  "  "<ΐΎ£'ΐΉU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "W€Ψβ£'ΐΟ$"  "  "=ΰι¥¤'€Ύ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "XΐΖΠ¤'ΐΉU"  "  "<ΐΟΚ¥' αg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Y€ο¥'ΰά*"  "  "<ΐΨΔ¦'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Z€¨ι¦'ΐΟ$"  "  "<€Η²§' αg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "[ΰ£έ§'ΰά*"  "  "<€Π¬¨'€I" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "\ΐΡ¨'ΰ§"  "  "<ΰ–©' αg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "]ΐσ²©'ΰά*"  "  "<ΰ‚' αg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "^ΰ‰³'ΐΟ$"  "  "<ΰ¨ό'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "_ΰ’­«' χ6"  "  "=ΰζ¬'ΰε¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "`ΐ­κ¬'€κ0"  "  "<ΐΜ­'ΐΉU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "a€Ρπ­'ΐΟ$"  "  "<ΰβ³®'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "bΐΏή®' Β"  "  "< Ρ΅―'ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "c€®Μ―'ΐΟ$"  "  "<ΐη΅°'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "dΐΡ°' Β"  "  "<€Φ±'€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "e ΝΖ±' Β"  "  "=ΰΡƒ²'ΰ„ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "f Ά³'€κ0"  "  "< ώƒ΄'ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "g θ΄΄' Β"  "  "<ΰ΅µ'ΐξm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "h ρ®µ'€κ0"  "  "<€Έ¶'€Τa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "i€Ά»¶'ΐΟ$"  "  "<ΰ³ώ¶'€‰z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "jΐΕΑ·' Β"  "  "=ΐδΈ' –€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "k€ιΗΈ'€κ0"  "  "=€¤ύψ* τ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "lΐΊ®ϋ*ΰε¤"  "  "=ΰωµƒ+ΰ£·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "mΐίΪ„+ΰΖ["  "  "=€Ά†+€Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "n€•Υ‡+€I"  "  "=€ς°‰+€Ύ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "oΰƒτ‰+€κ0"  "  "= ε+ΐΨ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "p „Φ+ χ6"  "  "=ΐηΕ+ΰ€“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "q Ζ›+€Τa"  "  "= ω§+ ‰«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "r€…©+ΐΉU"  "  "=ΰ¨Υ«+ΰ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "sΐΊ¬+ΰά*"  "  "=ΐ±ψ1€ί%" λΊ¦ή‚ύΑ"  " "λΊ¦ή‚ύΑ" "tΰ°Κύ1 §ε" "  ">ΰΚ2€±Ή§" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "u€¦γ£2ΐ†Άη"  "  "vΰ€4€—ύ5"  "  "w€γ­Δ4 €±"  "  "= …Ιό: μ½n" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "x€ΘΙ:€ψf"  "  "?ΐΧ¨ΧΣ …©…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "yΰΒΞήΣΰίω"  "  ">€ΎΉέΧΰφέw" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "z€ ΄βΧΰΔάo"  "  "? ξμΥΨΐμµ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "{ΐΨΞΧΨ Ϊρ―"  "  "?€ΑΫ€νο»" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "|ΰϊΫ Ν“·"  "  ">€³™Μά Μά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "}ΰΝΦΝά€Ω€"  "  ">€ρζέΰη¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "~€¬εθέΐΩβ"  "  ">ΐΪνυέ Τ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " Φαφέ –€"  "  "> ¥­ξή ωΎ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΐΪφοή€—“"  "  ">ΰ‘®α ο" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰξ‰Τα έ"  "  "> ωάα µΙ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚€ΦΤέα ¬O"  "  ">ΰΤΕ¬δΐθΏ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒ€Ώ§®δ Λ"  "  "„ †ε±δΐβά"  "  ">ΰΧΟΟε€γΎ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "…ΰ½¥ε Η½"  "  ">ΰΝΖαε έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "†ΰ΅¨βεΰΖ["  "  ">ΰ•­λΐ¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ΐΨ°λΐΗξ"  "  "ΐ—ο¶λΰξ"  "  "‰ ΐΕΌλΐΤΓ"  "  " ΈΑλΰΒ€"  "  "‹ΰφƒΕλ αg"  "  "ΐ§Ζλ χ6"  "  "ΰ§ΑΗλΰ„ξ"  "  "ΰΨώΙλΐ„="  "  "€ΘΛλ€κ0"  "  ">ΐ–Κϊλ€΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰµΔόλΰ€“"  "  "‘ΐπ„μ ο€"  "  ">ΐδύμΰ“Λ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’ •μΐ«"  "  "“ΐΒ’μΐΎτ"  "  "> ψπΩν€ΡΚ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”ΐηρήνΐχη"  "  "? ϊψδνΐ••ΐ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΰΌΘζνΐ“µ"  "  "– ­±οΐΓ“"  "  "?ΐίγ¦ο ‹¦Έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "— Ε¨ο ’®"  "  " §φΧπ —Δ"  "  ">ΰ²ω―ρ „Α" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™€―Ο³ρ Ω±"  "  "€ΉΊρΰ£·"  "  "›ΰσΧΑρΐό†"  "  ">ΰ²®Θρ ¤Ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ώΖΙρ€¤θ"  "  " Έ―Ξρΰϋs"  "  " Κ£Πρΰε¤"  "  "€€ΟΤρΐ¬€"  "  " ΐΛΧρΐξm"  "  "΅ Π†ΩρΐΉU"  "  "Ά ²ήρ€±½"  "  "£€Ιγαρΰ–β"  "  "?ΰ²Φ‹σΰΎΝΘ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ΰωβσ άπΐ"  "  "¥ ΅μΟτ ζ†"  "  "?€ΉΗρτΰΫΓΦ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¦ΰ›ύ½φΐ€β"  "  "§ΰή®Βφ€—“"  "  ">€’μέψ€Δυ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¨ΐΜ…δψ€›ξ"  "  ">ΐί½λψΰ΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©ΐ³μψΰ¨Φ"  "  "ΐςυςψΰΛϊ"  "  "«€οόχψΰΎ¥"  "  ">€Φ–€ωΰ©Ο#" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬ΐΥωΐΤΓ"  "  "­€κ’ω€I"  "  "®ΰΟ·ω κα"  "  "―€Μ’ωΰ°"  "  "°€τΠ“ωΐ¬€"  "  "± Ε–ω αg"  "  "² ³Ψ›ω ϊ"  "  "³ ƒίω …Π"  "  "?€χ¤ωΐΠΙ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄€Λώ¤ωΰτ"  "  "µΐ®²ω ¬O"  "  "¶€ΌΦ³ωΰβ	"  "  "·ΰ§ή½ωΐξm"  "  "Έ ΖΏω€Ύ’"  "  "Ήΰ–®Αω Β"  "  "Ί€ψ•Βω ¬O"  "  "»ΐΊεΓω ¬O"  "  "Ό  Εω χ6"  "  "½€ΪΗωΰ—¦	"  "  "Ύΰ―ΙΡωΐ„="  "  "Ώΰι€ΥωΰΓΔ"  "  "ΐΰ­φήω€φ‹"  "  ">€λ³ξϊΰγ†'" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΑΰΊ‰πϊ€π“	"  "  "Β «ϋϊΐ¬€"  "  "Γΐ—ιϋΐΤΓ"  "  "Δΐ³ϋ  Ύ"  "  ">ΐ²¤ϋ€Ζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ε€‹«¥ϋΐΤΓ"  "  "Ζ ρ±©ϋΐΛΙ"  "  "Η€½¬­ϋΐ„="  "  "Θ€Ζ¦®ϋ€κ0"  "  "Ι ϋο―ϋ€’τ"  "  "Κ€§²ϋΐ„="  "  "Λ€…ύ΄ϋΐ„="  "  ">ΐ‡ηΒϋ€­Μ5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ€ΰ…Δϋΰη¬"  "  "ΝΰκΣϋ€®Ϋ""  "  "> °§ωϋΐΥ!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξ ΨκϊϋΐύΚ
"  "  "Οΰ²‘‡όΐ™ύ"  "  "?€Ε©µύΐΈζ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π ΞΤ·ύ€κ„b"  "  "ΡΰνΣώ Ξc"  "  " …†ΰρΎt"  "  "ΣΐξΓό€²©Λ"  "  "Τ ¨ήΰΛ―"  "  "Υ€Ό§ώΐΈ°D"  "  "Φ ΫΐΙ‚ΰ½€I"  "  "ΧΰΛΥƒ€Ότ&"  "  "Ψ Κ™Ηƒΐ·‚"  "  "?ΰµΐΦ†ΐ―ρ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ω —ΩΨ†€„ΈΩ"  "  "Ϊΰ©Ή υγ"  "  "> χµ‰ΐΰΎ-" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϋ Όλ·‰ΰ€Θ"  "  "ά ε§Χ‰€Νο
"  "  "?ΐπΦ‰ΐΈ„¦" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έΰ·” Ιƒ΅"  "  "?ΰ•¨Έ«ΰ¥¥γ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή ¤£Ώ« Οόx"  "  "ί “όΌ¬€›Ψ3"  "  "ΰ€Βό¬€®Ϋ""  "  "αΐ©Π­ΐρ£B"  "  "βΰ®Ών­€έΫ"  "  "γΰθφπ­€Ύ’"  "  "δΐ«χσ­€§΄"  "  "εΐΫ¥‰®ΰΊΚ"  "  "ζ ΊΨ”® —Δ"  "  ">ΐϊκ®°€ΔΙk" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "η€άƒ±°ΰΌρ["  "  "θΰί±ΐύΚ
"  "  "?ΰΊ»Ϋ³€ΞΈ―" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ιΰΏΪή³ΰά‡ά"  "  "κ Ύ¦¶€©`"  "  ">ΐτΤ•» “½j" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "λ€Φν—» δZ"  "  "μΰ—ώφ»ΰΩ“"  "  "?€¦ήΌ€ζτ€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ν ώίΌ ω­Ϋ"  "  "ξΐΪφΓΐ ΄Υ‚"  "  "οΰγρΜΓΐ¤™"  "  "π€ΖΞΖ€„ƒΑ"  "  "ρΐ•ΥΗΐν²u"  "  "ςΰϊζΜΘ Νή"  "  "σ€¨ΗΚ ΌΌ»"  "  "τΰΘΜήΠ ΜΖ7"  "  "υΰ„φΡΐη„…"  "  "φ Εϋ¦ “½j"  "  "χ€Ά―΅Σΰ'"  "  "ψΰ»’ΞΣ€±±"  "  "ωΰ‘ο„Υ€Ζ²-"  "  "ϊ Φ°ΒΥΐƒ®"  "  "ϋΰ—ρΦΥΰοβ"  "  "?ΐ’¦ρΨΰ¨ώΘ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "όΰ›ΡσΨ ©«³"  "  "ύΐΑƒ¬Ϊ€”ό"  "  ">ΐ°ά©Ϋ€αΥH" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώΐχθ«Ϋ Ί"  "  "ΰ·ιΡΫΐ›…"  "  "? ¦ίέΰ³α‹" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ Σ€δέΰή»…"  "  "? φ΄Έ… ιΡ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰιΑΌ…ΰΌϊΥ"  "  "‚ΐ†—‡ΐƒD"  "  "ƒ€ζ“ε‡€μν#"  "  "„ ώΠ‰€ΡQ"  "  "…ΰκαΰ°"  "  "†ΐΊΰβ ¬O"  "  "‡€“γ •ρ"  "  "ΰ–ήψ€ΏΦ"  "  "‰€‡£ƒ‰ΐ¤Κ"  "  "?ΐΠ‹ΐ—ΟΜ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€΄­‹€΅Ϊ…"  "  "‹€‚Ύ· ηιR"  "  "ΐΑ ²ΌΗ"  "  "ΰώΣέ“€ΦU"  "  " ΊΎ” ΑΔ+"  "  "€¦Βν”€ΡΚ"  "  "?€€Βςΰ®χ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΛΌφ οΎ—"  "  "?ΐΠƒω΅ΐςΐΘ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘€²ϋ΅ΐ’ί~"  "  "’€ƒπύΆ γΓm"  "  "“ ΦΗτ£ Όπ"  "  "”€¶Ύ…¤ΐΑυ,"  "  "•ΐ±Ώ—¥€ϊπX"  "  "– έ€χ¥ΰ§ό0"  "  "— ­ρ¦ΰΟΥ"  "  "9 ΩΔΒ¦€κε" νιΟτίΛΊΖΣ" "νιΟτίΛΊΖΣ"  Ρ@Ψm
tf_ComputeμΤ
"=ΐ€ρ"ΰ¬ζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™ΐΨυ" Τ’"  "  ">€έσ#ΐΎ‹" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " Ύ„–*ΐχη"  "  "=ΰ—ΡΝ*ΐ…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "›ΐΡΧΟ*ΰχ"  "  "= ΫδΤ*ΰ‰" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐφƒΩ*€I"  "  "=€ΟΆΪ* —Δ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Ί—ΰ* χ6"  "  "? οΞΰΣ€ϊ¦µ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΘγΣΰ¤Π­"  "  ">ΐΏ—Ϊ Οόx" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€¨››ΪΐΡ€m"  "  ">ΐώΫΐΊνg" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "  Ύ¤–Ϋ °ώ_"  "  ">ΐϋΛΦάΐ΅³" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΅ΐΐΨάΐΜ"  "  ">€‡°εάΐ‰ά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά θ—ζά σΫ"  "  ">ΐΚοάΰά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£€‡πάΐΛΙ"  "  ">€Έιί Ν=" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤€•Εί€‰z"  "  "¥ΐΧ”ίΐΠ6"  "  ">€βΚίΰχ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¦ΐ›τΚί€‰z"  "  ">ΐΚΝί€…σd" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ΰψ€ΞίΰΠ™"  "  "¨ΐόΥί ι"  "  "©ΐ’λίΰ£μ"  "  "€ΞΑ†ΰ χ6"  "  "«€΄—‰ΰ Κ"  "  "¬ΐΨαΰΐχη"  "  "­€¤«’ΰΐϋχ"  "  "® όΝΰΰε¤"  "  "―ΐζ―¬ΰ €±"  "  "°ΰΠ‘®ΰΰ‘C"  "  "±ΰ¤σ®ΰ χ6"  "  "²ΰ­ν―ΰΐ„="  "  ">ΰΘΫ²ΰΐσ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³€ΐ’³ΰΐ¬€"  "  ">€δϊ¶ΰ€­β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄ΰυ½·ΰ ¨τ"  "  ">€Ζυ»ΰΰχ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µΰΆ Όΰ €±"  "  ">ΰς¦Ώΰ€λτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶ έΉΒΰΰ‘C"  "  "·€ΖΓΰ€µ"  "  ">ΐ…®Εΰΐ…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΈΐοήΕΰΐ–±"  "  ">ΐΪΣΛΰΰχ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ήΐ®µΜΰΰ„ξ"  "  ">ΐ‘»γβ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ίΐ„ζδβΰ°"  "  "»ΰ¬Ϊηβΐζ·"  "  ">ΰΔ±ςβ Θ¶!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό€ρ€σβ ¬O"  "  "½ΐΙτβ€­—"  "  ">€Β€”γ ¨τ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ύΰά”γ€Τa"  "  ">€ν™δι€“Ά8" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ώΐ™ζι€ψΙ"  "  "ΐΰ§δλι ¬O"  "  "ΑΐΛ›ξιΐ ο"  "  "Βΰηώχιΰϋs"  "  "Γΐ‚Όωι€¬Σ"  "  "Δ ‹Ί‘κΰ°"  "  "Ε ³ύ’κ€σ"  "  "Ζ€Κί–κ αg"  "  "Η •ψ—κΰά*"  "  "Θ ιΩκ€I"  "  ">ΰάζκ β«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ι σ—κΰξ"  "  ">ΰζ¤£κ€Μ«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ€ήΫ£κ Η½"  "  ">ΐΪβ¨κΐκ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΛΐΔ“©κ €±"  "  ">ΰμ‡¬κΐµϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΜΰΦΈ¬κΰά*"  "  "Νΰ­κΐΟ$"  "  ">€Κ”―κ€ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξΰ¦Ώ―κΐ–±"  "  ">ΐΉΖµκΰ¬±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ο€έΜ¶κΰ„ξ"  "  "> ο―ΜλΐξΆ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π€Ώ…Ξλ ›"  "  "ΡΰΘ’Σλ€‰z"  "  "ΐγΟΤλ€Ύ’"  "  "Σ€ΠΦλΐ£†"  "  "Τ ϊ±Ψλΰά*"  "  "Υΰ³‡Ωλ€Τa"  "  "Φΰ•‚ήλΰΟΥ"  "  "Χΰ΅αλΐσ"  "  "> ­χελΐ›Ί6" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ψΰ±΄ζλ "  "  "Ω Ϋςλ€™"  "  "ΪΐΛβϊλΰΧ	"  "  "Ϋ€”•…μΐ„="  "  "ά ΆίμΰΖ["  "  "έ€ϋ®μΐξm"  "  "ήΐεΑμΐ·"  "  "ίΰ„Ό’μΰΛϊ"  "  "ΰ λσ—μ€±½"  "  "α ΪΓ›μ€I"  "  "> ΠΊ­μ ½΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β Ν®μΐ€β"  "  "γ ²µ²μΰΖ["  "  "δ ¥ΰ³μΰΖ["  "  "εΐγ£¶μΰ°"  "  "ζ Έμ€I"  "  "ηΐ³Ήμ χ6"  "  "θΰƒβ½μΐΨ"  "  "ιΐΠΏμΐµϊ"  "  "?ΐχΓμΐΈ„¦" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "κΐ–ιΓμ€ "  "  "λΐΨΚμΐ„="  "  "μ ωφΜμΰ‡ο*"  "  "ν€ΐΌώμΰΨΟ"  "  "ξΰμννΐ’Φ"  "  "ο€ϋ·‡ν€οΟ"  "  "πΰϋΚ‹ν€Τa"  "  "ρΰξυνΰά*"  "  "ςΰα νΐ„="  "  "σ€ψ νΰά*"  "  "τΐ—Μ’ν€φ«*"  "  "υΰαΏν€Τa"  "  "φΐ¶Γνΰ¤ϋ"  "  "χΐ¦“ΝνΐΨ"  "  "ψΰεΥν€’τ"  "  "ω€ΩφΧν€I"  "  "ϊ€“®Ϋνΐχη"  "  "ϋ€Νήνΰξ"  "  "όΰΰβνΐέ½"  "  "ύ —θν αg"  "  "> ϋΌκνΐϊ³" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώΐ‘½λνΐΎτ"  "  "ΰ–πνΐκ’"  "  "€€έτνΐ­Δ"  "  "> ςχξ€…‰4" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΚ–ƒξΰ–β"  "  "‚ΐ‹ξ€‰z"  "  "ƒ ψΪξ€¶ά"  "  "„ ρθ”ξ €±"  "  "… Χ—ξΰ¶ο	"  "  "†ΐΠύΆξΐΨ"  "  "‡ΐγµξ€›ξ"  "  " ƒ’―ξΰό·"  "  "> α±¶ξ€€κ0" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‰ ΎΈξ Δ¦"  "  "€―΄Εξ ¬O"  "  "‹ΰώ‰Ηξΐ ο"  "  " ’¤Ρξ ¬O"  "  "€»ϊΦξΐ‹δ"  "  "> ΛΜηξ€ι‹D" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΨƒιξΰψά"  "  "ΰ†σξ€¨7"  "  "?ΐ•ΐ¬οΐϊ―·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΊ­ο€Τ¨"  "  "‘ ΗΧΏρ Ρ°""  "  "?ΰλ‹χρΰ¦ιθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’ΰύψρΐΎή4"  "  "“ΐΌ―ςΐ‹Ξ?"  "  "” ΄φςΰ©χζ"  "  ">ΰόΐ’υ πΔ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "• Ώ”υΐ¬€"  "  "–ΐΔΰυΐΎτ"  "  ">ΐΆ‰φΰµ«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—€ρ›φ€’τ"  "  ">ΰ‘φΐκΗ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰελφ€’τ"  "  "™€ΩΗΆφ€‰z"  "  "ΰψ£§φΐ’Φ"  "  "›ΰΔΟ¬φ€Π†"  "  "ΰΌ™±φ χ6"  "  "ΰϊ«²φ€κ0"  "  "ΰνΦ³φΰξ"  "  "ΐϋΎ¶φ χ6"  "  " ΰδΉφΐΉU"  "  ">ΰ·―ΖφΰΖϊI" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΅€φςΘφΰθπ"  "  "Άΰ»ΏάφΐΎτ"  "  "£€‰ΰθφ ϊ‚"  "  "¤ΰνυόφ αg"  "  "¥ΐ–Μ‚χ€‰z"  "  "¦ΰΛ•„χΰ‘C"  "  "§ΰό†χ ¬O"  "  "¨ Υρ‡χΰξ"  "  "© ·μχ ϊ"  "  "ΰ·Ξχΰ‘C"  "  ">ΐΰΜΨψ€Έδ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«€χύΪψΐζ·"  "  "¬ΐο©βψ β«"  "  "?ΐτύύψΐ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­€‚µψΐβξϋ"  "  "®ΐ‘ώϊ ό"  "  ">ΐ°Ψϊϋ ±ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―ΐ£ƒόϋΰΉ†"  "  "> ζƒϋ ζ»" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "° …Νϋ€I"  "  "± Γί€ό€Τa"  "  "²€η–ƒό€Γ±"  "  "³ΐ·ό Λ"  "  "΄ΰ†όΰ–β"  "  "µΰΚθ‘όΰχ"  "  "¶ ’Χ–ό€Γ±"  "  "?€ΊΣΗό€Τ¨" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "·€β–ΙόΰφΎ."  "  "Έΐσ’ωό€Ί΅4"  "  "ΉΐΙζµύ †"  "  ">ΐόιΥώΐ¨5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ί€μ›άώΰ¶ο	"  "  "»ΐ½Θηώΐ£†"  "  "Όΰς‘ιώ€Τa"  "  "½ €ΙκώΐΟ$"  "  "Ύ σσλώ Λ"  "  "Ώ€άξώΰ‘C"  "  "ΐΰ±θοώ χ6"  "  "ΑΐΥςώΐπυ"  "  "Β€χώώΐΉU"  "  "ΓΐΟρΐΒΟ"  "  "ΔΐΤ…ΰΎ¥"  "  ">€‹ΰςΝa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ε€γ‹ΐ”"  "  "Ζΐ½²ΰΥΧS"  "  χZΌ
tf_ComputeμΤ
"= §“ρ"€ϋ•" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Λ €”υ"ΐ¬€"  "  "=ΐΆΪ‡#ΰµ«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ άΰ‰#ΐ¬€"  "  ">ΰ‘Ϋ#ΰ€Β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ν µλΛ) κα"  "  "=ΐκ½Η*ΰ€“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξΐ±ΚΙ*ΐ·"  "  "= Ώ²Μ*ΐχη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ο “”Ν* ¬O"  "  "=ΐ“ΕΞ*ΰΏι
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π€μγΟ* ϊ"  "  "=ΐΚεΩ* ωΎ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ρ€Θ°δ*ΐΉU"  "  "?€α„ΫΣΐΘΦς" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€’ΒέΣ ΠΫμ"  "  "?ΰΓΞΧ€―’Σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΣΰυΪΧΐϋκΒ"  "  "?ΰ¬€£Ωΰ²ρη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΤΰΓ“¨Ω€ΕΒ‰"  "  "Υΐό“·Ϊ ήοQ"  "  ">ΐ³Σ‹ΫΐΏΈ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Φ€εςΫ€Π†"  "  "> Ϋ•ΫΐΛθK" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Χ€Πη–Ϋ Κ‰"  "  "Ψΐ€ψ®ΫΐΨ2"  "  ">ΰξΥέΰκΓ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΩΐοΓΧέΐ–±"  "  ">ΐΐΰέ ϊ‚" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϊ€ϊγΰέΐΉU"  "  "Ϋ άηέΐψ«"  "  ">ΐ—Όρέ Ω±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "άΐ ¶ςέΰΉ†"  "  ">ΐίωέΰΨΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έΰ”Φϊέ αg"  "  ">ΐϋ’γήΐωο" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή Λθδή€ "  "  ">ΰ„Έαΐσ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ίΐΟΪΉα ϊ"  "  ">€‚σάα€ω
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ€”ηήα ¨τ"  "  ">ΐύκτα€Ύ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "αΐ·Άψα αg"  "  ">ΐΑά£δ€Χ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β ά™¥δ€­β"  "  ">ΰςΣ΅εΰη¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "γΐκμ¤εΐΠθ"  "  ">€”µεΰΔ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δ ΄”¶ε€‰z"  "  "εΐΨ­»εΰΗ"  "  ">ΰ“ΪΕε€²" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ζ ‡ηΙεΐχη"  "  ">ΰϊσΝεΐ£†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ηΰδ¤Ξεΰ‘C"  "  ">ΐΊέΨεΰΧ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "θΰ…φΩεΰ‘C"  "  "ι€φΪεΐΠθ"  "  "> ϊΖγεΐψ•8" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "κΐρύγεΰθ"  "  "λ ι–ηεΰ€“"  "  "μ αΰλεΐΟ$"  "  "νΰΊΓςεΐ…"  "  "ξ ”¦ωεΐΨ"  "  "ο ρϋε€κε"  "  "πΰί¤”ζΰ°"  "  "ρ Άτ•ζ€σ"  "  "ςΰ™ά—ζΐ„="  "  "σΰΆΦζΐΟ$"  "  "τ ά«™ζΰά*"  "  ">€µϋ›ζΰ–β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "υ ¬²ζ ϊ"  "  ">ΐσοζ ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φ …³ ζΰ£·"  "  "> ή³¤ζ έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "χΰβπ¤ζ €±"  "  ">ΐπΨ§ζΰΨΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ψΐΆ¨ζΐΟ$"  "  "ωΐ®λ¨ζ Β"  "  ">ΐ‹Ηζ€βϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϊ θρζ€Η"  "  ">€½ζ―ζΰ„ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋ€Ζΰ°ζΰΖ["  "  "> ‘ω±ζΐΩβ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ό€αΞ³ζ έ"  "  ">ΰτ™ΏζΐΉ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύΐ¥¦ΐζ€Τa"  "  "ώ€Ϋ Γζ ­“"  "  ">€ύµΤζΰψ‘!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ©…Υζ χ6"  "  "€ΐΏ…Φζ€—Θ"  "  ">€Θφζΐ’Φ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰδ—ωζ€Τa"  "  ">ΐΣ¶ϋζΐΦκW" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚ΐ½ηϋζΰκΓ"  "  "ƒΰα€ηΰίφ"  "  "„ΐ«¨’ηΐ¬µ"  "  "… Ο”­ηΰά*"  "  "† ΒΏ®η Η½"  "  "‡ ΠΨ²ηΐΨ"  "  "ΐΊΊ΄ηΐϋχ"  "  "‰ΰ’έΜηΰϋs"  "  "ΰΊ Ξηΰε¤"  "  "‹ ύοΟη χ6"  "  " ΡΡΠη€µ"  "  "ΰΥΡη χ6"  "  ">€ίΉΣηΐ«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΙκΣη€έΫ"  "  ">ΐ³ύΦηΰΛϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ¨Χη€ό¤"  "  "?€§Ϋηΰ–Ώί" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΙΪ³ι€…"  "  ">€µ€»ιΰΛϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘ΰ°τ»ιΰ‘C"  "  "’ΰξ†½ιΰά*"  "  ">ΐ¨Ώι β«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ΰΔΏι€Ί·"  "  ">ΐ²ΛΕιΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”ΰήΖιΰΖ["  "  ">€υΗιΰΛϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΐωΧΗι κα"  "  ">ΐθ§Λιΰ–β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "– ―ƒΜι µΙ"  "  ">€»‹‡λΐΎτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "— ¥νλ €±"  "  "> Χ£©λΐέ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ™σλ€Η"  "  ">ΐΘ€›μΐΞ•" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™ΐ¥άμΐΗξ"  "  "ΐΡΆμΐ€β"  "  "›ΰ•΅§μΐο±"  "  "?€·ΨΜμΐΦζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΡ•Ξμ ‘€G"  "  "ΐ–—ν „«H"  "  " Έ‹ζνΐ—Λ"  "  ">ΐ‰™δοΰΜΎ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰµθδο €±"  "  " €ϋζο€Υ¥"  "  ">€³‚ξο Ϊυ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΅€ϊπο€σ"  "  "Ά ™‰ςο —Δ"  "  "> χ¨ωοΐ R" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£ΐ£ψωο ωΎ"  "  "¤ΐ·τ†πΐϊC"  "  ">ΐή’Μπΐ²γ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥ΰ“άΝπΐκ’"  "  "¦€δ“πΰµ«"  "  ">ΐγΧπ ήX" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ ‰ράπ€Χ­"  "  "¨€Ο½ππ Π·"  "  "©ΰΠ”ϊπ Όπ"  "  "ΐΉ…ρΐ£†"  "  "«€ηΙ“ρΐξm"  "  "¬ Ρ«•ρ ¬O"  "  "­€φ¦ρΐ·"  "  "®ΰνΏ ρ€¤θ"  "  "―ΐ›µ©ρ ¤™"  "  "°ΐπΪ―ρ€I"  "  "> · αρ€¬0" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "± Ι”γρΐ¶Ύ"  "  "²ΰ§–νρΐ“Ο""  "  "?ΐκκΡς ΐ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³ΰΤΜΣςΐ«ΫE"  "  "΄ Ω¨σΐόΪf"  "  "µ Εµτ ½Σ^"  "  "? δμτ  ζΚ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶ΰιφ ήΠ"  "  "· •ƒφΰΛ―"  "  ">€ΎΈφΐ‘Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Έΐα”Ήφ€…"  "  "Ήΐο­½φΐΉU"  "  "Ίΰ¤χΎφΰϋs"  "  "»ΐτΜΐφΐξm"  "  "ΌΐηχΑφ –€"  "  "½ΰΑΓφ χ6"  "  "ΎΐΤΗφ€’τ"  "  "Ώ «ΌΚφΐΗξ"  "  ">ΐιΖΩψΰ­υ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ€όήψΐ‰ά"  "  ">€ή—γψ Γ—" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Α ΏγψΰκΓ"  "  "Βΐ±ιψΐξm"  "  "Γ€Χ¥νψ Η½"  "  "ΔΰΆ ρψ€ "  "  "ΕΐσΉψψ€κ0"  "  "Ζ€β§ωψ ¬O"  "  "ΗΐΓΐϋψ Τ’"  "  "Θΰλ΄ώψΰ‘C"  "  "ΙΐπΆ€ωΐΉU"  "  "? Ε ω ΉΜΒ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΚΐΕΡ€ϊΐηΉ"  "  "Λ ύΒΆϋ€—Θ"  "  ">ΰϊΒΕϋ υΝ>" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μΐ‘¥Ιϋ Όπ"  "  "Νΐ¦εΫϋΰαΙ"  "  "Ξΰεμγϋ€΅"  "  "Ο Άυϋΰϋs"  "  "Πΰςφϊϋ€›ξ"  "  "Ρΐ¨Άϋΰµ«"  "  "?€ΐΏΚύ Ρξ΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ¦ΖΞύΰίφ"  "  "ΣΐΡ†βύΐτ"  "  ">€Ά€ΰά³z" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΤΐΟρ ήΠ"  "  "Υΰ¶Ό‹ βl"  "  "?€Βτ·‚ΐΧ΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΦΐΎΊ‚ΰ‚…@"  "  "Χΰ¥ύ‚ΐνή"  "  "Ψ ξ—ƒΐτ…""  "  "Ω Ρ‹½ƒΐηT"  "  "Ϊ ζκ„€Ύ’"  "  "Ϋ ψή„ αg"  "  "άΐ­¨„ΐΌ΅"  "  "έ€Ύ«®„ΰζθ"  "  "ή€ΐ³Ή„ΐο±"  "  "?€ΐ―ή†ΰ²Χ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ί€Ϋα†€¬Ήν"  "  "ΰ€κιΊ §„_"  "  "?ΐς©ό‹€λη°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "α ¬°ώ‹€›α­"  "  "?€¶ωΒΐƒΣ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "βΐΜΕΰ£Ιφ"  "  "γ€Κ‡Δΐ¨ΔM"  "  "?€δή£΅ ί€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δΐϊ¦΅ Υ”"  "  "εΰΤΜ½£€θI"  "  "ζ ΐΒ¤ £“’"  "  "? αΒ¨ πίψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ηΰθΕ¨ΐΓ†²"  "  "θΐβΎώ©ΐΦΎΉ"  "  "?ΰΰςφµ Νπ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ι€κωµΐ’Ι―"  "  "κ€Ρ¬· ωσ#"  "  "λΐΎΎΨ·€΅†$"  "  "μΐώύ·€Κχ["  "  "ν φϊαΈΐ·"  "  "ξΰ¬δΈ€Τa"  "  "ο€ΒυεΈ€Χ­"  "  "π ϊψΈΐϋ"  "  "ρΐθ‚Ή —Δ"  "  "?ΰ¦‡» σνφ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ς€ϋ™΅»€©ϊ±"  "  "σ€ΊέΌΐ›Ί6"  "  ">ΐΗΜΥΔ€»°!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "τ ΣΧΔ ΙΕ"  "  "υ ¬βιΔ€· "  "  "? —’λΝΐά‹σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φΐλ¤νΝ€¶Ο°"  "  "χ Ζο¥Οΐ›Ί6"  "  ">ΰέ«ΊΡ …¤f" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ψΐκΓΡ “R"  "  "ωΐ¬‘ΐ­Δ"  "  "?ΰ’‹ΗΣ€ΐ·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϊΰΩ—ΙΣΰ£­"  "  "?ΰ”ΰέΐ΅ρ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋ€Ή¥εέ ‘"  "  "?ΰƒ΅νΐΐƒ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "όΰΕνΐΪΞΟ"  "  "ύΰϊƒψξ μ%"  "  "?ΐ£ΙΝς £ωη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώΰ¬τΟςΰΚ£"  "  " ‰χτ ΄6"  "  "€ΰΕυµυΐ€λ|"  "  "?ΰϊ±ω ½°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΟΔ ω ο‰"  "  "‚ΰ”®ϊ€ΥΪ"  "  "?€ε¥”ϋ€Ι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒ€–γ–ϋ ƒ†"  "  "„ Βπ©ό ²²	"  "  "?ΰ·Ωύ€±ϋ”" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "…ΰθ–ύΰΣ0"  "  "† ϋΠύΰ…ΡP"  "  "‡ΰ©ώ¨ώ€ "  "  "?ΐήχ›€ΰφ±Ω" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ»€€κΨΓ"  "  "‰ ΕζΰΪΧ"  "  "?€Ε•χ‚ΐ―¥“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰεω‚€Ε€"  "  "‹€ΐά† Άϋ*"  "  ">ΐϋΊ™‹ΰ§2" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ–©‹ΐσΑ"  "  "ΐύχΌ‹€θέ"  "  "?€ΔΗθ”ΰΉΔ”" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ·£λ”ΐφα"  "  "?€ΌΜθ Ν" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ―¨λΰεέ"  "  "ΐξΟ ®$"  "  "?ΐΓγ’Ά€ιΙΦ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘ΐΏ•Άΐξΰ«"  "  "’€ΆΜ£ΰξΣ"  "  ">ΰ€θ®¥€υ=" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ΐΊξ°¥ π0"  "  "” ήδ¥ΰΠ™"  "  “‚όi
tf_ComputeμΤ
"> ³µ µΎ΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "=ΐΒΈΨ+€η™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΐκϋΩ+ΐ«"  "  ">ΐ†Κ/ΐνι•" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "–ΐ΅ΐΝ/ΰΚλ"  "  "—€΄Λκ/ΰ¤χ¬"  "  " ¥Π2 ³½›"  "  "™€ύΧΑ4ΰΒκ3"  "  "ΰ“¤φ4ΐΨ"  "  "›€ώ…ψ4ΐ»ζ…"  "  "ΐ’„6ΰ¨ΐ6"  "  "ΰΑςΓ6€κ¬¥"  "  "ΰάάλ8€Ύ±J"  "  "ΐ ΐ½9 ρς@"  "  " ΰΉφ9€΅†$"  "  "΅ΐμΏ¤:ΐ•4"  "  "ΆΰΎιδ:ΰƒ³v"  "  "=€ύµα;ΐν“," νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£ΰΜ‹γ;ΰ¥&"  "  "=€ο†<€εε^" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ΰ“<€\"  "  "=ΐΣψS ›ΎM" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥€Β­S€„ϊF"  "  "=ΐ€ϊψS ή…!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¦ΰ–ϊωSΐ›…"  "  "= Η_€ΖηE" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ ώ–Φ_ ‹>"  "  "=€ηρ`ΰΩΘ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¨ΐΥί™`ΰσς"  "  ">ΰυΉ`ΰςμ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©€ΣΊ`€”±%"  "  "€½λ` ―οr"  "  "=ΰΙ§εaΰµ•5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«€πζa€Ώ/"  "  "=ΰφσbΰιΣ`" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬ΐςη›bΰΞε]"  "  "> ½ Χ… §ΟF" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­ΰ½‚Ϊ… ©@"  "  "> φ²†€η " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "® ¬†€¶‘"  "  "?€οΔΏ†ΰΫΎ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―€Γ¦ΐ†ΰιι/"  "  "°ΐΙΥό† ¬Ψz"  "  ">ΐ‘―ώ‡ΐΏΆ:" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "±ΐξ€ΐ€Μ3"  "  "> ύ Ή€έδ{" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "²€®­Ί Ζ‚x"  "  ">ΰμΰΐ―λ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³ ‡θ€ύ‡Q"  "  ">ΐµ½ΰα " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄ ζΎ€—Θ"  "  "?ΐ†ήΐ¬±Ώ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µΐ¥Χήΐ«¦-"  "  "¶ΐρμ” €―όƒ"  "  "?€Ά€Φ°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "·ΰ‚•Ά ®φT"  "  "Έ …φΆ ―Π)"  "  "Ήΰ ¥£ΐϊΌ"  "  "?€ό·¤€Υ¦Κ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ί€¤ήΈ¤ΰ…η"  "  "»ΰ²ΏΩ¤€΅Π"  "  "ΌΰΓΥ«§ΐωV"  "  "½ΐαύ†¨ΐ³‘A"  "  "ΎΐχΙ¨ ®$"  "  "Ώΰεξ¨€Ώ/"  "  "ΐ€‡Κ¥©€Ϋ§X"  "  ">ΐ—‚ Λ·J" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΑΐύΑƒ€ϋE"  "  ">€‚Νΰ‹@" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Β€ΐ°Ξ€΅»<"  "  ">ΰ¬ζ«€‚§T" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Γΰκψ« ΗάL"  "  ">€Ϋάγ« Ω±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Δΐ³ϋδ«€Ύ’"  "  "Ε ƒΡζ«ΐΉU"  "  ">€ά ι« ψδ6" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ζΐ•φι«€κ1"  "  ">ΐ€Υ ¬ π®;" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Η€Ωσ΅¬ΰ–9"  "  "?ΰήγ»­€Ύ…¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΘΐγΡ½­ ζ»"  "  "ΙΐΎε­ ›¨~"  "  ">€ΙΈθ®ΐίϊ'" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ ίΈι®ΰ²Ι$"  "  ">ΐέΛ― ΣΆ^" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Λ Ψ‘― Ξƒ["  "  "?ΰ—τδΣ€Ε€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ€΅ηΣΐΘω–"  "  ">ΐξ·Φΐ¦ρZ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΝΰΒΚƒΦΐΪΕU"  "  "? ΰάΦΐΞΖΠ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξ ήΦΰΝ³Μ"  "  "> ‘ΓΥΰ€· " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ο ξΧΰΐό†"  "  ">ΰϊΖλΰ ¨ή3" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π€ΨΣξΰ€Ί·"  "  "Ρΰ£Ξςΰΐ„="  "  "ΐέΤτΰ€η™"  "  "Σ€ώΰΰ°"  "  "Τΐίάΰ€¬Σ"  "  "Υΐυΰ—α€Ύ’"  "  "ΦΐΌ™α€σ"  "  "Χΐ―›α€I"  "  "Ψΰ€αΐΟ$"  "  "Ω ΄†α χ6"  "  ">€Ψ½αΐ‰ά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϊ€Βξαΰ„ξ"  "  "> ‰¬£α ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϋ€ζΦ£α ΎΓ"  "  "> Μέ§αΐκ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "άΰΠ¨α €±"  "  ">ΰλ«αΰέξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έ€γΏ«αΐ„="  "  "ή υδ®α€κ0"  "  ">€™±α€“Έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ίΰί±αΰΛϊ"  "  ">ΐ…·ΏαΐΥ‡	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΰ¤±ΑαΰΛϊ"  "  ">€ΞιΙα Φ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "αΰ‡πΛαΰοβ"  "  "> τΪαΰΉ†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "βΰ—‘Ϋα€Τa"  "  ">€›γ Ω…g" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "γ€¬™γ€τξ"  "  "δ γ‚΅γΰΌ"  "  "ε€Ώ΄γ€Γζ"  "  "ζΰ“ΘΡγΐΉU"  "  "η€‹ΤγΐΩβ"  "  "θ άϊΫγΐα"  "  "ι ΒΠήγ ΉΩ"  "  "κΰςΰφγ Λ"  "  "λΰΟΌψγ€σ"  "  "μΰ¬ϊγΐΉU"  "  "νΐ¨ϋγΐΟ$"  "  "ξΰΤΫϋγ χ6"  "  ">ΰΓ«γ€™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ο ΘθγΰΉ†"  "  ">ΐωΦƒδΰΤτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "πΐ „δΐ€β"  "  "> ƒδδ€Ί·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ρ Ά­‰δ€¨Γ"  "  ">€ε­δΰθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ς€Οήδΰά*"  "  "σ °Ζδ Β"  "  ">ΰ§®δΐΠθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "τ ¬λδΰξ"  "  ">€Ϊ•δ€Ί·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "υ€²—δΰ½"  "  ">ΐΑ‘Όδΰ“µ<" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φ€Ήω½δ€ΏΦ"  "  "χΐΩθΖδ€I"  "  "ψΰβ“Ιδΐβά"  "  "ω€ΜΡδΰϋs"  "  "ϊΐΰΥδΐΖί"  "  "ϋ€°ινδ€Ύ’"  "  "όΰιοοδΐ·"  "  "ύΐΒΏςδ€Τa"  "  "ώΰΨσδ ¬O"  "  "ΰΛκτδ χ6"  "  ">ΰ¤λψδΰ’‡" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΰ ϋδ Τ’"  "  ">ΐμδΐΗξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΪδ ¨τ"  "  ">ΰΫ‹„ε όΥ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚ΰΕΌ„ε µΙ"  "  "> €ε Κ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒ ξαεΐ„="  "  "„€κΥ‰εΰά*"  "  "> ΔµΕεΐΏΈ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "…€Ι£Ηε έ"  "  ">ΐ’ξΈζ€η™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "†ΰ’ΊζΰαΙ"  "  ">ΐ«ΩΟιΐ«ρ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ΰζιΐΒΟ"  "  "ΐ›νΨι ©Έ"  "  ">ΐΏ“οκΰ‡L" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‰€‚γπκΰ£·"  "  "€ισκΰΖ["  "  "‹ ­φκΐλΦ"  "  "€¶μƒλΐ­Δ"  "  "ΐ·’λΐα"  "  "ΰιΔ•λΐΏΈ	"  "  "ΐ³μ¦λΰ„ξ"  "  "ΰω©λ€Τa"  "  "‘ΐ§Ϋ­λΐΉU"  "  "’ Ψη®λΐΛΙ"  "  "“ΐό€΄λ ο€"  "  "”€ΡΉλ –€"  "  ">€α„μ Κ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΰΉΤ’μΰ„ξ"  "  ">ΐ¤ΐΪν ψϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "–ΰψάν€…"  "  ">ΐ―ανΰήη#" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—€ρΝβνΐ¨¥"  "  " •ηηνΐΨ"  "  "™ Ζ¤κν€ή"  "  "ΰέ™ςνΰΒ€"  "  "›ΰΥγφν€κ0"  "  "ΐπ ψνΰΉ†"  "  " θΉϋν€±½"  "  "€›ν€σ"  "  "ΰιίƒξΐ£†"  "  ">€Δ“΅οΰ‹•" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "  ¬ΆοΰΒ€"  "  "΅ΰ·Ρ¦ο †”
"  "  ">€πζΈο€ΎΗ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά€γ‘Ίο€®¦
"  "  "£ΰΧ“ΕοΰΖ["  "  "¤€ΛοΗοΰ—¦	"  "  ">€ίοΰΟή{" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥€΅ΩΣο ©Έ"  "  "¦ Σ‹έοΐ‚Ύo"  "  ">€³ΦΟπ ”—?" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ ‡ιΡπ€Ά•"  "  "¨ΰ¶µδπ€ν±)"  "  ">ΰκσρΐ…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©ΐΜ½“ρΰΉ†"  "  ">ΰ™‰σΐΝΡ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ¥Π‹σΐΤΓ"  "  "«€νΎσΐ›Π"  "  ">ΰύς£σΰ”" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬ ΅ω¤σ ο€"  "  "­ΐ™τσ ¨τ"  "  "® ‡―σΰσ½"  "  "> ππΊσΰ¬ζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―€΅ύ»σ€Π†"  "  "°ΰμχΏσΰΖ["  "  "±€ΆΑΑσ€σ"  "  "²ΐΈςΓσ€Ύ’"  "  "³ΰΆΤΕσΰ°"  "  "΄ΰ―Ησΰϋs"  "  "µΐ–’Λσ€±½"  "  "¶ΐ¤«Οσΰ›"  "  ">€‡Χσΐρξ)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "·€ΫιΧσ Δ¦"  "  "ΈΐΠώζσ€I"  "  "Ήΐ‘θσ€†γ"  "  "ΊΰΐΓρσΐ„="  "  "»ΐƒΔτσ€‚"  "  "> Υ΅τΐ«ΫE" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό ©ƒ‚τ€©‡"  "  "½ τ ¥Η;"  "  ">ΐ’ΐΗτ ύ™	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ύ€¶ΖΘτΰΉ†"  "  "Ώΰ­ίΛτ€Γ±"  "  "?ΐΟΓΫτΐΈΉΎ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ €Πάτ€©‡"  "  "Α€Ϊγδτ αg"  "  "Β ΦΉθτ€¨”"  "  "Γΰώυ µΙ"  "  "ΔΐΒα‡φ€Ύ’"  "  "Εΰ¬Γ‰φ€I"  "  "Ζΰξφ µΙ"  "  "Ηΰ›“φΰ–β"  "  "Θ€”“φ€Ω€"  "  "ΙΐϊΕφΰϋs"  "  ">€άήφ  Ύ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ€––φ ±ξ"  "  "> βς¤φ€³Ε" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Λ€©Ξ¥φ€Π†"  "  "Μΐύ‘©φΐΛΙ"  "  "Νΐμα¬φ ¤™"  "  ">ΐΐψΕφ Π΅5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξ δ―Θφ Ξδ"  "  "ΟΐπΫφΐό»"  "  ">ΰΊε‡χ€ Β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΠΰΗχ€ζΥ"  "  "Ρ ®ς‹χΰ°"  "  "ΐρΤ‘χΐ€β"  "  "Σΐΰ¤•χΐ«"  "  "Τ ΚχΐΉU"  "  "Υ ’υ›χ€I"  "  "Φ ΊΈχΐ–±"  "  "Χΰο² χΐ„="  "  "ΨΰΘ³¤χ€¨Γ"  "  "?€ΞΈΑχ€Ύ…¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΩΐΕ ΓχΐΓ“"  "  "Ϊ έΖΜχ ¬O"  "  "Ϋΐ­ώΠχ ”β&"  "  "άΰ„’όχ€…"  "  "έ€θχ€βϊ"  "  "ή€·Δ…ψ€οΟ"  "  "ί€Εέ‰ψΐΉU"  "  "ΰ€ƒπψΐ„="  "  "α ΞψΐΉU"  "  "β€”ψΰά*"  "  "γΐρψΐΫ*"  "  "δ€ΰµ½ψ€Τa"  "  "ε€ζΖψΐΓ“"  "  "ζΰήυΞψΰ½"  "  "ηΰθ³Υψΐ·"  "  "θΐ–©ήψ€Τa"  "  "ιΰ’αψ€I"  "  "κΐβΤγψΰΨΟ"  "  "λ μαθψΐ–±"  "  "μ€™“μψ –€"  "  "?ΰΨόφψ€‰" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ν€οόχψ€ηΆ‚"  "  "ξΰ†¬ϋωΰκΓ"  "  ">€ΰέ€ϊΐά®" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ο€΄Ώϊ€²"  "  "πΰ€ώϊ€I"  "  "ρ€©ςϊ ύ™	"  "  ">ΰ³Γϊ ‰ΚK" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ς ν™ϊΰΠ™"  "  "σ€όΔ΅ϊΐ‡σA"  "  ">ΐΑΚδϊ€ΡΚ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "τ€αυηϊ Λ"  "  "υ€σιιϊ€›ξ"  "  ">€—νϊΰΘγ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φ Γ΅ξϊΰψά"  "  ">€γ²‹ϋΰΠ™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "χ Ν”ϋ …Π"  "  ">ΰΆ–ϋ υγ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ψΐΣ¨—ϋΐξm"  "  "ω€αίϋ έ"  "  "ϊΐέζϋΐ«"  "  "ϋΰΥ΅ϋ€I"  "  ">€Ηκ³ϋΐµ―" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "όΰχφ΄ϋ€οΟ"  "  "ύΐβΊΉϋΰ‘C"  "  "ώΰβλΊϋ€‰z"  "  " ΪΣΌϋΰϋs"  "  "€ Νώ½ϋ αg"  "  "ΐ—Ώϋ€I"  "  "‚ΰΆ†Ηϋ ϊ"  "  "ƒΐε†Κϋΰά"  "  ">ΰκ°όΰώτ)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "„ ά²όΐβά"  "  "…ΰ‹ό»όΰΛ―"  "  ">ΐ¥³Κύ€έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "†ΰΙΜΟύΰΗ"  "  "‡ΐ¨Ϊύ Θ	"  "  ">ΰ³²θύΰ¬±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Κ²ιύ€Η"  "  ">ΰιξύΐ‰ά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‰ •ού€ό¤"  "  ">€η¨χύΰ»" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ²Αψύΐξm"  "  "‹ΐΡ»ϊύΰχ"  "  " Εωύΐµϊ"  "  " ΄ΙƒώΐΉU"  "  ">ΐΖξ†ώΐίΕ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΠ‡ώΰΒ€"  "  "ΐΎΈ‹ώΐ¬€"  "  "ΐψοώ χ6"  "  "‘ΐκώΐ„="  "  "’ΐήΕ‘ώ€¨Γ"  "  "“ΰηπ“ώ χ6"  "  "”ΰΔΜ•ώ ¬O"  "  ">ΰΛσ£ώΐ‡Ύ)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΐΗη¤ώΐ“
"  "  "–ΰίο°ώ αg"  "  "—ΐΞ³ώ Θ	"  "  "ΰ½ώ χ6"  "  "™ΐβΐώ µΙ"  "  " ΅°Εώ€“Έ"  "  "> ½βΝώΰάJ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "› °ΟώΐΏΈ	"  "  "ΰβπΩώΐήλ:"  "  ">ΰ†ψ¦€‡ΖW" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ·„¨€τξ"  "  "€Ή°ΰµΚM"  "  ">ΐΠ›έ g" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΏλΰ€¦X"  "  " ΰΒ‡Ό‚ΐΓ“"  "  "? »άΚ‰€ξ―ζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΅ΐΝΞ‰ΰγγ‚"  "  "ΆΐΒΟΘΰΛΞe"  "  "?ΐμ±ΰ³–¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£ Ξϋ €”Ί"  "  "? ΉάίµΰΔϋΈ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ ²βµ€ύΫ²"  "  "? ΏεΨΔ€Ζ®" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥€ΔΣΪΔ Κ"  "  "¦ΐκΝ―Ζΐ•φv"  "  "?ΰΟΣίΚΐθωΗ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ΐσβΚ έΣ"  "  "¨ ΣυΜ Λ·J"  "  "©ΰΆΙΝ υ‚W"  "  "? ©ΌΙΣ ν¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€μΌΜΣ ά»¨"  "  "?€Τ¬ΞΥ€μφ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«ΰ£‚ΠΥΐ¤΄7"  "  "¬ΐΪΫΦ »ΛS"  "  "­ΐΎύγΦΐβά"  "  "?ΰΔΨϊΨΐ’―…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "® ϊύΨΐ±ΠΒ"  "  "―ΰλΖΖάΰ›λ7"  "  "?ΐΘ―ΩέΰΑ©" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "°ΐΌΫέ€ΗΚ”"  "  "± ιφήΰΘγ"  "  "?€Σ›Γϋ ό»Ω" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "²€ΒλΖϋ€ΟΛw"  "  "³ Β¥Βό€Ώΐ7"  "  "΄€½τ†ύ€Ύό1"  "  "µ §ΐΉύ β•7"  "  "¶ ²¬φύΰ°"  "  "·ΰτϋχύ αg"  "  "ΈΐΉωύ Έ•"  "  "Ή€τώ€Ύ"  "  "Ίΰ–ώΐΊ™"  "  "?€αώΰ¤ΥΜ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "» χώΐΦ€'"  "  "ΌΰμδΕώΐΛ³3"  "  "½ΐε«ώ€³ϊ%"  "  "Ύΐ‘΄―€―κ"  "  "ΏΐίηΏ€Δί:"  "  "ΐΰΪ‡€ΰ®ζ"  "  "Α Πς έΰd"  "  "Βΐ”…ί‚ΰ‘ο"  "  "Γ€ώεΠ„ΐθτ("  "  "Δΰ”²……ΐΚΊ"  "  "Εΰ¦Ϋ… €¤¬"  "  "ΖΰζθΦ† πγS"  "  "Η€σώ²‡€ ,"  "  "Θ€“α‡ τ"  "  "?ΰΓ‚λ‡€£β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ι ΅μ‡€θ’&"  "  "Κΐαΐ•ΐΙ•Y"  "  "Λΐ³φ Ίθ"  "  "?ΐπρ€‰ΐΛΆƒ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ Φ–‚‰ΰ«Χ."  "  "Ν ξ±‰ ›‰5"  "  "Ξ€Ήψμ‰ ¤ƒ6"  "  "Οΰ—•© ¦΅"  "  "Π€’Ί »–;"  "  "Ρΐ½σ€σΔ"  "  "€ΝΐΎή4"  "  "?ΐεύ “Έ€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Σ „ΐΠΆ½"  "  "Τ€θ΅Σΐ—Ε"  "  "Υΐϋ‹δ“€Ρ›"  "  "ΦΰΦ¦–€£ΜΑ"  "  "ΧΰΩµν—ΰό‹h"  "  "ΨΰΤΧΰβ–ƒ"  "  "Ω θ–ΚΐμζΉ"  "  "Ϊ ¶‰±΅ΐήλ:"  "  "Ϋ €Μτ΅ ³"  "  "ά ΈΉΆ€ΐ"  "  @c
tf_ComputeμΤ
"= ςϊπ"ΐΉ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύΐ„ τ"ΐ€β"  "  "=ΰ©ύώ"€Π†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώΰ¨€#ΐ·"  "  "= ώΐ‚#ΰΉ†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ϊ΄ƒ#ΰϋs"  "  "= δ–…#ΰ„ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΐΕώ…# αg"  "  "=€Σµ‡#ΐµϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΑ£#ΰΖ["  "  ">ΰζ#ΰ«‹" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚ Ξη’)€έΫ"  "  "=ΐΏγΝ*ΰέξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒΰΪ‚*ΐξm"  "  "= ¦ΜΤ* ϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "„ΰΙΥ* ¬O"  "  "=€άχΨ* Μά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "… ΨΝά*€Τa"  "  "=€©Ρ”+ΐ’Φ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "†ΰ­Ώ–+ Λ"  "  "=€Α¨+ΐχη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ Ά +ΰΖ["  "  "<€–Ξ¥+ΰϋs" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ‹¦+ΐΟ$"  "  "<€Γ°+ ¬O" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‰ΐ’Υ+€µ"  "  ">ΐ›¨¬/ΰ»©ϋ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ¶Η°/ΐ¦ΌB"  "  "‹€©ψ/€ΕΨX"  "  "€π“σ0 ω"  "  "ΰπ™Ά2 ‰«"  "  "> ρ¥€γ¨;" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ³υΰ¬›4"  "  "?ΐ•»ΨΣΰµ§©" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ‘ΰΪΣ Χ£"  "  ">ΐ¬Κ‚ΦΐΚΩ^" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€…ιƒΦΐχY"  "  "? £σαΦ€όΥΏ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘€Ύ°γΦ Ή"  "  ">€’Μ›ΪΰΫοt" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’ΰ¬‰Ϊΰ…†i"  "  "> ήβΩέ —Δ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ΐ“¬Ϋέ€οΟ"  "  "> Υρέ Ώ‡" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”ΐηςέΐΎτ"  "  ">€ΖΝψή ί”" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•€ξϊή Λ"  "  "–ΐχμύή€¶ά"  "  "> “‡ί ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—ΐ†Κ‡ίΰ„ξ"  "  ">€„Κΰ€ίγ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ψΛΰ€©‡"  "  ">ΰζΚήΰΐΏ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™ Ρίΰ€‰z"  "  "ΐ“όαΰ Ώ‡"  "  ">ΰΚμ·α€ύθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "› ΌΉαΐ‰ά"  "  ">ΐω¥Ζαΰη¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ώβΖαΐξm"  "  " ΫοΙαΰέξ"  "  ">€οΊΥαΐβά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐψ–Ωα µΙ"  "  "> Κδωκ€Ζύ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ΄ϋκΰ¬±"  "  "  ΩΑƒλ Ίθ"  "  "΅€μΘ‰λΐ£†"  "  "Άΐμλ€¨Γ"  "  "> ϋ’λΰδΚ," νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£€εΦ“λ υ"  "  "¤ Υ›λ τΤ "  "  ">  Χλ€―(" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥ Θμ ‚Ή"  "  "¦€χΣμ εχ"  "  "?€Γ§Ψν ‰ΣΕ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ΰ…¨ΫνΰΥΈ
"  "  "¨ ΰΞην€Π΄"  "  "? λΓοΐ–ΩΕ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©ΐΔο€ΡΚ"  "  "ΐϊΡ©οΐμΉ"  "  ">ΰ–’ρ €ζ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«ΰ¨†‘ρΐσ"  "  "¬ Ιυ™ρΰω "  "  "? §ςός όύΖ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­ΐξ―€σΐύΚ
"  "  "®ΐγβσΰΏάµ"  "  ">ΐχΡΔτΰ‰χ5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―ΐ€ΜΕτ ΈΚ*"  "  "°€€Τστ β«"  "  "?ΐβ°ϋτ€¦‚ƒ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "± “½ότΐαυέ"  "  "²€ΐΛΫφΐΫ "  "  ">ΐ„Ψ€χΰΤτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³ΐ–Μ‚χΰΨΟ"  "  "> ­®†χ€έΫ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄ΰζƒ‡χΐξm"  "  ">ΰγμχΰυΕ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µ€ΕΤχΐ’Φ"  "  "¶ΰν•χ ’¥"  "  "·ΐ‡χ€βϊ"  "  "> ύι΅χΰΗΤ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΈΐπΕ¤χ€ζΥ"  "  "Ή χ§χ€‰z"  "  "Ί ΕΊ©χ€Ύ’"  "  "» Χ®«χΰΖ["  "  "ΌΐΆΗ¬χΰ‘C"  "  "½ΐ«Α­χ€Τa"  "  "ΎΰσΒ¶χ€±½"  "  "Ώ€βΊχΐζ·"  "  "?€µ‡βχΰΐύ–" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ σΚδχΐ¨5"  "  "ΑΐΔ°ψΐβϋP"  "  "Βΰ…ίϋψ Υθϋ"  "  ">ΰΘΧϋΐΙ#" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Γ ΐΏϋΰ„ξ"  "  "Δΰ•“ϋ Λ"  "  "Ε€Β–ϋ …Π"  "  "Ζΐσµϋΐ£†"  "  "ΗΐΥ°Άϋΐµϊ"  "  "Θ ·ϊ¥ϋ€Π†"  "  "Ι€ν¥ϋΰσ½"  "  "> ‘ήψϋ ‡Ψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ Ψκϊϋ ¨τ"  "  "Λΰκΐϋ€²"  "  "?€ΔΘύ€„ΧΆ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ€―Ξύΐ¶ζΛ"  "  "Ν€¤κ σϊK"  "  "> λΨήΰχμd" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξΰ—Ωΰΰ‚ΊX"  "  "?ΐ†ρπ‹ ¬¬ά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ο€Ίσ‹ΰώ²Ό"  "  "Πΐ΄Δΐβά"  "  "?ΐαΐΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ρΰ–ΐ™²+"  "  "€ένΌ€“ΧP"  "  "Σ ρΉ” ο€"  "  "?ΐΑΈ¥‘ΰΓ΅δ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Τ€¬Λ¨‘ΰθ®¤"  "  "ΥΰΉ¦Φ’€κ1"  "  "? ‹›²ΐλώΛ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΦΐΙή΄ΰόΌ"  "  "Χ€θνυ› ΥΦ"  "  "? ΟΤ±¤ΐΔ΄θ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ψΰ°ν³¤ΰ® "  "  "ΩΰΆΗΪ¥ ς<"  "  ">€χΏ²¨ΐΧωD" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϊ ΄µ¨  ¨8"  "  "Ϋ€μάο¨ Μά"  "  "?€ίϊ›€Τ‰Δ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "άΰΝ™€ΤΤ«"  "  "έ€­΅Χ«ΰοβ"  "  "?ΰ‡ρ®°ΰΒΪ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ήΐΑχ°°ΐδΰ·"  "  "ί€Άν² µώ"  "  "?€ΉδΡ³ΰμΤ‰" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΐύΣ³€±Ζ|"  "  "α€ψΓ΄ ο"  "  "? ωΑΣµ€Β•Β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β ΫΌΨµ Ύ¶­"  "  "γ€Ή‰· "  "  "?€«“ΣΣ £¥†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δΐΑΔΥΣΐ©ά΄"  "  "ε€γκΥ€Δ""  "  "ζ ΰΊΥ υ&"  "  "ηΐΧΪαΥ€΄σC"  "  "θ ¬ξ―Φΰε¤"  "  "ι€±ά±ΦΰΖ["  "  "κ€¤‡³Φΐƒ"  "  "λΰάώΗΦ Μά"  "  "μ€ΝΓΦΐ¤Κ"  "  "? ΆΪΦΐςν	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ν€ΏΫΦ ·†%"  "  "ξ€ΡΧ€’ή2"  "  "ο€ξΊΧΐΏΆ:"  "  "πΰήωύΧ "  "  "ρΰ€Ψΐ‹Ξ?"  "  "ς€τ‰ΫΨ€†υό"  "  "σ€«ΌΪΪΐΧωD"  "  "τ ‘β§Ϋΐιΰρ"  "  "υΰαέ€‰δ1"  "  "φΐ¥‹Ϋέ πω""  "  "χ …·„ή€ζή|"  "  "ψ υΣ‡ί€’ή2"  "  "ω λ΄Κί §."  "  "ϊΐ¤Γϊίΰλ‡
"  "  ">ΰ¥ςΈγΰψ°j" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋΐίψΊγΰωΏW"  "  "όΰ…Ή”δ ‚Ή"  "  "?ΐαέ΅ε μ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύ€ή£ε€γψΑ"  "  "ώ€ςλθ€ ι"  "  "ΰώΩΦλ ‚Β†"  "  "€	 πΪτν€„ƒΑ"  "  "	ΰήπΈοΐ“ξk"  "  "‚	€αύ¦π€µ‹«"  "  "ƒ	 ι²ς Ό‡£"  "  "„	ΰτϊ³ψΐ—ί8"  "  "…	€±Υτψ€†μ‚"  "  "†	€ΞΤόω ²†k"  "  "‡	ΰΝ¬υϊΰΫΠ+"  "  "	€Ά¥ϋ ΐo"  "  "‰	 τΌ™ό€υ="  "  "	ΰ€Οήόΰμµ@"  "  "‹	€…΅ύ ―›"  "  "?€ωƒ€ κο" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	€‹ψ€€μιΘ"  "  "	 ©‚„ΐχΡ2"  "  "? „™¤…ΐτΓ΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	ΰϋ€¦…ΰΗ’±"  "  ΝjΜi
tf_ComputeμΤ
"=ΐ£΄ά"€ν±)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	ΐ±Νΰ" ο"  "  "=ΐƒ‘‡# Γβ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	ΐ°#ΰΉ†"  "  "> €Ι#ΰάο" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘	€αω)€’τ"  "  "=€εΧ*ΐΗξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’	€„»Ϊ*ΰ°"  "  "=ΰΦάμ* Τ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“	ΰ”ον* ¬O"  "  "=ΐ―¬ο*ΐ›Π" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”	ΐΈ¦π*ΐΉU"  "  "=€κΕυ*ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•	 Λ­φ*ΰ‘C"  "  "=ΐ–Ζχ*ΐ·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "–	€Π›ψ*€κ0"  "  "=ΐίΪ„+ΐΑE" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—	€Δ©Θ+ αg"  "  "= ‹ηΛ+ΐΆχ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	€ΒΜ+ΐ­Δ"  "  "™	 ϋϊΤ+€έΫ"  "  "	€οΈΪ+ΐ€β"  "  "> ±εσ/€ΧΏ…" λΊ¦ή‚ύΑ"  " "λΊ¦ή‚ύΑ" "›	ΐψΧ0ΰ–Ώί" "  "8ΰΚ2 Ν " λΊ¦ή‚ύΑ" "λΊ¦ή‚ύΑ" "= ςΜιaΰΓF" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	€ΒΆλa Ι―?"  "  "=ΐƒΝ°b ς w" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	ΐΧ®±bΐ¦¦s"  "  "=ΐζΛxΰΆΗ^" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	ΐψƒΝxΐ”ύX"  "  "=ΐ€y τΤ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	€¤”«yΐζμ"  "  ">€·Λyΐƒ”ι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " 	ΰύάΛy ¨ή3"  "  "΅	€α¶zΐΟ ¥"  "  "> λ΅µ|€Έφ„" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά	ΰΝΈ|€φS"  "  "£	ΰ}€Χβ)"  "  "¤	 «Ε»}€€‰z"  "  ">€κσΊ~ΰΪ„π" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥	€¨†Ό~ΐ›…"  "  "¦	 ΏΪ~€Δρ®"  "  "§	 •ε‹ΐ£¥J"  "  "¨	ΐ€άΐ³‘A"  "  "©	 ι„‚ υ&"  "  "	€γΪΕ‚ΰδΚ,"  "  "«	 ‹Ήω‚ Ρ-"  "  "> ³ζ«ƒΰΨΉ3" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬	ΰΐ­ƒΐιΈ."  "  ">€«ιίƒ€Τµb" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­	ΐνΈαƒΰ™Ν]"  "  "> «ξΒ„ ‡Β@" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "®	ΰƒΔ„ΰ…8"  "  ">ΰήƒ…ΰΎ¥" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―	ΐ……ΐΨ"  "  "°	 ία†…€Τa"  "  ">ΰίΓ‰… λ8" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "±	ΰώ…ΐ¬κ2"  "  ">ΰΖΗΒ… Ε)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "²	 ΤώΓ…€‡ά&"  "  "?ΰλ‡€ηΆ‚" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³	€Ά“‡ΰφέw"  "  ">ΐηΛΰ‘c" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄	€β¶ΝΐΚΩ^"  "  "? βΖη­ γψ…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µ	€²ι­ΰκΜ~"  "  ">ΰ¦§ξ® ΅μ=" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶	ΐΑδο®ΐΜχ8"  "  "?€£ςΩΣΰΘ‡τ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "·	€ΠΤήΣ€ΓΥμ"  "  "? π¶ΞΧ€πΡ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Έ	€ΐΠΧΰα‡–"  "  "?ΐΰΉλΨΐΤ©Ω" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ή	ΐχΜπΨΐ™®Π"  "  ">€ΐχΔέ ‡Ψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ί	 ίρΖέ€Δυ	"  "  ">ΰΕήδέΐΧ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "»	 ®ζέ ·Ρ"  "  ">ΐΚέΐ΅³" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό	ΐ”€ήΰϋs"  "  "½	€–Λή γ	"  "  "> μεήΰ»ψ=" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ύ	ΰΪΣήΐ’Φ"  "  "Ώ	€΄…•ή τ"  "  "ΐ	€Ίθήΐ„="  "  "Α	ΰ¨‡ ήΰ…²"  "  "Β	 ώ©ήΰΟΥ"  "  "Γ	€ν­«ή ΒΣ"  "  "Δ	€¬ΔήΐΨ"  "  "Ε	ΰΖήΐ·"  "  "Ζ	€ϋϋΗή ¬O"  "  "Η	€„φΘήΐΟ$"  "  "Θ	€πΙή χ6"  "  ">ΰ„‰Νή ζ†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ι	€±ΨΝή€’τ"  "  ">ΰ’ΆΡήΰά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ	 ΜχΡήΐ€β"  "  ">ΐη–Φή ’¥" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Λ	 ωΩΦή €±"  "  ">ΰ®ΤΩήΐέ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ	ΐ‹Ωήΰά*"  "  "Ν	ΐΌΌάή Β"  "  "> Αήή ­“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξ	ΰϊήή ‰«"  "  ">€Ύβδή€Π†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ο	€’Δεήΐχη"  "  "> ««εαΰρκ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π	€°™ηαΐ£†"  "  "Ρ	ΐά™ια »¬
"  "  ">ΐ”ώωαΐΦµ?" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "	 Εϋα€™"  "  "Σ	 §…€β ήΠ"  "  "Τ	€Ώ«‰βΐ„="  "  "Υ	ΐΥά‹β€ύθ"  "  "Φ	ΰΌ§•βΐχη"  "  "Χ	 ΣΨ—β »"  "  "Ψ	ΐά°β€Ύ’"  "  "Ω	€Β¬²βΐ·"  "  "Ϊ	 ¬΄β αg"  "  "Ϋ	ΰΟ”µβ χ6"  "  "ά	€ζ”¶β€I"  "  ">€©ΖΊβ Π·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έ	€ύ§»β€Η"  "  ">€΅Ώβ€—“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή	 ΝίΏβ έ"  "  ">ΰίµΔβΰ±Π" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ί	ΐΑΗβ µΙ"  "  ">ΐΖΛβ ¨τ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ	 £ΙΛβΰά*"  "  "α	ΐ„±Μβ€κ0"  "  ">ΰΘΧΧβ€¤θ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β	ΐΪΨβΰΨΟ"  "  "> ΌδΫβ ΥΦ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "γ	€Ίέβΐµϊ"  "  "> ¥†οβΐέ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δ	 ωηοβΰ½"  "  ">ΰ—Βδ μι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ε	ΰΘδΰΗ"  "  ">ΰΘ½­εΰ¶ο	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ζ	€Π―εΰΛϊ"  "  ">ΰιΉε μι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "η	 ΨόΉε ²²	"  "  ">ΐάΤδκ€ΐ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "θ	ΐΉ°ζκΐσ"  "  "> ω™ρκΐ‘Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ι	ΰ²ορκΰ½"  "  "κ	ΐ΅τκ µΙ"  "  "λ	 ­φκ κα"  "  "μ	ΐφ³ϊκ€’τ"  "  "ν	 εόκ χ6"  "  "ξ	ΐϋύκ€Η"  "  "ο	ΐ›ΰƒλΐΒΟ"  "  "π	ΰβ‡λΐ£†"  "  "> ―ΰ§λΐσ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ρ	€΄Ξ©λ€έΫ"  "  ">€”φ»λΐθΏ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ς	€½λΰ€“"  "  "σ	 «ΊΒλ ’¥"  "  "τ	 µΗλ …Π"  "  ">ΰυ©πλΰΠƒ8" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "υ	€«σρλΰλ‡
"  "  "φ	€ΜΔύλΰ”Δ)"  "  "> ΎϋΓμ€Ζ²-" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "χ	ΐΎ¬Εμ ­“"  "  "ψ	€μπΜμΐ“Ο""  "  ">€Γ§ΨνΐΕ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ω	€“®Ϋνΰµ«"  "  "ϊ	ΰ‚‘γνΐΩβ"  "  "> ΅ο€‰™J" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋ	€‚¦Άο ¥έ
"  "  "ό	 ¬Ά°οΰΖ["  "  "ύ	ΰ„Α±οΰϋs"  "  "ώ	€Ί³ο Β"  "  "	€Γ„΄ο€Τa"  "  "€
ΰΗςµο€I"  "  "
ΐψώ¶ο χ6"  "  "‚
ΐύΊοΐ”"  "  "ƒ
ΐ ΖοΐΉU"  "  "„
€‰‚Ιο ό"  "  "…
€™£Ψοΐ¬€"  "  "†
€ς£άο€Τa"  "  "‡
ΰΧΘέοΐΉU"  "  "
ΰΥίοΰϋs"  "  "‰
ΰ‘€αο€ό¤"  "  "
ΰ€Πδο ΎΓ"  "  "‹
ΰΊ‡θο €±"  "  ">ΰ©Χλο€¶ά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "
ΐπ²μο Ίθ"  "  ">€Άροΰ€Θ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "
ΐΫ§ςο€›ξ"  "  "
ΰ•χοΰ‘C"  "  "
ΰ»ψοΰΒ€"  "  "
ΐκ„όο€©‡"  "  "‘
€‹τ„π ¬O"  "  "’
€Ι††πΐ„="  "  "“
ΐ‹Φ‡πΐΒΟ"  "  "”
ΰίθ‰πΐ„="  "  "•
 ΄¬πΐΉU"  "  ">ΐΔπ ΤΗ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "–
 ‹Ϊπ ψϊ"  "  "—
ΐω¦πΰΟΥ"  "  ">€€ά©π " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "
ΰε€«π€Ω€"  "  "™
 ­ο―πΐ«"  "  "
ΰ­΄πΰ  
"  "  ">ΰΧΏπ κ€K" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "›
 Ζΐπ€¨Γ"  "  "
€κΑΒπΐ£†"  "  "
 ‰ΌΔπΰ°"  "  "
ΐσΖπΰ°"  "  "
ΰέΗπ€Τa"  "  " 
€»Λπ ϊ"  "  "΅
ΐΪ·Ξπ †ώ:"  "  ">ΐΦ“ρΐο±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά
ΐή±•ρΐσ"  "  ">€£ζύς Έ•" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£
€‹€σ ¨τ"  "  "¤
 Μ†σΰ­υ"  "  "? δυσ έϋ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥
ΐ―’σΰ°΄Δ"  "  "¦
ΰΑΫΨτΐίϊ'"  "  ">ΰνΐδφΐ¦‡*" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§
ΰειφΐ†Ε"  "  "¨
 ”“φφ °ί"  "  "?ΰχΦψ Ωο—" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©
€°ρΨψΰύϋ"  "  "
€΅Ιηψ€‹‹†"  "  ">€‰°οω ‘@" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«
ΰ–ςω€—ύ5"  "  "¬
ΐΔχ«ϊΰχ"  "  ">€Έ„°ϊ€λτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­
ΐρΩ°ϊΐ·"  "  "®
ΐΈζ²ϊ€›ξ"  "  ">€¶ϊ Ύψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―
€α‹·ϊ τ"  "  "°
ΰ­Κΐϊ€‰z"  "  "±
ΐ‹ΉΖϊΰΉ†"  "  "²
 Χ³Κϊ€Ύ"  "  ">€¨ΝΡϊ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³
 ΎΝϊ€ή"  "  "΄
 ¥ηΪϊ ΰΨ"  "  ">ΰΧΨώώ€Ϋ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µ
 °χώΰΕ"  "  "? ΅Οΐ‰Ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶
€Γ€βφ©"  "  "·
ΰξΑ€µΝ"  "  "> ­Ζέ€ΰΖ[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Έ
ΐΏλΰ€­W"  "  "?ΰΛΥƒ £“’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ή
ΰέΙ ƒ ‚£="  "  "Ί
 ς‘αƒΐΈ°D"  "  "»
ΐ΄Ο„€¶ά"  "  "? ΔΏΈ„€Νώη	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό
€υΛΉ„ ”β&"  "  "½
ΐµύΰ„ΰΟΏ2"  "  "Ύ
€μ¥…ΐε7"  "  "Ώ
ΐ€ξΪ…ΐ›…"  "  "ΐ
ΰ—ηω… ‚£="  "  "Α
ΐ©ΙΓ†€βω"  "  "Β
€ΡΓΐΟh"  "  "Γ
€‹ύ³‰€Μ½ω"  "  "Δ
€Ζ±‹ΰΆέ-"  "  "Ε
ΐ¤φλ‹ ‰•3"  "  "Ζ
ΰΐΓ¦ΐΝΪ‡"  "  "Η
ΰ΅ΈΈΐ‹Ξ?"  "  "Θ
ΰΦο "  "  "Ι
ΐ‚’ ο"  "  ">ΐΞ¤‘ ›σe" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Κ
ΐ½έ§‘ ·πU"  "  "Λ
ΐγμ‘ ²²	"  "  "?€κΊ…“€οΨ²" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ
€¤ς“ΐΞΟΚ"  "  "Ν
 …ΙΩ– £ωη"  "  "Ξ
 λΒΔ ΜΟ±"  "  "Ο
ΰ†ΪΦ ΄βΧ"  "  "Π
ΰύΌ±ΰΨ£d"  "  "Ρ
ΰί‹€Χ–"  "  "
ΰ°΅ΰυϋμ"  "  "Σ
€ΏΌΨ§ΐΜχ8"  "  "Τ
 εΗ™¨ΐΘ»„"  "  "Υ
ΐ‡—¦©ΰςΝa"  "  "Φ
 ¥τ™€ΠΪd"  "  "Χ
ΐ‡‡«ΐ—@"  "  "Ψ
 ά‡Π«ΰ§ό0"  "  "Ω
ΰΠΒ¬€Μΰ"  "  "Ϊ
ΰθΞ­¬ΰλ‡
"  "  "?ΐ‘ω”­€Ώί€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϋ
€Ύω–­ Χ“+"  "  "ά
€ΨΕ­€φΰB"  "  "έ
ΐ®ΰζθ"  "  "?ΐλ–®€”ΦΡ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή
ΐ–®€ςΠ,"  "  "ί
ΐ›Ζ®€ϋΚ-"  "  "ΰ
ΰ•ό® ‡("  "  "α
ΰήΰ­―ΰ’Ό"  "  "β
ΰρΝΝ―ΰτλ;"  "  "γ
€ΰ©–°ΰΏ°—"  "  "δ
€ΘΞ°²ΐ™ηC"  "  "ε
ΰΟ‘½³€τ–Κ"  "  "ζ
€‹ζµΰ¥τ%"  "  "η
ΰ®‡Ύµ€ΆΚ)"  "  "θ
ΐΝ‰ξµ€‰Έ“"  "  "ι
ΐω„·€Ί΅4"  "  "κ
 ΏΔ·€µΝ"  "  "λ
 ωΰ·€²"  "  "?ΰνΩι· Ο±‘" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "μ
 ‘ΰκ· ‚ξ$"  "  "ν
ΰω‘Έ ρY"  "  "ξ
ΐΪ®σΈΰΠ™"  "  "?ΰ°Ι€Ή α½κ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ο
€ΗΙΉ ±&"  "  "π
€νξ¨Ή ―…B"  "  "ρ
€ϊβπΉ€µ‚1"  "  "ς
ΰ΄—©ΊΐΨ"  "  "σ
 ρΈΊΊ ω¨<"  "  "τ
ΐ³φƒ»€ϋιv"  "  "υ
€νΤώ»ΐ¦ΌB"  "  "φ
ΐΙνΖΌ€‚όω"  "  "χ
€¥ΜΗΓ ‹>"  "  "ψ
 ΏΖΔΐΦκW"  "  ">ΰΈαΝΐΰa" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ω
 „ΪγΝ€Ωκ4"  "  "ϊ
€τ¦ΞΐγΥ%"  "  "?€ΖΊΙΡΐρ¬Ό" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋ
 £ΗΜΡΐ½ξ"  "  "ό
 ²βθΐΒ„"  "  "?ΰΘ²΄ΣΰΏ‘Ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύ
ΐ΅‚·Σΐƒ΅Ύ"  "  "ώ
€ίΪψΤΐλΦ"  "  "? …Ρ®Ϋ ΡΌ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "
€©±Ϋΐ½–Τ"  "  "€ ©’³ίΐ‰Ζ4"  "  "?ΐΏ±ύί δΌ‹" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ·¬ƒΰ ³«'"  "  "‚ΰΓ§­ΰ  έP"  "  "ƒ ςΞƒα€­β"  "  "?€…Φ‰α εΐ«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "„ΰΠΠα€Ηφ2"  "  "…ΐήΆΑαΐΗΨ5"  "  "† ϋΡώαΐϊC"  "  "‡ΐϋ΅Ιβ€Χ­"  "  "ΐρΫβΰΡΗ="  "  "‰ΰΫ™¦γ ½Σ^"  "  "ΐΈδ€φ‹"  "  "‹ΰΆΈε ³µε"  "  "ΐ§μ€αa"  "  " πωμ€μ«¶"  "  "> «Ξυ ³Κp" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Ή†ΡυΐΨςb"  "  "ΰΟΌ¶φΐ­Δ"  "  ">€ΜΏϋ ο‰" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ύόϋΐΨ§{"  "  "? ίμ…€τα±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘ΰ”η… αΪ«"  "  ‹h\
tf_ComputeμΤ
"= §“ρ"€ίγ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’€¬τ"ΐα"  "  "=ΰτδώ"ΐκ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ΰ¨€#€σ"  "  "=ΰγ΄‚#ΐ–±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”€δεƒ#€Τa"  "  "=ΐϊ–†#ΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•€΄μ†# ¬O"  "  ">ΐΰμ#ΰί¬ν" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "–€γσ( κα"  "  "=ΰίΧ*ΰΤτ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—ΰ„Ϊ*ΰ½"  "  "= –ΰέ*ΰ±Π" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰό—γ*ΐξm"  "  "= ξΡτ*ΐ€β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™€‰φ*€‰z"  "  "=ΰ‰Άϊ*ΐΥ‡	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ώΑ‚+ΰ‘C"  "  "=ΐ‹ωƒ+ΰ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "›ΰμΰ„+ΰ‘C"  "  "=€ƒα…+ΰΟΥ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Ά†+ αg"  "  "=ΰρ‡+ΰε¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " «Υ+€κ0"  "  "=€„¥‹+ΐ·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ½ϊ‹+ χ6"  "  "= ·›+ΰ¬±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Ό‰+€‰z"  "  "=€• +ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ΐƒψ +ΐ„="  "  "=ΰΘ·Χ1ΰχ‚4" λΊ¦ή‚ύΑ"  " "λΊ¦ή‚ύΑ" "΅€έδε1ΐλ‹!" "  "=ΐΕ§2 γ¤$" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά€ς©2€ΏΦ"  "  "£ΰΛΆ±2ΐ„ς"  "  "= ‚†ζ;€Γ›5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ΰγθ;ΐΧΔ,"  "  ">ΐ¦‰<€―ρ¶" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥ΰρ΅< υ·o"  "  "¦ΐ²£= ΣΣ›"  "  "§€²Ζ«@€ί‚V"  "  "¨ΐά’„Aΰα¦ί"  "  "© ­‰ηBΰΗβ·"  "  "ΐΫ£¤GΰύΒ"  "  "« ΎI€πύ9"  "  "¬ΰƒώόI ΓO"  "  ">ΰ¶βΣJ ²³Ν" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "­€΅ΔΥJΐψΰ"  "  "®€ςΓφJΰ£Χ"  "  "―ΰ½¤ΠM€ΥΔO"  "  "°ΐΦ­¨N ρς@"  "  "±€³λN€ΙΙ%"  "  "² Ζδ‘O ³•X"  "  "³ΰ“οO ΄ο,"  "  "= Κύ΅PΰφR" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄ΐ¤PΐΤβL"  "  "=ΐ…ΙυP€ΒΑ`" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µΰΊ’χPΐρΨZ"  "  "=€ΰΦQΰµe" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶ ¶©ΨQ ΄Ω]"  "  "=ΰΥ¨½R Μά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "·€ΦΩΎR€σ"  "  "Έ μΐR Λ"  "  "= ™ΌΔRΰ¨υN" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ή Έ…ΕR€Λ†I"  "  "= ξ€”S ΉΓH" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ί€Τ¥•SΐΰσE"  "  ">ΐ»Tΰη‰θ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "»ΰΪ¤T ΅·%"  "  "Ό ®ΛT€ΘΓ²"  "  ">€κƒV€‡£³" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "½ΐΤΆ†V †θk"  "  "Ύ ρ»τV €›2"  "  "Ώ€Ο”«Wΰηµ†"  "  ">ΰ··XΐΘ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐΐθΈX οµ"  "  "Αΰ•γΦXΐ‰Ψ¨"  "  "Β€ΩΑ[ΰ‚0"  "  "Γ€ηΔ¶[ Ϋ£A"  "  "Δ Ωϋό[ΰέ£"  "  "Ε ‡\€”ζ="  "  "ΖΰΠ·ΰ\ΰ„£"  "  "=€Ό’€]ΰ‡Ω[" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ηΰ‹θ] μV"  "  "= ύΐά] κ€K" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Θ€Ν–ή]ΐ’E"  "  "=ΰ“‘¨^ΐP" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ιΐωµ©^ ϋ°G"  "  "=ΐƒ“ω^ΰΘγ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΚΰΧ¥ϋ^ΐΛΙ"  "  "Λΐ„Χώ^ΐα"  "  "= ·ύώ` Ϋr" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μΐ‹a€ηνi"  "  ">ΰ•ς³bΰ¨±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ν ΨΑµbΰΌΫ"  "  "Ξ ΣΰΔcΐΆΊΕ"  "  "ΟΐδΉhΐΘπ"  "  "ΠΐΓΫ«i ΘΏ›"  "  "Ρΰ§Ιj αΦΠ"  "  "ΐΗn€ϊ®λ"  "  "Σ€θδp€ΈM"  "  "Τ€…Ϊp€Τµb"  "  ">ΰΤ£ΓqΰΊχι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΥΐοΰΔq ­Θ"  "  "ΦΰΝζεqΐΑ‡΅"  "  "Χ€”άt€ς…E"  "  "Ψΰ³ΧΦtΐθ©A"  "  "Ωΐ†γ™uΰ¥τ%"  "  "Ϊ Λ ΐu€ΒΧ/"  "  "Ϋ€τΰφuΐΒΉ2"  "  "=€ώ®vΐΨ½J" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "άΰΊ±v€ΚΒC"  "  "=ΰ‚–ωvΰώ©B" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έΰΩϊv ς<"  "  "=€»•Όw€ΆA" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή€γΨ½wΰ—:"  "  "=ΐ–κώwΐ»έ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ίΐ±Ψxΰξ"  "  "ΰΐΩ…xΰξ"  "  "= Ιώxΰύε<" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "αΐίώ‹xΰΗ‰7"  "  "=€ζ­Θxΐσ«M" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β ›χΙx µθJ"  "  "?ΰμί”‡ΐΌ‹" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "γΰ³μ–‡€ζƒ"  "  "> η΅ Θλ9" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δΐΡώΆΰχ‚4"  "  "?ΐ™ΉΫ€‚Ρ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "εΐδά Ι™p"  "  "ζ υ¨Π‰€ώΠυ"  "  "η ΗΫΖ€ρ«p"  "  "θ€½υΈ€›"  "  "ι€ή„ΧΐψξΈ"  "  "κ€ΞΫ‘” γΫ"  "  "λΐ‡ξ•€ΆA"  "  "μΐ΅Χ΄–€Χs"  "  "?ΰ†£­—€ξνψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ν€Όμ®—€Ξ"  "  "ξ ώΝ—€†ί­"  "  "ο λΈύ™ ¤ΈN"  "  "π€ΧίΠΐώψ@"  "  "ρΐύ›“› ’Ϊ"  "  "ς€ΌΕ―› Β1"  "  "σΰΚΫθ›€Ρ΄9"  "  "> γώ¦€¤‡M" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "τ€³Τ¨ΐάH"  "  ">ΐ³Υτ€Δ”S" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "υ€™ψ€Π¥L"  "  ">ΰ£ΉΘΰύU" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φ€¤κΙ ζ¥M"  "  ">ΐωΐό†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "χ Ξ…΅ –€"  "  "ψ φΘΆ€Τa"  "  ">€Ο¥ οκ5" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ω ΖΟ¥ π0"  "  ">ΰχΨΫΐθ©A" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϊ€ψ‰έΰω?"  "  "> –¨·¬ΐαμc" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋ€…ΗΉ¬ΰΈ–^"  "  ">€οΛ›­  σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "όΐ’­ΐ’‹"  "  ">€§Ου­ άm" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύ€Ο’χ­ΰ€ηe"  "  ">ΐΥ«δ®€«ωB" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώ€‚¬ζ®ΐπί<"  "  ">ΰΗβ― δ‡s" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ­‡¬―ΐo"  "  ">  ΩΖ€ύ‡Q" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΰΏ„΅ΖΐΉτI"  "  ">ΰ΅οΖ€πΘ!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰπΖΰόμ"  "  "?€Τ…‘Ηΰ„" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚ΐΒσ‘Η€…‰4"  "  "ƒΰ¥ΝΞΗ€†‹Μ"  "  "? ¨ίΙ ‚Λ€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "„ΰ¨ΑΆΙ€·ΏT"  "  "…ΰρτψΙΐφ0"  "  "†ΰ«–­Κ Φξn"  "  "? ρ…΅Λ€Ύ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ΰΙ¤ΆΛ ……"  "  "ΐ½—ΐΛΐΩ©“"  "  "‰€τΥΝΐΡα#"  "  " –€ΞΐΕ…C"  "  "‹ΰΧΒΕΞΐ΅θ&"  "  " τμΞΐ¥)"  "  " Ή›Ο€ζ"  "  ">ΰ”ΌΟΐ9" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Σέ½Ο€Ί΅4"  "  ">ΐΛσυΟ ωέT" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐσ¶χΟ€ΏυO"  "  ">€ρ ΛΠΰρΤC" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΦΕΜΠΐΡ–<"  "  ">€ΕΡΐ±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘ΐ²ΛΡ€Ύ’"  "  "’ΐΪ’Ρ€Τa"  "  ">ΐυό”Ρΰ›λ7" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ΰ΅Μ•Ρ δ2"  "  "> °±ΝΡΐ‡Ύ)" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "”€ΛξΞΡ ώ’'"  "  "?ΰ—τδΣΐύάώ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "• ωηΣ ο’ω"  "  "?€ά¬δΥ ΅Ι™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "– άέεΥ€““"  "  "?€ΔΡώΧΐΦΐ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—ΐσΌΩΰςυ¤"  "  "ΰ¤ϋδΫ€ρ'"  "  "™ΰ΄Ρά ¬O"  "  "€ΎόάΰΗ"  "  "›ΰψΖ–ά€±½"  "  "ΰη–άΰ½–"  "  "€κ²άΰ„ξ"  "  "€Ϋ§µάΰ„ξ"  "  "€ΧΜ·άΐΉU"  "  " ΐ―λΈά€I"  "  "΅€Ίάΐξm"  "  "> α»ΏάΐΠθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΆΰΟ©ΐάΐ«"  "  ">€τΒΕά ¶" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£ΰΊΖά€­β"  "  ">€ςοά όΥ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ΰƒ³Σά€I"  "  "¥ΐ¦Τά χ6"  "  ">ΰ‹ηάΐ²γ
" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¦ΐΕ¤ιά Ίθ"  "  ">ΰ– σάΐΙφ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§€ψ‡τάΐ£†"  "  "¨ΐψιφάΐή
"  "  ">ΰ―»ƒέ ά²." νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©€ά„έ€›ξ"  "  " ξ―‡έΰ‘C"  "  "«ΰε—‰έ€ΏΦ"  "  "¬€»ξέ αg"  "  "­ΰΥ«’έ £"  "  "® πμέ Λ"  "  "― ΝΘ¬έ €±"  "  "°€®έΰ‘C"  "  "±ΐ‹―έΐΟ$"  "  "²ΐίν―έ Β"  "  ">ΐΫ’²έ ±ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "³€ΰΟ²έ€Η"  "  ">ΰΑ™¶έ Ίθ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "΄€ξθ¶έ€›ξ"  "  ">€±»έ ±ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "µ ¨Ρ»έ€¨Γ"  "  ">ΰ±­Ώέ Κ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶€ήόΏέΐΉU"  "  "·€Αέΰά*"  "  ">€σΌέΐ ο" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Έΐ€τΣέ ¨τ"  "  ">ΰωηχέΰΠ™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ή ¦θωέ€›ξ"  "  ">ΐΘ®ή€Ω€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ίΰή®ήΰΖ["  "  "> ¨θΞβ€υ²" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "»ΰΠΠβ Γβ"  "  ">ΰ΅ΨΫβΰχ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό Ζάβ€’τ"  "  "> ¶Αε€΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "½ΐΥ–Γε ήΠ"  "  ">ΐ‘ƒ΄κ ‚Ή" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ύ –ρµκ€λτ"  "  ">ΐ•“ΚκΰΕ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ώΰ«“Λκΰϋs"  "  "ΐ€‰ Ξκΰ’‡"  "  ">ΐπ›Ωκ υ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Α ΝάκΐΒΟ"  "  ">€»‹‡λΰΪΧ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΒΐΡΌ‰λ€Ύ’"  "  "Γ ‹Γ‹λΐχη"  "  "Δΰ¦“‘λ€…"  "  ">€γƒ΅λΰβΒ!" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Εΐ¥ΣΆλ ¤™"  "  "Ζ Ά‹©λ »"  "  ">€µζμ Μ‘" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Η€έ©’μΐέ½"  "  "Θ Γ—μ Λ"  "  "Ι€·ξ›μ ’¥"  "  "Κ€ϊ μΰχ"  "  "Λ€½Ρ¤μΰ‘C"  "  "ΜΰΈΕ¥μ€κ0"  "  "ΝΐΣ‚§μ Η½"  "  "Ξ ‰®«μ αg"  "  "ΟΰΗΆ―μ€κ0"  "  "?€Γ½§νΰΉ²" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Πΰ’“©ν€³δV"  "  "Ρ „μƒξΰήπ"  "  "€ƒκ§ο€―"  "  ">ΐƒ©†ρ π0" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΣΐΑ»‡ρ Θ	"  "  "Τ ή€”ρΰ…η"  "  ">ΰ²Φ‹σ Ω±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Υΰ®ϋσΰΉ†"  "  ">ΐ―’σ€Π†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Φΐν “σΰΟΥ"  "  ">ΰφΛ•σ ΐ€&" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Χ °΅–σΰό·"  "  "Ψ ηΑ΅σ€‰z"  "  "Ωΐ°¥σ€Δ"  "  "Ϊΰδ¬σΰ€“"  "  "Ϋ ¦²σΐΉU"  "  "άΐΤο³σΐξm"  "  "έ€¶¶σ ΎΓ"  "  "ή Θ­Ήσ ¬O"  "  "ί€γκΊσ€I"  "  ">€έΌΚσΰχ΅}" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ€›ΟΛσΐηϋ
"  "  "αΐυυΧσ Ντm"  "  "?ΰ¨ΐΘτΐΩΘά" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "β  ¨Κτ€Η½Ώ"  "  "γ€ρςχΰϊδ"  "  ">€¬£°χ ‘@" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δ €¶²χΐΜ"  "  "εΐ™Όχ€έΕ2"  "  "? Ά¥ψ ΐόΚ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ζΐΑ™§ψΰ·@"  "  "ηΰΔ€κψ ΏΫi"  "  "θΰαΚΩωΐΛ‡•"  "  ">€¥©„ό ΐΛ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ιΰΏζ…όΐ·"  "  "κ Φ—όΰχ"  "  "λ Έ’όΐ€β"  "  "μΐ΄θό€Τa"  "  "?ΰΧ°»ύΐΌΞ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "νΰµΠΒύΰδΏ"  "  "ξΰ„»ƒ€¶ά"  "  ">ΐ Ό€‹·$" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "οΰλΤ‹€‹‚"  "  "π £Χ™ κα"  "  "ρ σέΐΉU"  "  "ςΐ¨§ΐξm"  "  "σΐΠκΰϋs"  "  "τ Ώ‰Άΰά*"  "  "υΐΏΊ£€I"  "  "φ Δ¨¥€κ0"  "  "χΐΝΣ§ €±"  "  "ψ€δ„ Π·"  "  £s”!
tf_ComputeμΤ
"=ΰξΜέ"ΰξΣ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ωΐ±Νΰ" ο"  "  "=ΐΠΛω"ΐ’Φ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϊΐλΉό"€Ύ’"  "  "=ΐχώ"ΐκ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϋΰη€#ΐΒΟ"  "  "= ³Ω‚#€Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ό ‡»ƒ#€‰z"  "  "=ΰθΣ…# µΙ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύ Ά©†#ΰΖ["  "  ">€Ξ‡#ΐ‰±©" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ώ ΄ς­)ΐ¬€"  "  "=€ύ“Μ*ΰ‰" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰλ²Ξ*ΰ½"  "  "= ίΏ*ΐέ½" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΰ³ƒΦ* ¬O"  "  "= ΰƒΨ*ΐµϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ϋΐΩ*ΰϋs"  "  "=ΐ‘ςΫ* ΎΓ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‚ΰ§ςά*€‰z"  "  "= ¦¶…+€›ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒ€Ϋ†+€Ύ’"  "  "=ΐΈΫ+ΰε¤" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "„ΰδ‰+ χ6"  "  "=ΐ•·+€σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "… §ϊ+ ¬O"  "  "= Ί²’+ ‰«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "†ΰ’Ρ“+ΐΉU"  "  "=€Θ•+ΐ–±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ΐΥΡ–+€I"  "  "=€γ+ΰ–β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΪ΅›+ΰά*"  "  "=ΰΦΎ/ ‰«" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  ">€…¬Β/ ‘ΗΣ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‰ΰΥΕΙ/€Ω±Α"  "  "= Τ…—2ΰΟ©c" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Ωσ2€θόV"  "  "‹€ι³ρ2€ΡΚ"  "  "= °Ψ;ΰ·X" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰάΨ;ΐψΚP"  "  "=ΰ‹βι; ΣΈ-" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΦλ;ΰμ€("  "  "= ΦΡ—<€ς…E" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐμΡ<€°B"  "  "=€ΥΏ„_ΰ—:" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€χ‡_ΰ³0"  "  "=ΰΐ±Ώ_ΰΚ /" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€Κΐ_ΰΞϋ,"  "  "= Υ‚ϊ`ΰωτo" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘ΰƒό`ΰέΒg"  "  "=€…ΤοaΐδΞC" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "’€­—ρaΰΪΑ>"  "  "?€ΗΟ¦―ΰ§ΧΉ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ Η€¨―€ιυt"  "  "”ΰΟ΅ °ΰΆψ›"  "  "•ΰΣ½³ΰ―ηD"  "  "–€°ιƒ΄€Μι—"  "  "— ζ΄µΰΔ"  "  "€Γ»ΐ–Γφ"  "  "™ Υ¶Ά½ΐ®§V"  "  "€µΜό½ ΣΆ^"  "  "?ΰχ αΎΰ›®ι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "›€β‚γΎ Πμ"  "  "€®γ€Ώ £Ρ¤"  "  " γ¨§Α Φ„>"  "  "ΰψ΄νΑ€«ωB"  "  " Ίί²Βΰ‘­1"  "  "  ­τδΒΰ―²,"  "  "΅ ©ƒΓ °”/"  "  ">€Ν¤ΛΓ Τ±K" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ά ΝΥΜΓΐινF"  "  "> ΐ—Δ€Ϋ§X" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "£ΐΐΠΔ πγS"  "  "> ΊπΔΐΌF" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ βΣρΔΰΘΝ<"  "  "> σΆ·Ε Ω±" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¥ΐσΣΈΕΐ£†"  "  "¦€‹ΊΕ€Τa"  "  ">ΐνΌΕ€ψθM" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "§ΰΏΕΰ«G"  "  "> ‹™‹Ζΰε2" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¨ΰγ·Ζΐ‘ό/"  "  "?€φύΰΡ η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "©ΰϊλβΡΐσΚ–"  "  ">€οΙύΐβ‘ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ΊβώΐΗ£"  "  ">ΰ“ϊΣΐ”Θ@" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "«ΐΦϊ Σ€Ϋ½'"  "  "> ±ΌίΣ€Σ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬ΰσ‹αΣ ύ™	"  "  "­ ΈγνΣΰµ«"  "  ">ΐϋΕσΣ ο€" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "® ακτΣ ϊ"  "  ">ΐό‰ωΣ Ρε:" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "―ΐο΄ϊΣΐέ½"  "  "°ΐ¥‘€Τΐ¨Ϊ"  "  "±ΰ‡ςΤ σΫ"  "  "²€„ΘΆΤΰ„ξ"  "  "³ Β‹¥ΤΰΟΥ"  "  "΄ΰ£¤§Τΐξm"  "  "µ€««Τ  Ύ"  "  ">ΰμΈ΄ΤΐΗξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¶ΰίγµΤΐχη"  "  ">ΐφΕΉΤΐ€β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "· §ΊΤΰε¤"  "  ">€αΨΌΤΰ¬" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΈΐΟΖ½Τ β«"  "  "> Ώ©ΕΤ ›" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ή ύ»ΖΤ€Η"  "  "> ·σΙΤΐ·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ίΰ¥αΚΤ χ6"  "  "?ΰ®ΫΛΤ€ΐΒΟ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "»ΐυ¶ΜΤ —Λ"  "  "> ’¤ΦΰΗ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό€­αΦΰχ"  "  ">ΐ©θΆΦ€΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "½ ¤Φ€Τa"  "  "ΎΐΔΦ¥Φ ®Χ"  "  ">ΐ£Ί²Φΐµϊ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ώ€’¨³ΦΐΉU"  "  "> ›ΣµΦ€­W" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΐ Ωε¶Φΰύϋ"  "  "ΑΰρΔΦ€“Έ"  "  "ΒΰΨΈΜΦΰ‘C"  "  "Γ ³ΟΦ€²"  "  "ΔΐώχΩΦ€Π†"  "  "Ε ςµίΦΐσΑ"  "  "Ζ€ζ¨ύΦ€ό¤"  "  "Η ——Χΐ¬€"  "  "Θ€Ϊ—„Χΐα"  "  "Ιΐ’‡ΧΐΨ"  "  "Κ€τ‰Χΰ½"  "  ">€ΓΧ€ΘΠ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΛΐΆΚΧΐζ·"  "  ">ΐ–Ή•Χ ϊ‚" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Μ ΗΕ–Χΰ·³"  "  ">ΰϊμ¦Χ Π·" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ν€‘ν§Χΐ–±"  "  ">€κν«Χ€™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξΰ°Ι¬Χ€‰z"  "  "Οΐµ·®Χ –€"  "  "> ο½°Χΰ©" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Π€¶™±Χ€έ	"  "  ">€ƒ‰ΌΧΐ€β" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ρΰθ­½ΧΐΨ"  "  ">ΐν›ΏΧ€έ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ά‰ΐΧΐΜ"  "  ">ΐζ©ΙΧ€ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Σ€Υ—ΚΧΰά"  "  ">€ΦΫΟΧ€Αή" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Τ ΦΡΧΰ°"  "  "Υ€Ά‡ΥΧΰλ‡
"  "  "> Γ‰βΧ " νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Φ ΦΑιΧΰΨΟ"  "  ">€«¶ξΧ€—“" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Χ€„·ςΧ αg"  "  "? Ή€τΧΰΨΛ§" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ψΰά†υΧ ΚΠ "  "  "> ―ΡΚα€ΰ§" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ω€ΉήΟα€η™"  "  "> ›Δε φ§" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϊΰ¶”“εΐΏΈ	"  "  ">ΐϋ΅ε Τ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ϋ€£Άε€‰z"  "  ">ΐννΝεΐκ’" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "άΰΆ·Οε –€"  "  ">ΐ°¨Μζ Ρϋ	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έ€ΎίΝζΰκΓ"  "  ">€®—Ώκΰ†φ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ή€ΦΪΐκΐΓ“"  "  "> ©ήΥκΰκΓ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ί€ΪκΦκΐχη"  "  ">ΰώεήκΐ…" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΐ©ίκΰαΙ"  "  "> ν½λ ώ’'" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "α€§Δ’λ Π·"  "  "β κ¦λ Γ—"  "  "> ΖΗμΐΥ‡	" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "γ ££μΐο±"  "  "> σΡΦνΰίφ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "δΐ’ΜΨν σΫ"  "  "εΐ–έν γ	"  "  "? —θνΰΕη" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ζ€εΌινΰΊ°έ"  "  "η€¥½Λπΐζμ"  "  ">€Ωϋφπ ‡Ψ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "θΰέιψπΐΎτ"  "  "ι ®ώπ€ΘΠ"  "  ">ΐμΚ™ρΐ‘ό/" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "κ Μ ρΰ¤ϋ"  "  "λΰΌ­ρ€Τa"  "  "μΰΪΌ±ρΰΗ"  "  "νΐφ½Έρΐκ’"  "  "ξΐΨΈ½ρΰ‘C"  "  "οΰ£ΡΎρ χ6"  "  "πΐσ¦ΐρΰ£·"  "  "ρΰΊδΓρ ¬O"  "  "ς ‚ΣΘρΰ‘C"  "  "?€Εβρ€Δθ΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "σ€δΡδρ †”
"  "  "τ όΩπρΐξm"  "  "υ ¶‘τρ€ΒΧ/"  "  "φΐηι¨ς€›ξ"  "  "χΰ®§¬ςΰ‰"  "  "ψ ΄¨²ς€Ω€"  "  "ω α·ςΰΖ["  "  "ϊΐ–ΤΈςΐ„="  "  "ϋΰ–…Ίς€I"  "  "όΰΊς χ6"  "  "ύ€ΥΎςΐφ0"  "  "ώ€ΝόρςΐΉU"  "  "€™¨χςΐϋ"  "  "€ΐΞς –€"  "  "€ψ‹„σ€’τ"  "  "‚€Ρσΐξm"  "  "ƒΰΘ¥‹σΰΒ€"  "  "„ΰ·υσ€ό¤"  "  "…ΰφ’σΐ–±"  "  "†€Ψ³–σΰΖ["  "  "?€χοΑτ §ξ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‡ΐξΧΓτ »¬
"  "  "€ίΝΟτΰϋs"  "  "‰ΰωΡτΰϋs"  "  "ΐΙΰτΐΟ$"  "  "‹€νζΣτ αg"  "  "ΐ™ηΥτΰΖ["  "  "€„ϊΨτΐ„="  "  "ΰεΓάτΰβ	"  "  " ΔΕζτΰΖ["  "  "ΰΪφθτ€—ηf"  "  ">ΐΆ‰φΰΝ‚" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "‘€ρ›φ Π·"  "  "’ΰ¬ψ΅φΐ¤Κ"  "  ">€Κ‰βφ€΅" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "“ ΉηφΰΨΟ"  "  "”€ΠμκφΐΠθ"  "  ">ΐΖΕφΐ¥" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "•ΰεΏχ ζ†"  "  "–ΐΩύ†χΰ€“"  "  ">ΰχΦψ ν­" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "—€€ψΫψΰχ"  "  " ¤‘αψΰΊΚ"  "  "?ΰΫύωΰλ―Ν" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "™€ά®΅ω€ΘΠ"  "  "ΐ›ηωΰ§Ύ"  "  ">€΅Εϋ€η™" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "›ΰΪΛϋ Ω±"  "  ">€µ•ϋϋ …Π" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰΟόϋ€ζΥ"  "  ">€Χό „Α" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€ΚΥόΰΤτ"  "  "ΰάά“ό έ"  "  "ΰΤ¦όΐ„="  "  " ΰ’Ή™ό€I"  "  "΅ΰΊόόΐκ’"  "  "Ά€‹΄όΰΖ["  "  "£ΐ”£όΰ‘C"  "  "> Ε¤ό °ί" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¤ΐρλ¤όΰ„ξ"  "  "¥ΰϊ–§όΐ„="  "  "¦ΐ©ό€Τa"  "  "§ΰ°«ό ¬O"  "  "¨ΰ‘¬ό χ6"  "  "© €­ό€I"  "  "ΐ—ά³όΰε¤"  "  "«€®¶όΐ’Φ"  "  "?ΰ·»ό Ρξ΄" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "¬ΐΙέ»όΰ…²"  "  "­ΰϋΕόΐΉU"  "  "®ΐ±»Ιό€ζΏ3"  "  "―ΐ‚π‚ύΰ£·"  "  "°ΰίό…ύΰΎ¥"  "  "± εύ‹ύ€™"  "  "²ΐζύ ¬O"  "  "³€βµ’ύΐ„="  "  "΄ —“ύ€Τa"  "  "µΰΊ…•ύΐΟ$"  "  "¶ΰήνύΐΥΌ!"  "  "·ΐ°€ΎύΐΉU"  "  "Έ€—ΈΓύ€Ζύ"  "  "Ή€Ώ°έύ€Ύ’"  "  "ΊΰγύΰαΙ"  "  "»ΰθΚθύΰζθ"  "  ">ΐΒήπύΰς" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ό ¨ƒςύΐΉU"  "  "½ ζ•σύΐΉU"  "  "Ύ€Ώευύΐ’Φ"  "  "Ώ€Ϊϋύ αg"  "  "ΐ€Ϋ—ώύΰ£·"  "  "Α€ΰ¶ώ€±½"  "  "ΒΰΑ€…ώ ¤™"  "  ">ΐ·Ζ•ώΐ΄λ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Γΐρ–ώΐ–±"  "  "Δ€•„ώ ¬O"  "  "ΕΐΆ»›ώΰϋs"  "  "Ζ άΑώ€Ύ’"  "  "Η Ήώ ¬O"  "  "Θΐ„¶ ώ ¬O"  "  "Ι όΞ£ώΰ„ξ"  "  "ΚΐΩΫ¦ώ ›"  "  "> γθ«ώΰΒκ3" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Λ€”υ¬ώΐ¶Ύ"  "  "Μΐά§·ώΐ‹™'"  "  "> Άΰώ€”ό" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΝΐΌ„βώΰ­υ"  "  "> Θ™ςώ€εΖ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "Ξΐ“²σώΰ¤ϋ"  "  "Ο€όώΰΖ["  "  "Π€£ρώ κα"  "  "Ρ –Ν‚ΰά"  "  ">ΰ±€ΝΓl" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  " ©…€νe"  "  "Σ€—ρΰχ"  "  "?ΰΎω­ΐήήε" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΤΐΓη― ΓΜ6"  "  "ΥΐλθΰΰΊ"  "  "Φ ‡ΐ»’$"  "  "Χΰβκ«€»ε9"  "  "Ψ€Ύνΰε¤"  "  "ΩΰΒόξ€I"  "  "Ϊΰµ§π Εκ"  "  "Ϋΐι€τξ"  "  "άΐ¶οΐο±"  "  "?ΐΙ§”ΰέΝι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "έ€ΆΖ•ΰ Υ""  "  "ήΰ±λ» µ³2"  "  "ίΐΌ¦χΐε7"  "  "ΰ€“ά¶€Ζ"  "  "α €Ι τ‰9"  "  "β °Ά‘ΐΥƒ"  "  "γΰ ΅•’ΐέάM"  "  "δΰδµθ’ΰΞ±‰"  "  "ε€ΗόΐΕΩ¤"  "  "ζ€πϋ­ΐΏΰΜ"  "  "? ΐχ§¤€²‚" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ηΰυρ¤ΰΐr"  "  "θΐΙ¥€έ	"  "  "?€·μ²ΐΔΰ†" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ι ‹·€—"  "  "? Ήάίµΰέα°" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "κ€ΗΔβµ€φ΄¤"  "  "?ΐλ‘κή σΧ§" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "λ€ΙΟξήΐƒΝ\"  "  "μΰΘςΞί€Εξ'"  "  "νΐΏ±ύίΐ†ϊ#"  "  "ξΐ€Άΰΰ‚…@"  "  "οΐΑκΰ Λ"  "  "π€Χμΰ αg"  "  "ρ Β ξΰΰυΕ"  "  "ς ƒΰ€ΏΦ"  "  "σΰ€Κα€τξ"  "  "?€ρΊγΰψχφ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "τΰ™αΏγΰ ―"  "  "υ ·µυε ήΊ9"  "  ">ΰλΙ’νΰΚΏx" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "φΰ†Έ•νΰ­Ιj"  "  "χ€¬ιξ τ"  "  "?ΐό™Ψυ ύφδ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ψ€ω έυΰΐλΆ"  "  "ω ©Ύ†χΰΛδ4"  "  ">ΐ€―ωΐφ¬v" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ϊΰΎςωΰΆΗ^"  "  "ϋΰ±ρςω€δ‚"  "  ">ΰΈ”ϋΰΖ{" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "όΐΖΎ–ϋ€ΟΛw"  "  "?€Ε•χ‚ΰ–Η" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ύΐίω‚€ΆΌ"  "  "ώ€Ο·„€ "  "  "? ίμ…€°ΦΨ" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ΰ”η…€εΉΐ"  "  "€ΰΰΊζ†ΐ»έ"  "  "? ΡΝθ”ΐπ¦Ι" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "€”Ξλ”€΄±Φ"  "  "‚€―™Κ–ΰΒΤd"  "  "? δβ€»" νιΟτίΛΊΖΣ"  " "νιΟτίΛΊΖΣ"  "ƒΰ³•ε ύΰ•"  "  "„ ƒώΐ„άI"  "  "…ΰ·‹Ρ€¶‘"  "  "†€€Βςΰ—¦	"  "  Σ„Etf_data_iterator_get_nextμΤ
"*Κ ν'ΰΞΓύ" νιΟτίΛΊΖΣ"  "  "#Ιΐ‰―‹,ΐΉ¥‡" ΔιϊΩίφ΅’x"  "BΘ€¥.ΐΕ›" “σ“Υξµ" ΔιϊΩίφ΅’x"8"	8"
 "  "ΗΐΫΨ‡/€’τ" "  δntf_data_iterator_resourceμΤ
"δ€ξγ¤/€ά¶E" 	"*α€κ§/ΐΆ¬," ΞΥώ›ΤυΣΆ" “σ“Υξµ"+ή€ψ΅«/ΰπ¦" ΚΒθΚξΒΤ‹" ΞΥώ›ΤυΣΆ"+έ ψ¬/ΐϋ" εµΨ‹–έπ„" ΚΒθΚξΒΤ‹"*ΰΐΞνΉ/ΐε¨" ΄“µ³•a" ΞΥώ›ΤυΣΆ"*ί€§»/ΰ‘C" Ίξδ¦οΖ΅„ή" ΄“µ³•a"+ίΐγ­Μ/ Ίθ" Ί“ϋώξΖ΅„ή" ΄“µ³•a"γΰεζΨ/ΐπ" λΊ¦ή‚ύΑ"8βΐΠέ/ γο" λΊ¦ή‚ύΑ" "λΊ¦ή‚ύΑ" "•†•|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/truediv:Mul"Mul"TΛOΛ7Adam/Adam/update_14/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΤοΞο»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΡΏΛΏ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_38/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ύ
Έ
­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_2_grad/Mul_1:Mul"Mul"ΟΒΙΒ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_43/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"’—’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_8/BiasAdd:BiasAdd"BiasAdd"ΘγΒγ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/SquaredDifference_grad/mul_1:Mul"Mul"ƒΖ~Ζtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_20/dropout/Mul:Mul"Mul"Όρ	¶ρ	«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_1_grad/Mul:Mul"Mul"ƒΓ~Γsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout_1/dropout/Cast:Cast"Cast"Θ–Β–±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/transpose_3_grad/transpose:Transpose"	Transpose"·*²*model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_30/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"TθOθ7Adam/Adam/update_51/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»ϊµϊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_1_grad/Mul:Mul"Mul"‰™‰…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_7/dropout/GreaterEqual:GreaterEqual"GreaterEqual"”ΈΈmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/Rsqrt:Rsqrt"Rsqrt"ΉY΄Ymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_48/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΈΘ²Θ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/mean_grad/truediv:Mul"Mul"U®
P®
8Adam/Adam/update_101/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"TώOώ7Adam/Adam/update_11/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"²ι¬ι—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_16/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ό™Ό…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_5/dropout/GreaterEqual:GreaterEqual"GreaterEqual"Ζΐ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/transpose_2_grad/transpose:Transpose"	Transpose"ΕΜΏΜ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_36/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"΄έ®έ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/Softmax_grad/Sum:Sum"Sum"ΡΰΛΰΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Ζ·ΐ·―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/transpose_grad/transpose:Transpose"	Transpose"νν‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_16/BiasAdd:BiasAdd"BiasAdd"»Η
µΗ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_2_grad/Sum:Sum"Sum"ΡδΛδ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_44/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Μ—Ζ—±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ί«΄«©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/sub_grad/Neg:Neg"Neg"ΌΪ
¶Ϊ
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/mean_grad/truediv:Mul"Mul"Ρ…Λ…ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"›΄	•΄	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/transpose:Transpose"	Transpose"{—{‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/transpose_1:Transpose"	Transpose"`‹[‹Qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/mul:Mul"Mul"‡――vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_13/dropout/Cast:Cast"Cast"†Ϋ	€Ϋ	vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_15/dropout/Mul_1:Mul"Mul"΄ξ®ξ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/Softmax_grad/mul:Mul"Mul"|‚w‚lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_58:AddN"AddN"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_grad/Mul:Mul"Mul"£¤£–Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dense_26/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_1:Mul"Mul"­α
§α
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/dropout_5/dropout/Mul_1_grad/Mul:Mul"Mul"¦† †’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_64/Tensordot/MatMul:MatMul"MatMul"ΚΑΔΑ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΥΦΟΦΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_21/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ήe΄emodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_44/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Γ”½”²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/SquaredDifference_grad/Mul:Mul"Mul"U¨P¨8Adam/Adam/update_156/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ζΜζΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_63/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"µ’µ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/add_1:AddV2"AddV2"ΥψΟψΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_14/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ί΄›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/moments/SquaredDifference:SquaredDifference"SquaredDifference"‡Ε	Ε	vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_12/dropout/Cast:Cast"Cast"ytiAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN:AddN"AddN"Ί™΄™©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_grad/Mul:Mul"Mul"¬¬‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_44/BiasAdd:BiasAdd"BiasAdd"TίOί7Adam/Adam/update_44/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ήό³ό¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_1_grad/Mul:Mul"Mul"”οο‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/moments/mean:Mean"Mean"„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_10/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΚΔ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/transpose_2_grad/transpose:Transpose"	Transpose"ΊΣ΄Σ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_grad/Sum:Sum"Sum"Β	Β	‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_40/BiasAdd:BiasAdd"BiasAdd"•γγ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_2:Mul"Mul"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_33/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"TΖOΖ7Adam/Adam/update_72/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"£model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_34/Tensordot/MatMul:MatMul"MatMul"T»O»7Adam/Adam/update_91/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_grad/Sum_1:Sum"Sum"ΠωΚω·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"²ϊ¬ϊ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_17/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"TΛ
OΛ
7Adam/Adam/update_57/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"y‰t‰gmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dense/Tensordot/MatMul:MatMul"MatMul"Α»°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/variance_grad/truediv:Mul"Mul"ΛΩΕΩ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_24/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"­®§®’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ή€
³€
¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/truediv_grad/RealDiv:Mul"Mul"ΠΚΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_59/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"­!¨!“model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_35/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΎΠΈΠ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_2_grad/Mul_1:Mul"Mul"`έ[έ?Iterator::Model::ParallelMapV2::Zip[0]::FlatMap[0]::TensorSlice"Iterator::TensorSlice"ΕρΏρ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/SquaredDifference_grad/Mul:Mul"Mul"ΰΰ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_47/BiasAdd:BiasAdd"BiasAdd"£„„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΈT³Tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_7/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"¥Μ¥ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_57/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΗΗmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/moments/mean:Mean"Mean"ΦδΠδ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_61/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Κυ
Δυ
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/transpose_3_grad/transpose:Transpose"	Transpose"«—«model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_9/BiasAdd:BiasAdd"BiasAdd"„ΪΪumodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_2/dropout/Mul_1:Mul"Mul"Π…Κ…·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_1/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Όϋ¶ϋ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/mean_grad/truediv:Mul"Mul"ΖΐµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/SquaredDifference_grad/Mul:Mul"Mul"ΎΆ
ΈΆ
­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_1_grad/Mul_1:Mul"Mul"Ή―³―¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_1_grad/Mul:Mul"Mul"¬Ό¦Ό›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_13/dropout/Mul_grad/Mul:Mul"Mul"ΐςΊς―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/variance_grad/truediv:Mul"Mul"’““model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul:Mul"Mul"Θ‚Β‚·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/SquaredDifference_grad/mul_1:Mul"Mul"SN6Adam/Adam/update_8/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"£	„£	ymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_11/Relu:Relu"Relu"^vZvQmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/Sin:Sin"Sin"’ααmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul:Mul"Mul"Ή†³†¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_grad/Sum_1:Sum"Sum" …model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/MatMul_1:BatchMatMulV2"BatchMatMulV2"Φ	Φ	‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_50/BiasAdd:BiasAdd"BiasAdd"ΐεΊε―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/variance_grad/truediv:Mul"Mul"|ΉwΉlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_32:AddN"AddN"†¨	€¨	umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_3/dropout/Cast:Cast"Cast"›ψ•ψ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_56/Tensordot/MatMul:MatMul"MatMul"”ΙΙ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/moments/mean:Mean"Mean"‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/mul:Mul"Mul"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/add_grad/Sum:Sum"Sum"TξOξ7Adam/Adam/update_83/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Σ™Ν™ΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_29/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"¬½¦½›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_14/dropout/Mul_grad/Sum:Sum"Sum"U¬P¬8Adam/Adam/update_117/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ί	’ί	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/moments/variance:Mean"Mean"TΝOΝ7Adam/Adam/update_82/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"―•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_36/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"§Μ§ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_61/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"™…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_6/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΘΣΒΣ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"TχOχ7Adam/Adam/update_34/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ύ‡Έ‡­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Α”
»”
°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/variance_grad/truediv:Mul"Mul"ΙΚΓΚ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_35/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"¥‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_22/Tensordot/MatMul:MatMul"MatMul"ΛλΕλ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/transpose_3_grad/transpose:Transpose"	Transpose"“ΞΞ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul:Mul"Mul"Ίω	΄ω	©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/sub_grad/Sum:Sum"Sum"–ΛΛƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/Rsqrt:Rsqrt"Rsqrt"TO7Adam/Adam/update_66/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»2¶2΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_22/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ήπ³π¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/add_grad/Sum:Sum"Sum"|ΒwΒlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_24:AddN"AddN"–‘‘ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/add:AddV2"AddV2"T€O€7Adam/Adam/update_26/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¬΅¦΅›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_16/dropout/Mul_grad/Sum:Sum"Sum"§Π΅Π–Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/dropout/dropout/Mul_grad/Mul:Mul"Mul"Ο¶
Ι¶
»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_32/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"«Ζ¥ΖAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_9/dropout/Mul_1_grad/Mul:Mul"Mul"ρ’ρ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/moments/variance:Mean"Mean"T„O„7Adam/Adam/update_17/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"§•΅•“Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/dense/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΉΤ³Τ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/sub_grad/Neg:Neg"Neg"Ρ¥Λ¥½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_15/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΦθΠθ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_64/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"|ΏwΏlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_34:AddN"AddN"Ίυ΄υ›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/moments/SquaredDifference:SquaredDifference"SquaredDifference"΄Ο
®Ο
£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/Softmax_grad/sub:Sub"Sub"Ίμ΄μ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/mean_grad/truediv:Mul"Mul"Ή³¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/sub_grad/Sum:Sum"Sum"Μ³Ζ³±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ά†Άzmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/Equal:Equal"Equal"΄®£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/truediv_grad/Sum:Sum"Sum"›Ό	•Ό	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/transpose:Transpose"	Transpose"ΉC΄Cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_39/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"“—“†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/transpose_1:Transpose"	Transpose"™·“·†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/Rsqrt:Rsqrt"Rsqrt"|φwφlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_48:AddN"AddN"ΚΔ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_4/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΚΪΔΪ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/transpose_3_grad/transpose:Transpose"	Transpose"Ί«΄«©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/sub_grad/Sum:Sum"Sum"Ά‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/MatMul_1:BatchMatMulV2"BatchMatMulV2"»ι
µι
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_2_grad/Sum:Sum"Sum"”‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/moments/mean:Mean"Mean"ΙΙIterator::Model"ΗήΑή³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_18/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"¶m±mmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"TO7Adam/Adam/update_74/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"U°P°8Adam/Adam/update_133/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΞξΘξ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"†½€½umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_5/dropout/Cast:Cast"Cast"Όΐ¶ΐ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_1_grad/Mul:Mul"Mul"›—•—„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/transpose_2:Transpose"	Transpose"™Ν	“Ν	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/Rsqrt:Rsqrt"Rsqrt"ΛΕ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_65/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Έα²α§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/truediv_grad/RealDiv:Mul"Mul"»΄µ΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/moments/SquaredDifference:SquaredDifference"SquaredDifference"§	™§	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_3/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΕΔΏΔ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/SquaredDifference_grad/mul_1:Mul"Mul"χ„χymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_55/Relu:Relu"Relu"UεPε8Adam/Adam/update_110/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»”µ”model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_41/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΜΫΖΫ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"»Κ	µΚ	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/moments/SquaredDifference:SquaredDifference"SquaredDifference"£λλmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/MatMul_1:BatchMatMulV2"BatchMatMulV2"·²model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_32/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΌΞ¶Ξ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_60/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΡβΛβ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_52/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Όƒ¶ƒ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_2_grad/Mul:Mul"Mul"Υ²Ο²ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_40/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΚΑΔΑ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/transpose_2_grad/transpose:Transpose"	Transpose"ΊΒ΄Β›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/moments/SquaredDifference:SquaredDifference"SquaredDifference"ΚΣ
ΔΣ
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/transpose_1_grad/transpose:Transpose"	Transpose"¤‘‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_7/Tensordot/MatMul:MatMul"MatMul"|ζwζlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_23:AddN"AddN"“¶¶model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/moments/variance:Mean"Mean"Ό
¶
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_2_grad/Mul:Mul"Mul"Υ•Ο•ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_47/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"·)²)model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_28/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΖΨΐΨµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/SquaredDifference_grad/Mul:Mul"Mul"TωOω7Adam/Adam/update_37/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"θθ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/transpose_1:Transpose"	Transpose"†€vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_19/dropout/Mul_1:Mul"Mul"µ\°\›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό­¶­«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_grad/Sum_1:Sum"Sum"Ο†Ο|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/truediv:Mul"Mul"Ίμ΄μ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_grad/Mul:Mul"Mul"Ήύ
³ύ
¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/sub_grad/Sum:Sum"Sum"”³³ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_2:Mul"Mul"ΈΓ²Γ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/sub_grad/Sum:Sum"Sum"Ίη΄η©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/sub_grad/Neg:Neg"Neg"U
P
8Adam/Adam/update_104/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Μ·Ζ·±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"»„µ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_54/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΤϊΞϊ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_8/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"›–›‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_34/BiasAdd:BiasAdd"BiasAdd"ΛΕ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/transpose_3_grad/transpose:Transpose"	Transpose"ΖΪΐΪ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/transpose_1_grad/transpose:Transpose"	Transpose"v«	q«	emodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/add_1:AddV2"AddV2"£~~‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_38/Tensordot/MatMul:MatMul"MatMul"•ιι€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/Softmax:Softmax"Softmax"ΚΔ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_3/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΛΆΕΆ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"ΡΰΛΰ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_50/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"»ή	µή	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/moments/SquaredDifference:SquaredDifference"SquaredDifference"ΜνΖν±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/sub_grad/Neg:Neg"Neg"°	’°	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/Rsqrt:Rsqrt"Rsqrt"vΐqΐemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/add_1:AddV2"AddV2"ΘΡΒΡ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/SquaredDifference_grad/mul_1:Mul"Mul"U¦P¦8Adam/Adam/update_164/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"•——€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/Softmax:Softmax"Softmax"ΘδΒδ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/transpose_grad/transpose:Transpose"	Transpose"φ‚φxAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/mul_grad/Sum:Sum"Sum"<α7α#Iterator::Model::ParallelMapV2::Zip"Iterator::Zip"……‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/transpose_3:Transpose"	Transpose"Ή΄³΄¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_grad/Mul_1:Mul"Mul"TΥOΥ7Adam/Adam/update_56/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"―•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_35/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Υ―Ο―ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_52/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UεPε8Adam/Adam/update_157/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΠΫΚΫ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_2/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΘΒ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/transpose_grad/transpose:Transpose"	Transpose"ΖΐµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/SquaredDifference_grad/Mul:Mul"Mul"|
w
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_10:AddN"AddN"Ι£Γ£²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/transpose_grad/transpose:Transpose"	Transpose"»µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_grad/Sum_1:Sum"Sum"TβOβ7Adam/Adam/update_69/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"µσ―σ¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/truediv_grad/Sum:Sum"Sum"Ζƒΐƒ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"ΛΦΕΦ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_66/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"¥Α	Α	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_40/Tensordot/MatMul:MatMul"MatMul"Σ—Σ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/transpose_3:Transpose"	Transpose")ζ$ζEagerCopyToDeviceAndAddCacheKey"Έ:³:model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_9/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΠψΚψΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_7/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΡΚΛΚ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_42/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"›™•™„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/transpose_3:Transpose"	Transpose"«¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/dropout_2/dropout/Mul_grad/Mul:Mul"Mul"Ό©¶©«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_grad/Mul_1:Mul"Mul"΄Ψ®Ψ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/truediv_grad/RealDiv:Mul"Mul"|Ύ
wΎ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_37:AddN"AddN"ΘΞΒΞ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"“ωωmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_56/BiasAdd:BiasAdd"BiasAdd"ΠΚ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_4/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ό¶ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_64/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Φ Π ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_59/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"TάOά7Adam/Adam/update_59/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"T­O­7Adam/Adam/update_32/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"·π±π¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/Softmax_grad/mul_1:Mul"Mul"Μ‡Ζ‡±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"¥ύύ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_20/Tensordot/MatMul:MatMul"MatMul"Ή$΄$model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_13/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"»Χ
µΧ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_2_grad/Mul:Mul"Mul"τ‚τxAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/add_grad/Sum:Sum"Sum"'η"ηValidateInputTypeAndPlacement"®»¨»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_14/dropout/Mul_1_grad/Mul:Mul"Mul"ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ί•΄•©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/sub_grad/Sum:Sum"Sum"Κ„	Δ„	³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/transpose_1_grad/transpose:Transpose"	Transpose"ΉK΄Kmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_42/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"››‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_22/BiasAdd:BiasAdd"BiasAdd"“ΏΏmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_65/BiasAdd:BiasAdd"BiasAdd"ΛώΕώ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_11/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"|’w’lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_60:AddN"AddN"TΆOΆ7Adam/Adam/update_58/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ό³¶³«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_1_grad/Mul:Mul"Mul"oΰjΰ[model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/concat_1:ConcatV2"ConcatV2"|ρ
wρ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_47:AddN"AddN"ΜΖΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_3/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"“––‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/sub:Sub"Sub"΄Ρ®Ρmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"±¦	«¦	–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_3/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"|uxufmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/strided_slice_1:StridedSlice"StridedSlice"δδParallelMapProduce"ΖΈΐΈµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/SquaredDifference_grad/sub:Sub"Sub"΄φ®φ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/Softmax_grad/mul:Mul"Mul"Ή…³…¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_grad/Mul_1:Mul"Mul"ΑΘ»Θ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/variance_grad/truediv:Mul"Mul"|rmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_10/dropout/Mul:Mul"Mul"¥Υ	Υ	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_50/Tensordot/MatMul:MatMul"MatMul"UP8Adam/Adam/update_159/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Όι¶ι«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_2_grad/Mul:Mul"Mul"›Ό•Ό„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/transpose:Transpose"	Transpose"–―	―	ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/add:AddV2"AddV2"TΜOΜ7Adam/Adam/update_76/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"vqemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/add_2:AddV2"AddV2"ΣνΝνΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_33/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"΄®model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"»£µ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_1_grad/Mul:Mul"Mul"•		€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/Softmax:Softmax"Softmax"†σ€σvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_18/dropout/Mul_1:Mul"Mul"΄―model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_3/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Φ„Φzmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/truediv:Mul"Mul"Ήκ³κ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_2_grad/Sum:Sum"Sum"°‹‹Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/sub_grad/Sum:Sum"Sum"°ΊΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/sub_grad/Sum:Sum"Sum"TΙOΙ7Adam/Adam/update_94/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΚΚ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_21/BiasAdd:BiasAdd"BiasAdd"»Ϋ
µΫ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_grad/Mul_1:Mul"Mul"™—™†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/transpose_3:Transpose"	Transpose"UξPξ8Adam/Adam/update_109/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"„ΎΎtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_5/Relu:Relu"Relu"Ί
΄
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_grad/Sum:Sum"Sum"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/add_grad/Sum:Sum"Sum"―11•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_18/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"T O 7Adam/Adam/update_65/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΎΑΈΑ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_2_grad/Mul_1:Mul"Mul" ππ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_18/dropout/GreaterEqual:GreaterEqual"GreaterEqual"•””„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_1:Mul"Mul"Υ¤Ο¤ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_53/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΥµΟµΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_50/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Χ™Χ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_2/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΚτΔτ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/transpose_3_grad/transpose:Transpose"	Transpose"ΉΕ
³Ε
¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/sub_grad/Neg:Neg"Neg"±»«»–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_5/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"ΔΛΎΛ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/SquaredDifference_grad/Mul:Mul"Mul"ΞοΘο³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"T²
O²
7Adam/Adam/update_97/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¶Ώ°Ώ¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/truediv_grad/RealDiv:Mul"Mul"`[Qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/Min:Min"Min"TΒOΒ7Adam/Adam/update_90/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"UγPγ8Adam/Adam/update_132/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ξ’Θ’³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"¥³	³	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_13/Tensordot/MatMul:MatMul"MatMul"»F¶F΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_10/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"‚ρ}ρsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_4/dropout/Mul:Mul"Mul"i–d–Wmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/Cumsum:Cumsum"Cumsum"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/sub_grad/Sum:Sum"Sum"ΟθΙθ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_42/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΟυΙυ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_14/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"΄‰®‰£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/Softmax_grad/sub:Sub"Sub"Έ	Έ	‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_52/BiasAdd:BiasAdd"BiasAdd"Θ’Θ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/add_1:AddV2"AddV2"ΏΩΉΩ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_65/Relu_grad/ReluGrad:ReluGrad"ReluGrad"U®P®8Adam/Adam/update_130/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‚}smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_6/dropout/Mul:Mul"Mul"½μ·μ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/mean_grad/truediv:Mul"Mul"»ώµώmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_27/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"=ι8ι3EagerLocalExecute: __inference_train_function_70431"vqemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/add_1:AddV2"AddV2"²Ό¬Ό΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/Softmax_grad/sub:Sub"Sub"TέOέ7Adam/Adam/update_49/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»`¶`΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_49/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"U·P·8Adam/Adam/update_118/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ίν΄ν©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_grad/Sum:Sum"Sum"ΥΗΟΗΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"Φ—Π—½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_62/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ώδ
Ήδ
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_17/Relu_grad/ReluGrad:ReluGrad"ReluGrad"U«P«8Adam/Adam/update_143/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΥΪΟΪΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_20/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΎκΈκ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Όϋ¶ϋ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_2_grad/Sum:Sum"Sum"Όή¶ή«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_2_grad/Mul:Mul"Mul"”Ι	Ι	‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/moments/mean:Mean"Mean"tΫoΫcmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/add:AddV2"AddV2"ξξtf.constant"ExecutorState::Process"Ήΐ³ΐ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_grad/Mul_1:Mul"Mul"ΟΙ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_31/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Υ£
Ο£
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_51/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UαPα8Adam/Adam/update_128/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_1_grad/Mul_1:Mul"Mul"¬±¦±›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_12/dropout/Mul_grad/Sum:Sum"Sum"Ί†
΄†
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/sub_grad/Neg:Neg"Neg"ΉΦ³Φ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_grad/Mul:Mul"Mul"“ΑΑmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/moments/mean:Mean"Mean"S”N”6Adam/Adam/update_1/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"®ε¨εAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_19/dropout/Mul_1_grad/Mul:Mul"Mul"U¨P¨8Adam/Adam/update_140/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"·µ±µ¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_grad/Sum:Sum"Sum"Ίψ΄ψ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/sub_grad/Neg:Neg"Neg"ΚΔ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/transpose_1_grad/transpose:Transpose"	Transpose"‚Ε}Εsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout_1/dropout/Mul_1:Mul"Mul"»,¶,΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_38/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ηί
Αί
¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/SquaredDifference_grad/mul_1:Mul"Mul"΄ό®όmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"+ο&οLogicalAnd:LogicalAnd"
LogicalAnd"–ƒƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/Softmax:Softmax"Softmax"€²{²qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout/dropout/Mul_1:Mul"Mul"·±¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_grad/Mul:Mul"Mul"Β	Ό	®Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_6/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"­S¨S“model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"Όγ¶γ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_1_grad/Mul:Mul"Mul"Π‡Κ‡·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΈΖ²Ζ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_grad/Sum:Sum"Sum"»µmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_32/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"½·¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_1_grad/Mul_1:Mul"Mul"®»¨»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_13/dropout/Mul_1_grad/Mul:Mul"Mul"ΛθΕθ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_55/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Υω
Οω
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_13/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΠΚΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_63/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"“ΩΩ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_1:Mul"Mul"—®	‘®	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/moments/variance:Mean"Mean"“}}ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/moments/variance:Mean"Mean"μμtrain_function"µ€―€™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΘώΒώ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/transpose_grad/transpose:Transpose"	Transpose"SN6Adam/Adam/update_9/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"·Π±Π¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/sub_grad/Sum:Sum"Sum"΅‰΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/mul:Mul"Mul"ΚηΔη³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/transpose_1_grad/transpose:Transpose"	Transpose"”έ	έ	‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/moments/mean:Mean"Mean"Ήΰ³ΰ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_grad/Mul:Mul"Mul"ΟρΙρ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_39/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"µ„―„¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_grad/Sum:Sum"Sum"¥·	·	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_52/Tensordot/MatMul:MatMul"MatMul"½Τ·Τ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_2_grad/Mul_1:Mul"Mul"”ΗΗƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_1:Mul"Mul"Η¥Α¥³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_65/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"υ‚υxAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/mul_grad/Mul:Mul"Mul"δδ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_14/BiasAdd:BiasAdd"BiasAdd"»«µ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_grad/Mul_1:Mul"Mul"ƒΪ	~Ϊ	tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_15/dropout/Mul:Mul"Mul"•ϋϋ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_1:Mul"Mul"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_28/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp")Ά$Άdiv_no_nan_1:DivNoNan"DivNoNan"•ήή„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_2:Mul"Mul"ς–ς‹Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dropout_21/dropout/Mul_grad/Mul:Mul"Mul"Λ“Ε“²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_45/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"‚—‚†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/transpose_1:Transpose"	Transpose"¦†¦|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/truediv:Mul"Mul"»ΡµΡAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_grad/Mul_1:Mul"Mul"›•‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_46/Tensordot/MatMul:MatMul"MatMul"–££€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/SelectV2:SelectV2"SelectV2"®α¨α•Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dense_26/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΟφΙφ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_38/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"|†w†lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_53:AddN"AddN"Ό
¶
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_grad/Mul_1:Mul"Mul"¦ε ε’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_59/Tensordot/MatMul:MatMul"MatMul"9¥9model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_6/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Θ¨Β¨±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/transpose_grad/transpose:Transpose"	Transpose"ΖΙΐΙµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/SquaredDifference_grad/Mul:Mul"Mul"—ψ‘ψ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/add:AddV2"AddV2"ΟΙ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ζ¬ΐ¬µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/SquaredDifference_grad/sub:Sub"Sub"U½P½8Adam/Adam/update_126/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"–ίίƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/add:AddV2"AddV2"΄ό®ό£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/truediv_grad/Sum:Sum"Sum"΄Ν
®Ν
£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/Softmax_grad/mul:Mul"Mul"·ƒ±ƒ¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/sub_grad/Sum:Sum"Sum"Η§
Α§
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_45/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"|ΣwΣlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_33:AddN"AddN"Ίχ΄χ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_1_grad/Mul:Mul"Mul"»Ί
µΊ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_grad/Sum_1:Sum"Sum"Φ‡Π‡½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_60/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"””‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_39/BiasAdd:BiasAdd"BiasAdd"TΚOΚ7Adam/Adam/update_77/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΞλΘλµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_grad/Mul:Mul"Mul"¥»	»	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_37/Tensordot/MatMul:MatMul"MatMul"ΉN΄Nmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_10/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"’ωωmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul:Mul"Mul"umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dropout_8/dropout/GreaterEqual:GreaterEqual"GreaterEqual"®²¨²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_15/dropout/Mul_1_grad/Mul:Mul"Mul"|πwπlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_40:AddN"AddN"TύOύ7Adam/Adam/update_29/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ί‘΄‘©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/sub_grad/Sum:Sum"Sum"΄€	®€	£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/Softmax_grad/sub:Sub"Sub"“ΠΠmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_45/BiasAdd:BiasAdd"BiasAdd"Ε΅Ώ΅±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_46/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ό¶¶¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_grad/Sum_1:Sum"Sum"UηPη8Adam/Adam/update_160/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΥτΟτΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_38/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UΖPΖ8Adam/Adam/update_173/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam""ρρEagerExecute: LogicalAnd"•••„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_2:Mul"Mul"®„¨„Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_18/dropout/Mul_1_grad/Mul:Mul"Mul"“¥	¥	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_12/BiasAdd:BiasAdd"BiasAdd"²h­hmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"UΏPΏ8Adam/Adam/update_111/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΜυΖυ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"|ΟwΟlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_29:AddN"AddN"®¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_16/dropout/Mul_1_grad/Mul:Mul"Mul"»μ
µμ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_grad/Mul_1:Mul"Mul"¬³¦³›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_15/dropout/Mul_grad/Mul:Mul"Mul"“Ά	Ά	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_11/BiasAdd:BiasAdd"BiasAdd"ΖςΐςµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/SquaredDifference_grad/Mul:Mul"Mul"²Γ¬Γ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_20/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ό¦¶¦«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/sub_grad/Sum_1:Sum"Sum" Ρ	Ρ	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/MatMul:BatchMatMulV2"BatchMatMulV2" φφEagerExecute: Identity"»
¶
΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_37/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"©Θ£ΘAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_9/dropout/Mul_grad/Sum:Sum"Sum"ΝΪΗΪ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"Υ«
Ο«
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_42/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ήι³ι¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_2_grad/Mul:Mul"Mul"½·΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_51/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"¶Ή°Ή¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/Softmax_grad/mul_1:Mul"Mul"{
v
kAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_9:AddN"AddN"ΗήΑή¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/SquaredDifference_grad/mul_1:Mul"Mul"¥ΙΙ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_21/Tensordot/MatMul:MatMul"MatMul"d_Smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/add:AddV2"AddV2"µν―ν¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/Softmax_grad/mul:Mul"Mul"Θ°Β°·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/SquaredDifference_grad/mul_1:Mul"Mul"|½w½lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_56:AddN"AddN"ΐ„ΐymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_65/Relu:Relu"Relu"Υ—Ο—ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_41/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ν—Ν†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/transpose_1:Transpose"	Transpose"Ή4΄4model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_29/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"§‚§xmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/truediv:Mul"Mul"HΔCΔ)AssignAddVariableOp_4:AssignAddVariableOp"AssignAddVariableOp"1χ,χWriteSummary:WriteSummary"WriteSummary"ƒ‹~‹tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_19/dropout/Mul:Mul"Mul"Ί‰Ίmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/mul_1:Mul"Mul"ΚεΔε³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/transpose_2_grad/transpose:Transpose"	Transpose"C”	>”	)Adam/Cast_2/ReadVariableOp:ReadVariableOp"ReadVariableOp"―[[•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"|λwλlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_12:AddN"AddN"SN6Adam/Adam/update_7/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΕΞΏΞ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_35/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΖβΐβµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/SquaredDifference_grad/sub:Sub"Sub"»²µ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_2_grad/Mul_1:Mul"Mul"΄ƒ®ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΥπΟπΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΕΑ
ΏΑ
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_24/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΟζΙζ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_19/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"¶	¶	‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_19/BiasAdd:BiasAdd"BiasAdd"³E®E™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"%τ τIdentity:Identity"Identity"½·΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_53/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ζ—Ζƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_9/dropout/GreaterEqual:GreaterEqual"GreaterEqual"••Adam/Pow:Pow"Pow"ΉΨ³Ψ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_grad/Mul:Mul"Mul"Ά©©‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΝοΗοΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_28/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"TθOθ7Adam/Adam/update_45/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ίω΄ω©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_2_grad/Sum:Sum"Sum"ΚΔ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΆΏ	Ώ	‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΕάΏά΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/SquaredDifference_grad/Mul:Mul"Mul"“ΟΟ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/sub:Sub"Sub"Ρ­Λ­½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_51/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"²ύ¬ύ–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"¥μμ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_16/Tensordot/MatMul:MatMul"MatMul"|³w³lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_55:AddN"AddN"ΣΓΝΓΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_28/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ήg΄gmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_49/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"UP8Adam/Adam/update_154/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"·ϊ±ϊ›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό›¶›«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_1_grad/Mul_1:Mul"Mul"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_grad/Sum:Sum"Sum" ®®†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_13/dropout/GreaterEqual:GreaterEqual"GreaterEqual"TΩOΩ7Adam/Adam/update_50/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"”ΪΪ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/moments/mean:Mean"Mean"™«“«‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/transpose_3:Transpose"	Transpose"UιPι8Adam/Adam/update_139/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ώwώlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_28:AddN"AddN"ΤΞ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_7/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UΆPΆ8Adam/Adam/update_151/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ΰ
wΰ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_44:AddN"AddN"¦ν ν’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_60/Tensordot/MatMul:MatMul"MatMul"΄―model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_1/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ή°³°¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/sub_grad/Sum:Sum"Sum"|¦w¦lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_31:AddN"AddN"TΡOΡ7Adam/Adam/update_93/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"²ΐ¬ΐ΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/truediv_grad/Sum:Sum"Sum"ΚδΔδ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Ο΄Ι΄»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_40/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"…™…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_59/BiasAdd:BiasAdd"BiasAdd"Δ–Δ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_30/BiasAdd:BiasAdd"BiasAdd"|Υ
wΥ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_42:AddN"AddN"T‹O‹7Adam/Adam/update_10/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Κ―
Δ―
―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"ΛΫΕΫ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_23/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"¬¦›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_11/dropout/Mul_1_grad/Mul:Mul"Mul"Ν‚
Η‚
²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"UP8Adam/Adam/update_152/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"™“†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/add_1:AddV2"AddV2"ΣΝΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_32/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Έ#³#model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_8/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"σ–σ‹Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dropout_21/dropout/Mul_grad/Sum:Sum"Sum"“ββ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/sub:Sub"Sub"Μƒ	Ζƒ	±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"¶V±Vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"―Α©Α”model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout_1/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"ζ—ζ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/transpose_3:Transpose"	Transpose"ΚΊΔΊ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/transpose_2_grad/transpose:Transpose"	Transpose"TψOψ7Adam/Adam/update_67/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΈΕ²Ε§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_grad/Mul:Mul"Mul"Ρ™Λ™½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_40/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ή	³	¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_2_grad/Mul:Mul"Mul"†Ψ€Ψvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_14/dropout/Mul_1:Mul"Mul"Ρ©Λ©ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΥΎΟΎΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_44/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ίο΄ο©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/add_grad/Sum:Sum"Sum"ΔόΎό³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/SquaredDifference_grad/sub:Sub"Sub"ΑΞ»Ξ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/variance_grad/truediv:Mul"Mul"$ωωEagerExecute: WriteSummary"ΛΕ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/transpose_2_grad/transpose:Transpose"	Transpose"ΛΕ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/transpose_2_grad/transpose:Transpose"	Transpose"U P 8Adam/Adam/update_149/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ί­	΄­	›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/moments/SquaredDifference:SquaredDifference"SquaredDifference"±p¬p—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_65/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"±‡«‡•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_55/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΚύΔύ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Γ¤½¤²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/SquaredDifference_grad/sub:Sub"Sub"|ΧwΧlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_35:AddN"AddN"ΛηΕη²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_18/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Οτ
Ιτ
»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_16/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ο»Ι»»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_49/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"„		umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_3/dropout/Mul_1:Mul"Mul"°ϊϊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/sub_grad/Sum:Sum"Sum"Λ’Λ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/moments/variance:Mean"Mean"U®P®8Adam/Adam/update_108/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‡Ω	Ω	vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_15/dropout/Cast:Cast"Cast"»ώµώmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_31/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"»¶΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_14/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΣΠΝΠΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_27/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"F΅A΅,div_no_nan_1/ReadVariableOp_1:ReadVariableOp"ReadVariableOp" ΆΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_3/Tensordot/MatMul:MatMul"MatMul"Ρ³
Λ³
½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_37/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ρρAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dropout_21/dropout/Mul_1_grad/Mul:Mul"Mul"ΰ’ΰ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/Rsqrt:Rsqrt"Rsqrt"‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_49/BiasAdd:BiasAdd"BiasAdd"Ζ–
ΐ–
µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/SquaredDifference_grad/sub:Sub"Sub"Ρ„Ρymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_45/Relu:Relu"Relu"•δδ€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/Softmax:Softmax"Softmax"ΙυΓυ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/transpose_grad/transpose:Transpose"	Transpose"ΚΈΔΈ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/transpose_3_grad/transpose:Transpose"	Transpose"T…O…7Adam/Adam/update_84/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"vΘqΘemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/add_2:AddV2"AddV2"ΛΕ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"ΥΥ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_10/BiasAdd:BiasAdd"BiasAdd"Ή<΄<model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_15/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"›―•―model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΊΜ΄Μ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/add_grad/Sum:Sum"Sum"Έ²model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"“““‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul:Mul"Mul"»‚µ‚Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_grad/Sum_1:Sum"Sum"Ήν³ν¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_grad/Mul:Mul"Mul"	—	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_7/BiasAdd:BiasAdd"BiasAdd"U£P£8Adam/Adam/update_137/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΟσΙσ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_54/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"P—K—1Adam/Adam/AssignAddVariableOp:AssignAddVariableOp"AssignAddVariableOp"»ΦµΦAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_2_grad/Sum:Sum"Sum"ΠκΚκΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_64/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΓΊ½Ί²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/SquaredDifference_grad/Mul:Mul"Mul"c‹^‹Smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/Cast:Cast"Cast"΄ψ®ψ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/Softmax_grad/sub:Sub"Sub"ΌΙ
¶Ι
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/mean_grad/truediv:Mul"Mul"Ήξ³ξ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/add_grad/Sum:Sum"Sum"•Ύ	Ύ	€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/Softmax:Softmax"Softmax"ΊMµM model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_7/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"*ό%ό EagerExecute: FlushSummaryWriter"™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_63/BiasAdd:BiasAdd"BiasAdd"”ΞΞmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/add:AddV2"AddV2"΅›†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dropout_8/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"r m cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dropout_8/dropout/Mul:Mul"Mul"†±€±vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_13/dropout/Mul_1:Mul"Mul"®χ¨χ•Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dense_25/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"΄π®π£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/Softmax_grad/sub:Sub"Sub"ΘΑΒΑ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/transpose_grad/transpose:Transpose"	Transpose"”‚”xAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/mul_grad/Mul:Mul"Mul"·‡	±‡	¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/sub_grad/Neg:Neg"Neg"ƒΧ~Χtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_14/dropout/Mul:Mul"Mul"Όµ¶µ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_grad/Mul_1:Mul"Mul"ΊΕ΄Ε©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/sub_grad/Sum:Sum"Sum"“ττmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/moments/mean:Mean"Mean"Γ»½»²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/SquaredDifference_grad/sub:Sub"Sub"ΌΛ¶Λ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_8/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"²o­omodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"U²P²8Adam/Adam/update_124/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"BΒ=Β(div_no_nan/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΚγΔγ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/transpose_3_grad/transpose:Transpose"	Transpose"Ί’΄’©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/sub_grad/Sum:Sum"Sum"—¶‘¶„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/add:AddV2"AddV2"›΅	•΅	‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_11/Tensordot/MatMul:MatMul"MatMul"r³m³amodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/add:AddV2"AddV2"Αρ»ρ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/variance_grad/truediv:Mul"Mul"ΌΖ¶Ζ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_2_grad/Mul:Mul"Mul"·–±–›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"|wlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_11:AddN"AddN"»θ
µθ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_2_grad/Mul:Mul"Mul"”υυ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/moments/mean:Mean"Mean"―		•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΈΜΈΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"Κ®Δ®―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"ΉΩ³Ω¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_grad/Sum:Sum"Sum"AΘ<ΘIterator::Model::ParallelMapV2"Iterator::ParallelMapV2"Ήμ³μ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_grad/Sum:Sum"Sum"΄Ό®Ό£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/truediv_grad/Sum:Sum"Sum"Ό¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/strided_slice_8_grad/StridedSliceGrad:StridedSliceGrad"StridedSliceGrad"ΈΜ²Μ™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/moments/SquaredDifference:SquaredDifference"SquaredDifference"Tέ
Oέ
7Adam/Adam/update_48/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"«“¥“’Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/dense/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"T·
O·
7Adam/Adam/update_80/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΌΗ¶Η«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_2_grad/Sum:Sum"Sum"Ρ
Λ
ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Ό—¶—«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_1_grad/Mul:Mul"Mul"½δ·δ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_2_grad/Mul_1:Mul"Mul"ΤΪΞΪ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"·+²+model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_33/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ήϋ³ϋmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_34/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"–κκmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/Softmax:Softmax"Softmax"Ί‹	΄‹	©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/mean_grad/truediv:Mul"Mul"»^¶^΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_44/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"•ΙΙƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/moments/variance:Mean"Mean"vξqξemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/add_1:AddV2"AddV2"‚’‚†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/moments/variance:Mean"Mean"|ΐwΐjmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_26/Tensordot/MatMul:MatMul"MatMul"TΝOΝ7Adam/Adam/update_88/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"«α¥αAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/dropout_6/dropout/Mul_grad/Mul:Mul"Mul"½	·	΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_47/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ή±³±¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/sub_grad/Sum_1:Sum"Sum"»†µ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_52/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"΄χ®χ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/Softmax_grad/Sum:Sum"Sum"ΊΔ΄Δ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/sub_grad/Neg:Neg"Neg"ΎΦΈΦ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΊΑ΄Α¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_5/Relu_grad/ReluGrad:ReluGrad"ReluGrad"ΘεΒε±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/transpose_grad/transpose:Transpose"	Transpose"Ο”Ι”»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_53/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ώ¨Ή¨©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_55/Relu_grad/ReluGrad:ReluGrad"ReluGrad"¥µ	µ	‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_19/Tensordot/MatMul:MatMul"MatMul"x™xmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_1/Tensordot/MatMul:MatMul"MatMul"¦ ’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_63/Tensordot/MatMul:MatMul"MatMul"―Ε©Ε”model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_9/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ν¬Η¬ΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_34/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΗίΑί³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_24/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"΄ο®ο£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/Softmax_grad/Sum:Sum"Sum"Tό
Oό
7Adam/Adam/update_36/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"—ƒ‘ƒ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/add:AddV2"AddV2"”²	²	ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_1:Mul"Mul"›Β•Β„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/transpose_3:Transpose"	Transpose"®Σ¨ΣAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_20/dropout/Mul_1_grad/Mul:Mul"Mul"T¶O¶7Adam/Adam/update_92/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"·Ζ±Ζ¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_2_grad/Sum:Sum"Sum"―•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_11/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΝΨΗΨ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"©Μ©ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_60/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"¤model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_9/Tensordot/MatMul:MatMul"MatMul"T»
O»
7Adam/Adam/update_64/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¬ω¦ωmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_5/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"θ‚θxmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/sub:Sub"Sub"Ί‰΄‰©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_grad/Mul:Mul"Mul"¬
¦
›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_17/dropout/Mul_grad/Mul:Mul"Mul"vΩqΩemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/add_2:AddV2"AddV2"ƒμ~μtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_16/dropout/Mul:Mul"Mul"½ύ·ύ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/mean_grad/truediv:Mul"Mul"”€€‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/moments/mean:Mean"Mean"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/mean_grad/truediv:Mul"Mul"Φ΅Π΅½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_58/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΖΠΐΠµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/SquaredDifference_grad/sub:Sub"Sub"ΥηΟηΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_19/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ίέ΄έ›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/moments/SquaredDifference:SquaredDifference"SquaredDifference"΄―model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"‘·‹·model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/add:AddV2"AddV2"UΤPΤ8Adam/Adam/update_138/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΜΗΖΗ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Ρ¥
Λ¥
½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_53/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"U›P›8Adam/Adam/update_170/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΚΔ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/transpose_3_grad/transpose:Transpose"	Transpose"TυOυ7Adam/Adam/update_24/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"δ	δ	‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_51/BiasAdd:BiasAdd"BiasAdd"U«P«8Adam/Adam/update_116/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Έϋ²ϋ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/truediv_grad/RealDiv:Mul"Mul"‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_43/BiasAdd:BiasAdd"BiasAdd"ΎΆΈΆ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/variance_grad/truediv:Mul"Mul"½ή·ή¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Ής³ς¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/truediv_grad/RealDiv:Mul"Mul"Ι‚Γ‚°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_36/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"‘“‹“€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/sub:Sub"Sub"’ΐΐ~model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/Softmax:Softmax"Softmax"±&¬&—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_18/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΈΡ
²Ρ
§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/truediv_grad/RealDiv:Mul"Mul"ΡφΛφ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_13/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"|†	w†	lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_54:AddN"AddN"±U¬U—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"µΔ―Δ¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/sub_grad/Neg:Neg"Neg"•„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_2:Mul"Mul"Φ–Φ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_31/BiasAdd:BiasAdd"BiasAdd"Άεε‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/MatMul_1:BatchMatMulV2"BatchMatMulV2"¨‰¨model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/sub:Sub"Sub"•ΉΉ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_1:Mul"Mul"³­—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_66/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"»ψµψAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_2_grad/Mul:Mul"Mul"½Ι·Ι¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/mean_grad/truediv:Mul"Mul"ΎΈΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_57/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Έ	²	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_1/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"“ίί‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul:Mul"Mul"¬½¦½›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_13/dropout/Mul_grad/Sum:Sum"Sum"ΥΝΟΝΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"’όόmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/sub:Sub"Sub"TƒOƒ7Adam/Adam/update_89/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ε§Ώ§±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_56/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Σ–Σ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_32/BiasAdd:BiasAdd"BiasAdd"|wxwfmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/strided_slice_3:StridedSlice"StridedSlice"|ΊwΊlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_22:AddN"AddN"ΊΚ΄Κ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_grad/Sum:Sum"Sum"½λ·λ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_grad/Sum_1:Sum"Sum"Ύ†Ύ|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/truediv:Mul"Mul"‚©	}©	smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_3/dropout/Mul:Mul"Mul"”»»model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/add_1:AddV2"AddV2"¶ω°ω¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/Softmax_grad/mul_1:Mul"Mul"ΗΞΑΞ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_46/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"· ± ¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/add_grad/Sum:Sum"Sum"ΎΈΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_62/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"U²P²8Adam/Adam/update_136/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ηπ
Απ
¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/SquaredDifference_grad/mul_1:Mul"Mul"™ύ“ύ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/add_1:AddV2"AddV2"±¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_11/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"€¤€™Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_11/dropout/Mul_grad/Mul:Mul"Mul"Θ¶Β¶­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"t΅o΅emodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dropout_8/dropout/Mul_1:Mul"Mul"AΠ<Π'Adam/Cast/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΥτΟτΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_15/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"›„•„‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_55/Tensordot/MatMul:MatMul"MatMul"Ύ•Έ•­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_1_grad/Mul_1:Mul"Mul"–ΤΤƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/add_1:AddV2"AddV2"™¥“¥‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/transpose_1:Transpose"	Transpose"m›h›Ymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/concat:ConcatV2"ConcatV2"­Ώ
§Ώ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/dropout_7/dropout/Mul_1_grad/Mul:Mul"Mul"UΟPΟ8Adam/Adam/update_114/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ί’
΄’
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/add_grad/Sum:Sum"Sum"Α»°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/variance_grad/truediv:Mul"Mul"ΕμΏμ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/SquaredDifference_grad/Mul:Mul"Mul"µ3°3›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"'Γ"Γdiv_no_nan:DivNoNan"DivNoNan"ƒς~ςtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_18/dropout/Mul:Mul"Mul"•“	“	zmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_26/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Υ¦Ο¦ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_48/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"•ΡΡ€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/Softmax:Softmax"Softmax"“φφmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_55/BiasAdd:BiasAdd"BiasAdd"94*gradient_tape/mean_squared_error/mul_1:Mul"Mul"{ΘvΘkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_3:AddN"AddN"»ΥµΥAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_2_grad/Mul:Mul"Mul"½ι·ι¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_1_grad/Mul_1:Mul"Mul"		Adam/add:AddV2"AddV2"ΝΉΗΉΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_33/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_13/BiasAdd:BiasAdd"BiasAdd"ΟΙ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_51/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΌΜ¶Μ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_grad/Sum_1:Sum"Sum"Αή»ή°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/variance_grad/truediv:Mul"Mul"·Ι±Ι¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_grad/Mul_1:Mul"Mul"ΝνΗν¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"ΞΘ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"½°·°¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/mean_grad/truediv:Mul"Mul" ½	½	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/MatMul:BatchMatMulV2"BatchMatMulV2"·@²@model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_29/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"——‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_41/BiasAdd:BiasAdd"BiasAdd"•ΝΝƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/moments/variance:Mean"Mean"¶΄°΄¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/truediv_grad/RealDiv:Mul"Mul"’§§model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_2:Mul"Mul"Ήχ³χ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/sub_grad/Neg:Neg"Neg"|Δ
wΔ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_38:AddN"AddN"Ό‹¶‹Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/strided_slice_9_grad/StridedSliceGrad:StridedSliceGrad"StridedSliceGrad"‘Ί	‹Ί	€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/mul:Mul"Mul"rΚmΚamodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/add:AddV2"AddV2"{ΫvΫkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_2:AddN"AddN"®Ύ¨ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/sub_grad/Sum:Sum"Sum"|ΞwΞlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_59:AddN"AddN"¶b±bmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_3/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ρ±Λ±½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_48/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"²—²†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/transpose_2:Transpose"	Transpose"Λƒ
Εƒ
΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/transpose_1_grad/transpose:Transpose"	Transpose"~ΑyΑfmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/strided_slice_8:StridedSlice"StridedSlice"†π€πumodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_4/dropout/Cast:Cast"Cast"U‘P‘8Adam/Adam/update_165/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ζ
wζ
lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_45:AddN"AddN"“ττ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul:Mul"Mul"‡ΕΕvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_20/dropout/Cast:Cast"Cast"›•‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_65/Tensordot/MatMul:MatMul"MatMul"Ν•Η•ΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_30/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ε©Ώ©±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_55/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΕΨΏΨ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_66/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΝοΗο¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_20/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"–ΏΏ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_6/Tensordot/MatMul:MatMul"MatMul"Ίά΄ά©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/add_grad/Sum:Sum"Sum"©Ο£ΟAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/dropout/dropout/Mul_1_grad/Mul:Mul"Mul"Υ­
Ο­
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_39/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"TγOγ7Adam/Adam/update_54/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"½’·’΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_16/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ζύΐύ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_5/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Cμ	>μ	/mean_squared_error/weighted_loss/value:DivNoNan"DivNoNan"U³P³8Adam/Adam/update_103/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"£ΓΓmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_30/Tensordot/MatMul:MatMul"MatMul"¬η¦η›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_19/dropout/Mul_grad/Sum:Sum"Sum"ΣΝΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_34/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Όϊ¶ϊ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_2_grad/Mul:Mul"Mul"Ή³¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_2_grad/Mul:Mul"Mul"•››„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_2:Mul"Mul"€Θ{Θqmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_9/dropout/Mul:Mul"Mul"Ί.µ. model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_9/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΠΩ
ΚΩ
·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΚώΔώ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Ή	³	¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_1_grad/Mul_1:Mul"Mul"²]­]model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΑΜ»Μ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/SquaredDifference_grad/sub:Sub"Sub"ΛΰΕΰ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_17/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"“ήή‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/sub:Sub"Sub"†ν€νvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_16/dropout/Mul_1:Mul"Mul"¥°°‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_15/Tensordot/MatMul:MatMul"MatMul"Ή³¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_2_grad/Mul_1:Mul"Mul"ΎΗΈΗ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_2_grad/Mul_1:Mul"Mul"’¦¦model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_2:Mul"Mul"TΐOΐ7Adam/Adam/update_98/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ΡwΡlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_21:AddN"AddN"{δvδkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_4:AddN"AddN"»—	µ—	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_51/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Έ…²…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΌΥ¶Υ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_1_grad/Mul:Mul"Mul"€Δ{Δqmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout_1/dropout/Mul:Mul"Mul"Ή‹³‹¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_2_grad/Sum:Sum"Sum"Ρ―Λ―ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"¥ΫΫ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_39/Tensordot/MatMul:MatMul"MatMul"›–†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/transpose_1:Transpose"	Transpose"½‚·‚΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_19/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΠΚΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_62/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"£‰£model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/sub:Sub"Sub"ΕσΏσ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/SquaredDifference_grad/Mul:Mul"Mul"ΝβΗβ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"µ†―†¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/add_grad/Sum:Sum"Sum"’±	±	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul:Mul"Mul"TƒOƒ7Adam/Adam/update_30/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam" ΑΑ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΥΦΟΦΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"¨¤¨™Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_10/dropout/Mul_grad/Mul:Mul"Mul" ζζ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/MatMul:BatchMatMulV2"BatchMatMulV2"|ΊwΊlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_19:AddN"AddN"Οψ
Ιψ
»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_15/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΟΕΙΕ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_28/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_38/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ή³¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_1_grad/Mul:Mul"Mul"»ΎµΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_2_grad/Mul_1:Mul"Mul"–©©ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/add_1:AddV2"AddV2"’ΨΨ~model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/Softmax:Softmax"Softmax"ΠόΚόΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_9/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/add_grad/Sum:Sum"Sum"ΥΟΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo" — †model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/transpose_2:Transpose"	Transpose"άά‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/transpose_2:Transpose"	Transpose"™­“­„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_4/BiasAdd:BiasAdd"BiasAdd"γ‚γxmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/sub:Sub"Sub"Ό‡
¶‡
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/sub_grad/Sum_1:Sum"Sum"”¤¤model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/add:AddV2"AddV2"ΑΧ»Χ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/variance_grad/truediv:Mul"Mul"U
P
8Adam/Adam/update_146/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"η–η…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/transpose:Transpose"	Transpose"ΕτΏτ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_11/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"―;;•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_12/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΞΛΘΛ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"“ϊϊ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul:Mul"Mul"%›	 ›	GatherV2:GatherV2"GatherV2"|®w®lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_25:AddN"AddN"‚Ω}Ωsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_2/dropout/Mul:Mul"Mul"†ώ€ώvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_17/dropout/Mul_1:Mul"Mul"Ό’¶’«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_2_grad/Sum:Sum"Sum"ΝΠΗΠ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"U΅
P΅
8Adam/Adam/update_141/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"½έ·έ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΤρΞρ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"—ΰ	‘ΰ	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/add:AddV2"AddV2"ΖΐµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/SquaredDifference_grad/sub:Sub"Sub"¥––‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_41/Tensordot/MatMul:MatMul"MatMul"ƒΗ~Ηsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_9/dropout/Cast:Cast"Cast"»%¶%΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_15/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΞΤΘΤ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"¦’¦…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/Rsqrt:Rsqrt"Rsqrt"ΜΡΖΡΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_4/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"TυOυ7Adam/Adam/update_39/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"™δ“δ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/add_1:AddV2"AddV2"ΖΈΐΈµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/SquaredDifference_grad/sub:Sub"Sub"Ξ…	Θ…	ΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_8/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΉH΄Hmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_16/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"HC)AssignAddVariableOp_2:AssignAddVariableOp"AssignAddVariableOp"£ΥΥmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_31/Tensordot/MatMul:MatMul"MatMul"·Ό±Ό¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/sub_grad/Neg:Neg"Neg"―’―…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/add_1:AddV2"AddV2"TO7Adam/Adam/update_27/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΜΜ
ΖΜ
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Ρ“Λ“ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Έ‚	²‚	§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/truediv_grad/RealDiv:Mul"Mul"’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_2:Mul"Mul"Ε£Ώ£±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_45/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"¬†¦†›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_18/dropout/Mul_grad/Sum:Sum"Sum"°ΦΦAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/Softmax_grad/sub:Sub"Sub"¶j±jmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΘΉΒΉ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/SquaredDifference_grad/mul_1:Mul"Mul"“ΊΊmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_18/BiasAdd:BiasAdd"BiasAdd"½‹
·‹
¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/mean_grad/truediv:Mul"Mul"ΊΙ΄Ι©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_grad/Mul:Mul"Mul"ψ’ψ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/Rsqrt:Rsqrt"Rsqrt"Ό™¶™«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_grad/Mul_1:Mul"Mul"Tξ
Oξ
7Adam/Adam/update_46/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‡όόvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_17/dropout/Cast:Cast"Cast"™Ν“Ν†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/Rsqrt:Rsqrt"Rsqrt"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/sub_grad/Sum_1:Sum"Sum"¤–Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dense_25/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ρ±Λ±½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_54/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ζ ΐ µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/SquaredDifference_grad/Mul:Mul"Mul"TχOχ7Adam/Adam/update_23/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»¶΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_48/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"­‡§‡Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/dropout_2/dropout/Mul_1_grad/Mul:Mul"Mul"—¤‘¤…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/moments/variance:Mean"Mean"²Χ¬Χ΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/Softmax_grad/mul_1:Mul"Mul"Νµ
Ηµ
ΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_27/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ήλ³λ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_grad/Mul:Mul"Mul"½·΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_50/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"|ΕwΕlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_36:AddN"AddN"Uτ	Pτ	8Adam/Adam/update_172/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Έ»²»§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/truediv_grad/RealDiv:Mul"Mul"ΛΛmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/moments/mean:Mean"Mean"µο―ο¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/Softmax_grad/sub:Sub"Sub"΄²®²£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/Softmax_grad/mul_1:Mul"Mul"“ΒΒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_66/BiasAdd:BiasAdd"BiasAdd"’Μ’ΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"TγOγ7Adam/Adam/update_47/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Π‰Πmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/mul:Mul"Mul"Ή6΄6model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_34/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"'΅"΅GatherV2_1:GatherV2"GatherV2"ΊΖ΄Ζ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/add_grad/Sum:Sum"Sum"Έ–	²–	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"U¥P¥8Adam/Adam/update_135/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"°ΤΤAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/Softmax_grad/mul:Mul"Mul"ΝθΗθ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"•υυ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_1:Mul"Mul"Έ„Έymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_17/Relu:Relu"Relu"Α‰»‰°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/SquaredDifference_grad/Mul:Mul"Mul"ΖΟΐΟµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/SquaredDifference_grad/Mul:Mul"Mul"®®„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_11/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΈΘ²Θ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/add_grad/Sum:Sum"Sum"ΟΩΙΩ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_44/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΝμΗμ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"’		model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/sub:Sub"Sub"|wdmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/strided_slice:StridedSlice"StridedSlice"ΥέΟέΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"¤ΜΜmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_8/Tensordot/MatMul:MatMul"MatMul"Ύ“Έ“­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/variance_grad/truediv:Mul"Mul"U·P·8Adam/Adam/update_122/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"±G¬G—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"faUmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/Equal:Equal"Equal"ΊΪ΄Ϊ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_grad/Sum:Sum"Sum"¥««‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_44/Tensordot/MatMul:MatMul"MatMul"TϋOϋ7Adam/Adam/update_21/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"TκOκ7Adam/Adam/update_42/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΠΚ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_3/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ή	—Ή	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/transpose_1:Transpose"	Transpose"Ξ‹Θ‹³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"‚Ι}Ιsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_9/dropout/Mul_1:Mul"Mul"΄ή®ή£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/Softmax_grad/sub:Sub"Sub"²°¬°΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/Softmax_grad/Sum:Sum"Sum"±ξ«ξ–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_4/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"‰‚ƒ‚ymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/sub:Sub"Sub"«ΐ
¥ΐ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/dropout_7/dropout/Mul_grad/Mul:Mul"Mul"½Ν·Ν΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_20/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"¶a±amodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_2/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΛΕ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"½†·†¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/mean_grad/truediv:Mul"Mul"±±‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_15/BiasAdd:BiasAdd"BiasAdd"ΊΔ΄Δ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_grad/Sum:Sum"Sum"Ό”¶”«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_1_grad/Mul:Mul"Mul"ΟεΙε»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_41/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"•¨¨€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/Softmax:Softmax"Softmax"¶µ°µ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/moments/SquaredDifference:SquaredDifference"SquaredDifference"‡λλvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_16/dropout/Cast:Cast"Cast"Ί
΄
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_grad/Mul:Mul"Mul"‘¬‹¬}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_36/BiasAdd:BiasAdd"BiasAdd"¶ί°ί¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/Softmax_grad/mul_1:Mul"Mul"«β
¥β
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/dropout_5/dropout/Mul_grad/Mul:Mul"Mul"²ο¬ο—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_18/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"TόOό7Adam/Adam/update_25/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"“zmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_25/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΗΣΑΣ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_55/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΘΒΒΒ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/transpose_1_grad/transpose:Transpose"	Transpose"“γ	γ	‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/sub:Sub"Sub"Μ¶Ζ¶±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"’ΖΖmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul:Mul"Mul"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_grad/Sum:Sum"Sum"|ΨwΨlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_15:AddN"AddN"Ρ©
Λ©
½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_41/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"TωOω7Adam/Adam/update_18/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"~θ	yθ	fmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/strided_slice_9:StridedSlice"StridedSlice"Π‰Κ‰ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_60/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_48/BiasAdd:BiasAdd"BiasAdd"–ΟΟƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/Rsqrt:Rsqrt"Rsqrt"¬-§-’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_6/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"΄ά®ά£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/Softmax_grad/mul:Mul"Mul"`®[®Qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/mul:Mul"Mul"ΘΩΒΩ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"TύOύ7Adam/Adam/update_22/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"μμ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/transpose_3:Transpose"	Transpose"΅€›€†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/MatMul:BatchMatMulV2"BatchMatMulV2"T	O	7Adam/Adam/update_12/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΉP΄Pmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_40/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"U΅P΅8Adam/Adam/update_153/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ήη³η¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_grad/Sum:Sum"Sum"έ™έmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_61/BiasAdd:BiasAdd"BiasAdd"Ό„¶„«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_2_grad/Sum:Sum"Sum"“··model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_17/BiasAdd:BiasAdd"BiasAdd"–¥¥ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/Rsqrt:Rsqrt"Rsqrt"ΜΗΖΗµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"ΟΤ
ΙΤ
»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_20/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ξ›Θ›·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"Ξ	Θ	µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"“ΪΪ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul:Mul"Mul"Ί«΄«©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/sub_grad/Sum:Sum"Sum"ΜΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_59/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΆΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/moments/mean:Mean"Mean"LήGή/Iterator::Model::ParallelMapV2::Zip[0]::FlatMap"Iterator::FlatMap"±r¬r—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_56/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"“xmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_25/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"T‚O‚7Adam/Adam/update_20/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Υ“
Ο“
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΛΡΕΡ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/transpose_1_grad/transpose:Transpose"	Transpose"U¦P¦8Adam/Adam/update_148/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΐΐzmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_6/BiasAdd:BiasAdd"BiasAdd"Ήα³α¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_grad/Sum:Sum"Sum"Ξφ
Θφ
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"S†N†6Adam/Adam/update_3/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"–ΔΔƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/add:AddV2"AddV2"¬Υ¦Υ›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_20/dropout/Mul_grad/Sum:Sum"Sum"{vkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_6:AddN"AddN"7ε2ε-TensorHandle::GetResourceHandleInfo WaitReady"ΘΉΒΉ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/SquaredDifference_grad/mul_1:Mul"Mul"Ό¨¶¨«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_2_grad/Sum:Sum"Sum"µ―¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/sub_grad/Sum:Sum"Sum"―•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_17/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"TO7Adam/Adam/update_19/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|¬w¬jmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_25/Tensordot/MatMul:MatMul"MatMul"aί\ίAIterator::Model::ParallelMapV2::Zip[1]::ForeverRepeat::FromTensor"Iterator::FromTensor"Η½
Α½
¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/SquaredDifference_grad/mul_1:Mul"Mul"TσOσ7Adam/Adam/update_40/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"”ββƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_1:Mul"Mul"ΝΝΗΝΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_29/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"µό	―ό	¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/Softmax_grad/mul:Mul"Mul"™ƒ“ƒ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/add_1:AddV2"AddV2"Η§Α§³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_56/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Σ‰Σmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/sub:Sub"Sub"{χvχkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_5:AddN"AddN"·Ί±Ί¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/sub_grad/Sum:Sum"Sum"»ΚµΚmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/moments/SquaredDifference:SquaredDifference"SquaredDifference"TO7Adam/Adam/update_75/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΐλΊλ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/variance_grad/truediv:Mul"Mul"¥ΨΨ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_42/Tensordot/MatMul:MatMul"MatMul"Ή>΄>model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_22/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"©©|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/Softmax:Softmax"Softmax"ΓΞ½Ξ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_35/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ƒ ~ tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_10/dropout/Mul_1:Mul"Mul"”		ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_2:Mul"Mul"Θ­Β­·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/SquaredDifference_grad/mul_1:Mul"Mul"SάNά6Adam/Adam/update_5/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"­π§πAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/dropout_3/dropout/Mul_1_grad/Mul:Mul"Mul"ƒύ~ύtmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_17/dropout/Mul:Mul"Mul"Ξ€Θ€ΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_7/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Κ‘Δ‘¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_2/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΎΧΈΧ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Π¤Κ¤ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_57/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"•¶¶‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/add:AddV2"AddV2"ΚΠΔΠ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Η΅Α΅³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_23/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΛτΕτ°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"¦η	 η	’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_57/Tensordot/MatMul:MatMul"MatMul"Δ„Ύ„­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/transpose_grad/transpose:Transpose"	Transpose"™•“•†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/add_1:AddV2"AddV2"–¥¥ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/add:AddV2"AddV2"¥ΤΤ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/dense_10/Tensordot/MatMul:MatMul"MatMul"Ο›Ι›»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_29/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"UΏPΏ8Adam/Adam/update_115/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"µO°O›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"’’‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_42/BiasAdd:BiasAdd"BiasAdd"“ΆΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/moments/mean:Mean"Mean"”³³‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/moments/mean:Mean"Mean"ΓΓ½Γ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/SquaredDifference_grad/sub:Sub"Sub"Ές²ς§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/truediv_grad/RealDiv:Mul"Mul"Q–L–4Adam/Adam/update/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¶	°	¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/Softmax_grad/mul_1:Mul"Mul"»“µ“model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_30/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"’ΡΡmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_1:Mul"Mul"T¦O¦7Adam/Adam/update_38/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΝΈΗΈΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_31/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"—φ‘φ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/moments/variance:Mean"Mean"‡vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_19/dropout/Cast:Cast"Cast"ΠΘ
ΚΘ
·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"ΥΥƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/MatMul:BatchMatMulV2"BatchMatMulV2"»i¶i΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_54/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΘΒ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/transpose_2_grad/transpose:Transpose"	Transpose"ΜΖ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"›Ή•Ή‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_18/Tensordot/MatMul:MatMul"MatMul"ΓΜ½Μ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_36/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΖΐµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/SquaredDifference_grad/sub:Sub"Sub"ΥνΟνΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_43/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ƒ°~°tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_13/dropout/Mul:Mul"Mul"½½zmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_5/BiasAdd:BiasAdd"BiasAdd"™σ“σ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/Rsqrt:Rsqrt"Rsqrt"Θ—
Β—
·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/SquaredDifference_grad/mul_1:Mul"Mul"Ή΄³΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/moments/SquaredDifference:SquaredDifference"SquaredDifference"Κ§Δ§―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Πκ
Κκ
·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"χ’χ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/moments/variance:Mean"Mean"ΊΔ΄Δ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/sub_grad/Sum_1:Sum"Sum"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_19/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΞΫΘΫ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"&Κ!ΚIteratorGetNextOp::DoCompute"λ	λ		Sum_2:Sum"Sum"ΚΎΔΎ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/transpose_1_grad/transpose:Transpose"	Transpose"Ξ—Ξ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/transpose_2:Transpose"	Transpose"΄β®β£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/truediv_grad/Sum:Sum"Sum"„  umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_6/dropout/Mul_1:Mul"Mul"Υ±
Ο±
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_37/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ΉΑ³Α¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_grad/Sum_1:Sum"Sum"Dπ	?π	*div_no_nan/ReadVariableOp_1:ReadVariableOp"ReadVariableOp"U‘P‘8Adam/Adam/update_158/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ήζ³ζ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_grad/Mul:Mul"Mul"™Χ“Χ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/transpose:Transpose"	Transpose"³5®5™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"½¤·¤¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_1_grad/Mul_1:Mul"Mul"¶–¶‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_29/BiasAdd:BiasAdd"BiasAdd"ΞΕΘΕ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"tτoτcmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/add:AddV2"AddV2"²¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_19/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ύ•	Έ•	Άmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_61/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"±X¬X—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_46/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό©¶©«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_1_grad/Mul:Mul"Mul"|οwοlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_50:AddN"AddN"Έ²§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/truediv_grad/RealDiv:Mul"Mul"„ΏΏumodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_5/dropout/Mul_1:Mul"Mul"°­­•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_11/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"TΟOΟ7Adam/Adam/update_86/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"»ά
µά
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_grad/Sum_1:Sum"Sum"ΚγΔγ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Ύ’	Έ’	Άmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_64/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"|³w³lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_30:AddN"AddN"U–P–8Adam/Adam/update_123/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‚wmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_35/Relu:Relu"Relu"zzmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_62/BiasAdd:BiasAdd"BiasAdd"»‰µ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_53/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΒΌ®Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_5/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ό›
¶›
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_1_grad/Mul:Mul"Mul"ΊΩ΄Ω©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_grad/Mul:Mul"Mul"U®P®8Adam/Adam/update_142/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΈΘ²Θ™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/moments/SquaredDifference:SquaredDifference"SquaredDifference"ΗιΑι³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_17/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Tώ
Oώ
7Adam/Adam/update_33/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¥ΜΜ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_53/Tensordot/MatMul:MatMul"MatMul"Όέ¶έ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/sub_grad/Sum_1:Sum"Sum"°ΰΰAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/sub_grad/Sum:Sum"Sum" ννTFE_DeleteTensorHandle"‡}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/truediv:Mul"Mul"·°±°¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/sub_grad/Neg:Neg"Neg"©Η£ΗAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_9/dropout/Mul_grad/Mul:Mul"Mul"»d¶d΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_41/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Έ—²—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"u΄p΄fmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dropout_21/dropout/Mul_1:Mul"Mul"{…
v…
kAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_8:AddN"AddN"½ϋ·ϋ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_1_grad/Mul_1:Mul"Mul"Άκκ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/MatMul_1:BatchMatMulV2"BatchMatMulV2"|ΥwΥlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_39:AddN"AddN" Δ	Δ	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_12/dropout/GreaterEqual:GreaterEqual"GreaterEqual"Ξ‹Θ‹·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"Ζ―ΐ―µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/SquaredDifference_grad/sub:Sub"Sub"•ΫΫ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_1:Mul"Mul"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_grad/Sum_1:Sum"Sum"Ό‚¶‚«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/sub_grad/Sum_1:Sum"Sum"Έ£²£™model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/moments/SquaredDifference:SquaredDifference"SquaredDifference"{vkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_1:AddN"AddN"ββFunctionRun"ΏσΉσ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_11/Relu_grad/ReluGrad:ReluGrad"ReluGrad"»¶΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_42/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"~±y±omodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout/dropout/Mul:Mul"Mul"½Φ·Φ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_2_grad/Mul_1:Mul"Mul"™¨“¨…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_35/Tensordot/MatMul:MatMul"MatMul"“ζ	ζ	‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/sub:Sub"Sub"ΚµΔµ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/transpose_3_grad/transpose:Transpose"	Transpose"»ψµψAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_2_grad/Sum:Sum"Sum"Ε’Ε…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/Rsqrt:Rsqrt"Rsqrt"Ζΐ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_6/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"½™·™΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_13/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΖίΐίµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/moments/SquaredDifference_grad/Mul:Mul"Mul"Ό¬¶¬«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_grad/Mul_1:Mul"Mul"•ΎΎ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_2:Mul"Mul"ΌΨ¶Ψ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/mean_grad/truediv:Mul"Mul"Ο©Ι©»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_13/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΡόΛόΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"Hο	Cο	)AssignAddVariableOp_1:AssignAddVariableOp"AssignAddVariableOp"¥‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_49/Tensordot/MatMul:MatMul"MatMul" ‰ umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dense/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"u­p­bmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_25/BiasAdd:BiasAdd"BiasAdd"“άάmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/moments/mean:Mean"Mean"0γ+γ&InstantiatedCapturedFunction::RunAsync"‡™‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_64/BiasAdd:BiasAdd"BiasAdd"ΉW΄Wmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_43/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"U¤
P¤
8Adam/Adam/update_131/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"›•‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_45/Tensordot/MatMul:MatMul"MatMul"’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/sub:Sub"Sub"¥»»‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_51/Tensordot/MatMul:MatMul"MatMul"ΟΛΙΛ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_30/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΠϊΚϊ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"¥‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_43/Tensordot/MatMul:MatMul"MatMul"Ή‰	³‰	¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_2_grad/Sum:Sum"Sum"Ύ›Έ›Άmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_58/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"¥ΩΩ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_47/Tensordot/MatMul:MatMul"MatMul"Η®Α®³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_12/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"|€w€lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_14:AddN"AddN"ώώ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_20/BiasAdd:BiasAdd"BiasAdd"™’“’†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/Rsqrt:Rsqrt"Rsqrt"t΅o΅cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/add_1:AddV2"AddV2"¶Π
°Π
¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/Softmax_grad/mul_1:Mul"Mul"έ—έ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/transpose_2:Transpose"	Transpose"Όq·qΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_59/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"XΰSΰ5Iterator::Model::ParallelMapV2::Zip[1]::ForeverRepeat"Iterator::ForeverRepeat"U–P–8Adam/Adam/update_119/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΠΚΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_8/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΟΤΙΤ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_27/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul" ¥¥…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/MatMul:BatchMatMulV2"BatchMatMulV2"ΤδΞδ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"TυOυ7Adam/Adam/update_99/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"“ΈΈ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul:Mul"Mul"Η…Α…¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/SquaredDifference_grad/mul_1:Mul"Mul" ϋϋ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_17/dropout/GreaterEqual:GreaterEqual"GreaterEqual"¬
¦
›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_17/dropout/Mul_grad/Sum:Sum"Sum"•ΌΌ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_1:Mul"Mul"T―O―7Adam/Adam/update_28/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"­ς
§ς
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/dropout_4/dropout/Mul_1_grad/Mul:Mul"Mul"–…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/transpose:Transpose"	Transpose"ΟΜΙΜ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_33/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"S‚N‚6Adam/Adam/update_6/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"±¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_17/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ί£΄£›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/moments/SquaredDifference:SquaredDifference"SquaredDifference"£model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_32/Tensordot/MatMul:MatMul"MatMul"Όσ	¶σ	«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_grad/Sum_1:Sum"Sum"†‡†}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/truediv:Mul"Mul"‡ΦΦvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_14/dropout/Cast:Cast"Cast"Όκ¶κ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_2_grad/Sum:Sum"Sum"”¨¨ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_1:Mul"Mul"»Ή
µΉ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_grad/Mul_1:Mul"Mul"ΣΙΝΙΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/dense_30/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Uψ	Pψ	8Adam/Adam/update_162/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|™w™lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_18:AddN"AddN"ΗξΑξ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/SquaredDifference_grad/mul_1:Mul"Mul"uίpίbmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_26/BiasAdd:BiasAdd"BiasAdd"µ=°=›model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"»¬µ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_grad/Sum_1:Sum"Sum"ΎΎƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/MatMul:BatchMatMulV2"BatchMatMulV2"½Ν·Ν§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/sequential_4/dense_35/Relu_grad/ReluGrad:ReluGrad"ReluGrad"•όό„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_2:Mul"Mul"’†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/moments/variance:Mean"Mean"–§§ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/add_1:AddV2"AddV2"›	•	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/transpose:Transpose"	Transpose"¬ ¦ ›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_16/dropout/Mul_grad/Mul:Mul"Mul"|±w±lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_16:AddN"AddN"»χµχAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_2_grad/Mul:Mul"Mul"U”P”8Adam/Adam/update_113/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"T‡O‡7Adam/Adam/update_16/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"–µµ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/moments/variance:Mean"Mean"ΠΧΚΧ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"™¤“¤„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_2/BiasAdd:BiasAdd"BiasAdd"ΡλΛλΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"		Adam/Pow_1:Pow"Pow"²»¬»΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/Softmax_grad/mul:Mul"Mul"ΥΟΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"t²o²cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/add_2:AddV2"AddV2"­B¨B“model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_36/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"T€O€7Adam/Adam/update_15/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‘‘‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/transpose_2:Transpose"	Transpose"S’N’6Adam/Adam/update_4/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Εγ
Ώγ
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_18/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"t΅o΅cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/add:AddV2"AddV2"U¶P¶8Adam/Adam/update_129/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Π‚Πxmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/sub:Sub"Sub"¶ρ°ρ¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/Softmax_grad/mul_1:Mul"Mul"»Ζ
µΖ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_2_grad/Mul:Mul"Mul"²Τ¬Τ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_14/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"Ί¥΄¥©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/sub_grad/Neg:Neg"Neg"Όθ¶θ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/sub_grad/Sum_1:Sum"Sum"%υ υEagerLocalExecute: Identity" ‰‰†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_19/dropout/GreaterEqual:GreaterEqual"GreaterEqual"ΘΈΒΈ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/transpose_2_grad/transpose:Transpose"	Transpose"ΖΟΐΟ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/transpose_grad/transpose:Transpose"	Transpose"Al=l)Adam/Cast_3/ReadVariableOp:ReadVariableOp"ReadVariableOp"±«•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_65/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό–Ό‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_27/BiasAdd:BiasAdd"BiasAdd"Ύ
Έ
­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΊΓ΄Γ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_grad/Mul:Mul"Mul"„ςςumodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_4/dropout/Mul_1:Mul"Mul"ΕέΏέ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/SquaredDifference_grad/sub:Sub"Sub"—Μ	‘Μ	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/add:AddV2"AddV2"U“P“8Adam/Adam/update_171/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ο·Ι·»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_50/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"†Η€Ηvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_20/dropout/Mul_1:Mul"Mul"¶°¥Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/Softmax_grad/mul_1:Mul"Mul"™«“«…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_36/Tensordot/MatMul:MatMul"MatMul"ΕΪΏΪ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_65/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"8κ3κ.EagerExecute: __inference_train_function_70431"ΥϋΟϋΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_10/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"·²model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_27/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ή‡Ή}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/mul:Mul"Mul"ΕήΏή΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/SquaredDifference_grad/Mul:Mul"Mul"°|°rmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_11/dropout/Mul:Mul"Mul"Fξ	Aξ	'AssignAddVariableOp:AssignAddVariableOp"AssignAddVariableOp"Μ½Ζ½±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΞΐΘΐ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΝΝ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_53/BiasAdd:BiasAdd"BiasAdd"SN6Adam/Adam/update_2/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"½α·α¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/moments/mean_grad/truediv:Mul"Mul"’§§model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul:Mul"Mul"¬΄¦΄›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_15/dropout/Mul_grad/Sum:Sum"Sum"›†•†‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_24/Tensordot/MatMul:MatMul"MatMul"T±O±7Adam/Adam/update_31/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam" ΔΔ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_20/dropout/GreaterEqual:GreaterEqual"GreaterEqual"χ	Μχ	ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_64/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"|εwεlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_49:AddN"AddN"―((•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_24/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"£µµmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_29/Tensordot/MatMul:MatMul"MatMul""λλTFE_Py_ExecuteCancelable"»µmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/moments/SquaredDifference:SquaredDifference"SquaredDifference"―__•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_46/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"U¬
P¬
8Adam/Adam/update_107/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ήβ³β¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/sub_grad/Sum:Sum"Sum"v²q²fmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dropout_21/dropout/Cast:Cast"Cast"µƒ―ƒ¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_grad/Mul:Mul"Mul"TΣOΣ7Adam/Adam/update_81/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ΒwΒlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_26:AddN"AddN"¬ζ¦ζ›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_19/dropout/Mul_grad/Mul:Mul"Mul"»‘	µ‘	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_47/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"¥γγ‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_14/Tensordot/MatMul:MatMul"MatMul"Λ¶Ε¶΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"΄®£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/Softmax_grad/mul:Mul"Mul"tΖoΖcmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/add_1:AddV2"AddV2"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_grad/Sum_1:Sum"Sum"‘©‹©}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/sequential_4/dense_35/BiasAdd:BiasAdd"BiasAdd"»Ψ
µΨ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_2_grad/Sum:Sum"Sum"¤yy’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_62/Tensordot/MatMul:MatMul"MatMul"θθEagerKernelExecute"·±¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_grad/Sum:Sum"Sum"T‰O‰7Adam/Adam/update_13/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΤάΞά»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"²­¬­—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_13/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"TμOμ7Adam/Adam/update_41/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΖύΐύµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/SquaredDifference_grad/mul_1:Mul"Mul"»€µ€Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_1_grad/Mul:Mul"Mul"ΞΘΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_9/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"—Γ‘Γ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/moments/variance:Mean"Mean"Ήη
³η
¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/sub_grad/Neg:Neg"Neg"“„„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_23/BiasAdd:BiasAdd"BiasAdd"··±·¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/add_grad/Sum:Sum"Sum"Tϊ
Oϊ
7Adam/Adam/update_35/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ε¥Ώ¥΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/SquaredDifference_grad/mul_1:Mul"Mul"ΗΗParallelMapConsume"ΡΤΛΤ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_22/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ή΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/dense_14/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"«’«…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/add_1:AddV2"AddV2"—Ϊ‘Ϊ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/add_1:AddV2"AddV2"Νϋ	Ηϋ	²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"Α»°Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/variance_grad/truediv:Mul"Mul"’ΆΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_1:Mul"Mul"™ή“ή†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/Rsqrt:Rsqrt"Rsqrt"ΡςΛς½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_39/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ά°°‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dropout_21/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"µ
―
¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/truediv_grad/Sum:Sum"Sum"TΡOΡ7Adam/Adam/update_71/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"U¦P¦8Adam/Adam/update_147/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΐ	—ΐ	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/transpose_3:Transpose"	Transpose"α†α|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/truediv:Mul"Mul"Ήφ³φ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/sub_grad/Neg:Neg"Neg"·²model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_31/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"·΄±΄¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/batchnorm/mul_grad/Mul:Mul"Mul"Έ†²†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"TΥOΥ7Adam/Adam/update_70/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"TΔOΔ7Adam/Adam/update_73/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΎΈ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_1_grad/Mul_1:Mul"Mul"„――tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_11/dropout/Cast:Cast"Cast"•ε	ε	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_2:Mul"Mul"Ύ¨Έ¨­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΞΉΘΉ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"΄®£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/Softmax_grad/Sum:Sum"Sum"»µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΜΖΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_1/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"¦½ ½’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_58/Tensordot/MatMul:MatMul"MatMul"|ίwίlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_43:AddN"AddN" ¬¬model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_4/Tensordot/MatMul:MatMul"MatMul"UσPσ8Adam/Adam/update_100/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"•β	β	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul_2:Mul"Mul"{ƒvƒkAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_7:AddN"AddN"Ή'΄'model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_21/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"ζ™ζmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_57/BiasAdd:BiasAdd"BiasAdd"Ί†Ί|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/truediv:Mul"Mul"»J¶J΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_40/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"™™“™†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/add_1:AddV2"AddV2"~yfmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/strided_slice_4:StridedSlice"StridedSlice"Όλ
¶λ
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/mean_grad/truediv:Mul"Mul"”©©ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_2:Mul"Mul"ΥΎΟΎΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_49/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"†Ψ€Ψumodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_2/dropout/Cast:Cast"Cast"»ωµωAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_2_grad/Sum:Sum"Sum"΄Ξ
®Ξ
£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/Softmax_grad/Sum:Sum"Sum"Λ 
Ε 
²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/sequential_6/dense_56/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"ά’ά†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/moments/variance:Mean"Mean"Ρ­Λ­½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/dense_43/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ΚπΔπ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/transpose_2_grad/transpose:Transpose"	Transpose"³ύ­ύ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"—‘‘‘„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/add:AddV2"AddV2"U€P€8Adam/Adam/update_168/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΉΧ³Χ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_grad/Sum:Sum"Sum"U¤P¤8Adam/Adam/update_150/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"%t!tGatherV2_2:GatherV2"GatherV2"η†η|model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/truediv:Mul"Mul"½„·„΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_52/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ζ•
ΐ•
µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/moments/SquaredDifference_grad/Mul:Mul"Mul"UΛPΛ8Adam/Adam/update_106/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"”ϊϊƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_1:Mul"Mul"Τ	—Τ	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/transpose_3:Transpose"	Transpose"U›P›8Adam/Adam/update_144/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"UΏPΏ8Adam/Adam/update_127/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"•ΟΟ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_1:Mul"Mul"U’P’8Adam/Adam/update_121/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"’³³€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/moments/mean:Mean"Mean"ΏΆΉΆ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_45/Relu_grad/ReluGrad:ReluGrad"ReluGrad""ςςTFE_Py_FastPathExecute_C"ΈΓ²Γ§Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/sub_grad/Neg:Neg"Neg"‹wmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dense/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"»µAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_grad/Mul_1:Mul"Mul"¦‡¦}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/sub:Sub"Sub"“ΈΈ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_2:Mul"Mul"Uφ	Pφ	8Adam/Adam/update_166/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"…™…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_58/BiasAdd:BiasAdd"BiasAdd"ΊΘ΄Θ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_grad/Sum_1:Sum"Sum" ΥΥ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_14/dropout/GreaterEqual:GreaterEqual"GreaterEqual"~yfmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/strided_slice_2:StridedSlice"StridedSlice"UιPι8Adam/Adam/update_163/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"d―_―Smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/add:AddV2"AddV2"ΡΌΛΌ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_49/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"“model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/moments/mean:Mean"Mean"ΎΉΈΉ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/variance_grad/truediv:Mul"Mul"—Μ‘Μ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/add:AddV2"AddV2"±	«	 Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/sub_grad/Sum:Sum"Sum"ΕνΏν΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/SquaredDifference_grad/sub:Sub"Sub"±0¬0—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"™½“½‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/transpose:Transpose"	Transpose"σσconvert_to_tensor"D—?—*div_no_nan_1/ReadVariableOp:ReadVariableOp"ReadVariableOp"U§P§8Adam/Adam/update_125/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"T΄
O΄
7Adam/Adam/update_96/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΤκΞκ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"›Ί•Ί„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/transpose_1:Transpose"	Transpose"Ζ‹ΐ‹―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/transpose_3_grad/transpose:Transpose"	Transpose"ΐΫΊΫ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/moments/variance_grad/truediv:Mul"Mul"ΏΚΉΚ®Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/variance_grad/truediv:Mul"Mul"µξ―ξ¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/Softmax_grad/Sum:Sum"Sum" ™	™	Adam/Cast_1:Cast"Cast"λ—λ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/transpose_3:Transpose"	Transpose"«Ύ¥ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/dropout_1/dropout/Mul_1_grad/Mul:Mul"Mul"ΟΙ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_48/Tensordot/MatMul_grad/MatMul:MatMul"MatMul" ΞΞ…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/MatMul:BatchMatMulV2"BatchMatMulV2"ΡβΛβ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_21/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"ƒ±~±tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_11/dropout/Mul_1:Mul"Mul"Ό ¶  model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_59/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"―AA•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_8/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"'π"πEagerLocalExecute: LogicalAnd"ΟζΙζ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_21/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Μ‘Ζ‘ΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_2/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"™¤“¤‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/transpose_2:Transpose"	Transpose"µ’µ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/moments/variance:Mean"Mean"ΚτΔτ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/transpose_1_grad/transpose:Transpose"	Transpose"ΌΕ¶Ε«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/sub_grad/Sum_1:Sum"Sum"Ό¶«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/variance_grad/truediv:Mul"Mul"΄•®•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_12/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"Λ—Λ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/transpose_2:Transpose"	Transpose"Εο
Ώο
΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/SquaredDifference_grad/sub:Sub"Sub"„umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_7/dropout/Mul_1:Mul"Mul"©Ώ£ΏAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/dropout_1/dropout/Mul_grad/Mul:Mul"Mul"ΏΒ
ΉΒ
©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_23/Relu_grad/ReluGrad:ReluGrad"ReluGrad"Θ­Β­±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/transpose_3_grad/transpose:Transpose"	Transpose"ΎΟΈΟΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_63/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ήά³ά¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/sub_grad/Sum:Sum"Sum"¬c§c’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_5/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"s³n³dmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dropout_21/dropout/Mul:Mul"Mul"²Γ	¬Γ	—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_12/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"U–P–8Adam/Adam/update_161/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"—†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/transpose_3:Transpose"	Transpose"ΟϊΙϊ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"£ΈΈmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_28/Tensordot/MatMul:MatMul"MatMul"Ό‰
¶‰
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_2_grad/Sum:Sum"Sum"ΉΩ³Ω¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/add_grad/Sum:Sum"Sum"Ό®¶®«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_2_grad/Sum:Sum"Sum"tΘ	oΘ	cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/add:AddV2"AddV2"΅ι›ι†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/MatMul:BatchMatMulV2"BatchMatMulV2"΄ώ®ώ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/Softmax_grad/mul:Mul"Mul"±¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_23/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΕΌΏΌ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization_1/moments/SquaredDifference_grad/mul_1:Mul"Mul"—‘€model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/transpose:Transpose"	Transpose"ΐέΊέ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/variance_grad/truediv:Mul"Mul"³Μ­Μ—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"TΰOΰ7Adam/Adam/update_60/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"—·‘·„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/layer_normalization_10/batchnorm/Rsqrt:Rsqrt"Rsqrt"ν	ν	Mul:Mul"Mul"¬°¦°›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_12/dropout/Mul_grad/Mul:Mul"Mul"»µmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/moments/SquaredDifference:SquaredDifference"SquaredDifference"ΊΗ΄Η©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_grad/Mul_1:Mul"Mul"ΞδΘδ³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"b—]—Smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/mul_2:Mul"Mul"…„…ymodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_23/Relu:Relu"Relu"΄΄}model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/moments/mean:Mean"Mean"»ϋµϋAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/mean_grad/truediv:Mul"Mul",κ	'κ	mean_squared_error/Mean:Mean"Mean"Ο‹Ι‹΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/MatMul_1_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"±/¬/—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_12/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ξ™ξmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_60/BiasAdd:BiasAdd"BiasAdd"‚’‚…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/add_1:AddV2"AddV2"Γ£½£²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/SquaredDifference_grad/Mul:Mul"Mul"―RR•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_45/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"TαOα7Adam/Adam/update_43/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"β—β†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/transpose_2:Transpose"	Transpose"«σ
¥σ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/dropout_4/dropout/Mul_grad/Mul:Mul"Mul"±‰±vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dropout_21/dropout/GreaterEqual:GreaterEqual"GreaterEqual"“††‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/sub:Sub"Sub"ΟΑΙΑ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/dense_37/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΉΦ
³Φ
¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/sub_grad/Neg:Neg"Neg"U¨
P¨
8Adam/Adam/update_112/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ήµ³µ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_grad/Sum_1:Sum"Sum"Ίψ΄ψ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_2_grad/Mul:Mul"Mul"·Ε±Ε¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_2_grad/Mul:Mul"Mul"Ό™¶™«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_grad/Mul_1:Mul"Mul"UP8Adam/Adam/update_155/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"|ίwίlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_41:AddN"AddN"Ό¬¶¬«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/sub_grad/Sum_1:Sum"Sum"›|–|†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/transpose_1:Transpose"	Transpose"ο™ο…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_4/dropout/GreaterEqual:GreaterEqual"GreaterEqual" model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_2/Tensordot/MatMul:MatMul"MatMul"®
¨
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/dropout_17/dropout/Mul_1_grad/Mul:Mul"Mul"TΣOΣ7Adam/Adam/update_78/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Β—Βƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout_1/dropout/GreaterEqual:GreaterEqual"GreaterEqual"tά	oά	cmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/add:AddV2"AddV2"Ήγ³γ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/batchnorm/add_grad/Sum:Sum"Sum"™‡“‡†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_18/batchnorm/add_1:AddV2"AddV2"UΧPΧ8Adam/Adam/update_167/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"T¤O¤7Adam/Adam/update_68/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"UP8Adam/Adam/update_102/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"“»»‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/mul:Mul"Mul"½·¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/moments/mean_grad/truediv:Mul"Mul"Cϊ>ϊ%FlushSummaryWriter:FlushSummaryWriter"FlushSummaryWriter"TΛOΛ7Adam/Adam/update_87/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"±¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_24/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ο‰Ι‰»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_10/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"’®®model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/sub:Sub"Sub"”€€ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_2:Mul"Mul"·ώ	±ώ	¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/Softmax_grad/mul_1:Mul"Mul"ΥΟΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"΅Μ΅ΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"TΠOΠ7Adam/Adam/update_63/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"—έ‘έ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/add:AddV2"AddV2"“ΰΰ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/sub:Sub"Sub"±ρ«ρ Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/sub_grad/Sum:Sum"Sum"›·•·„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/transpose_2:Transpose"	Transpose"™„“„†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_16/batchnorm/Rsqrt:Rsqrt"Rsqrt"–––‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_33/BiasAdd:BiasAdd"BiasAdd"¤™Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_11/dropout/Mul_grad/Sum:Sum"Sum"Ί¥΄¥©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/sub_grad/Sum:Sum"Sum"¬Τ¦Τ›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_20/dropout/Mul_grad/Mul:Mul"Mul"†Η	€Η	vmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_12/dropout/Mul_1:Mul"Mul"|υwυlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_51:AddN"AddN"¶8±8model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_4/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό…¶… model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_58/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"/ϋ*ϋ%EagerLocalExecute: FlushSummaryWriter"ΦΠ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_57/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"®―¨―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_12/dropout/Mul_1_grad/Mul:Mul"Mul"ΕΌ
ΏΌ
΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/moments/SquaredDifference_grad/sub:Sub"Sub"ΕςΏς±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_12/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"΄‚®‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"›“•“Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/dropout_8/dropout/Mul_grad/Mul:Mul"Mul"ΝΗΉAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_32/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΠφΚφΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_61/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"™α	“α	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/Rsqrt:Rsqrt"Rsqrt"tσoσcmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/add:AddV2"AddV2"Σ†Ν†ΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_31/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UP8Adam/Adam/update_105/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"±Φ«Φ–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/dropout_2/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"|κwκlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_46:AddN"AddN"ΉΉ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_37/BiasAdd:BiasAdd"BiasAdd"ƒ¨~¨tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/sub:Sub"Sub"»I¶I΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_39/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp")ψ$ψEagerLocalExecute: WriteSummary"Ήξ³ξ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/batchnorm/mul_grad/Sum:Sum"Sum"`™[™Qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/Cos:Cos"Cos"¬—¬†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/transpose_2:Transpose"	Transpose" ½½…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/MatMul:BatchMatMulV2"BatchMatMulV2"ΛόΕό²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_12/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"UΝPΝ8Adam/Adam/update_120/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΚΔ¶Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/dense_1/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"¬§¦§›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_10/dropout/Mul_1_grad/Mul:Mul"Mul"»µmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_50/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"“ΣΣmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_46/BiasAdd:BiasAdd"BiasAdd"Κχ
Δχ
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/transpose_2_grad/transpose:Transpose"	Transpose"”””ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_1:Mul"Mul"T‡O‡7Adam/Adam/update_79/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΥΚ
ΟΚ
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_22/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"²µ¬µ΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/truediv_grad/Sum:Sum"Sum"†Χ€Χvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/sub:Sub"Sub"›¤	•¤	‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/sequential_1/dense_12/Tensordot/MatMul:MatMul"MatMul"Όk·kΆmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_60/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"£»»model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_27/Tensordot/MatMul:MatMul"MatMul"Λ	’Λ	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/moments/variance:Mean"Mean"|wlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_13:AddN"AddN"v²q²emodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/add_1:AddV2"AddV2"ΟΒΙΒ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_47/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Ό€¶€ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_63/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ξ‘
Θ‘
·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"Ρ¶Λ¶½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/dense_47/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"›Π	•Π	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/transpose:Transpose"	Transpose"‡ρρvmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/dropout_18/dropout/Cast:Cast"Cast"|ΖwΖlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_27:AddN"AddN"―•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_23/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ο„Ι„»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/dense_34/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"–‚–xmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/sub:Sub"Sub"¥‰¥model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/mul_2:Mul"Mul"›Τ•Τ„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/transpose_1:Transpose"	Transpose"TθOθ7Adam/Adam/update_52/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"™Ώ“Ώ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_19/batchnorm/add_1:AddV2"AddV2"Ηυ	Αυ	³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/sequential_7/dense_66/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"΄Έ®Έ£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/Softmax_grad/sub:Sub"Sub"7‰2‰(gradient_tape/mean_squared_error/sub:Sub"Sub"ΉΟ³Ο¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/sub_grad/Sum:Sum"Sum"|μwμlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_52:AddN"AddN"upemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dropout_8/dropout/Cast:Cast"Cast"vqemodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/add_1:AddV2"AddV2"–ΌΌ‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/sequential/dense_5/Tensordot/MatMul:MatMul"MatMul"|ΓwΓlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_17:AddN"AddN"Ός	¶ς	«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_grad/Mul_1:Mul"Mul"Κ“Δ“³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/transpose_2_grad/transpose:Transpose"	Transpose"8747IteratorGetNext:IteratorGetNext"IteratorGetNext"»φµφmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/moments/SquaredDifference:SquaredDifference"SquaredDifference"†€umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_7/dropout/Cast:Cast"Cast"T«O«7Adam/Adam/update_85/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"΄Z―Zmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_2/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ήι³ι¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_3/batchnorm/add_grad/Sum:Sum"Sum" κκ†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_16/dropout/GreaterEqual:GreaterEqual"GreaterEqual"Ρϋ
Λϋ
½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_14/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Ή–Ή‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/dense_28/BiasAdd:BiasAdd"BiasAdd"¨Ά”Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/dense_26/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΣΙΝΙΊAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/moments/variance_grad/BroadcastTo:BroadcastTo"BroadcastTo"Εή
Ώή
΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/SquaredDifference_grad/sub:Sub"Sub"Θ°
Β°
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/transpose_grad/transpose:Transpose"	Transpose"΄½®½£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/Softmax_grad/mul_1:Mul"Mul"ΚΐΔΐ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_8/MatMul_grad/MatMul:BatchMatMulV2"BatchMatMulV2"ΓΝ½Ν²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/moments/SquaredDifference_grad/mul_1:Mul"Mul"»αµαAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_6/batchnorm/mul_1_grad/Mul:Mul"Mul"ΜΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_62/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Κ΄Δ΄³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/transpose_1_grad/transpose:Transpose"	Transpose"΄D―Dmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_4/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"–χχƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/add:AddV2"AddV2" ””…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/MatMul:BatchMatMulV2"BatchMatMulV2"Εε
Ώε
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/sequential_2/dense_17/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"ΉΫ³Ϋ¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/add_grad/Sum:Sum"Sum"”ΚΚmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization_1/batchnorm/add:AddV2"AddV2"ΥλΟλΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_16/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"Ί΄©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/sub_grad/Neg:Neg"Neg"Όί¶ί«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/mul_2_grad/Sum:Sum"Sum"U¬P¬8Adam/Adam/update_145/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΟβΙβ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_22/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"›ƒ•ƒ‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_23/Tensordot/MatMul:MatMul"MatMul"»²µ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_9/batchnorm/mul_2_grad/Mul_1:Mul"Mul"ΐΒΊΒ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_5/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"TΣOΣ7Adam/Adam/update_62/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"¶f±fmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"·‘±‘¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_1_grad/Mul:Mul"Mul"›¶•¶‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/sequential_2/dense_17/Tensordot/MatMul:MatMul"MatMul"±«–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_7/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"ƒΖ	~Ζ	tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/dropout_12/dropout/Mul:Mul"Mul"Φ•Π•½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_11/dense_63/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"©¤©™Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/dropout_10/dropout/Mul_grad/Sum:Sum"Sum"ΡΨΛΨ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_19/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"θθ‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_54/BiasAdd:BiasAdd"BiasAdd"Όω¶ω«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/sub_grad/Sum_1:Sum"Sum"Ώ„Ώzmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_4/truediv:Mul"Mul"–ƒAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/concat_1_grad/Slice_1:Slice"Slice"„tmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_10/dropout/Cast:Cast"Cast"»‰µ‰Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_1_grad/Mul_1:Mul"Mul"Ό­¶­«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/batchnorm/mul_2_grad/Mul:Mul"Mul"Ίά΄ά©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_18/batchnorm/sub_grad/Neg:Neg"Neg"•Ο	Ο	„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul_1:Mul"Mul"ΜύΖύ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"™“„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_1/BiasAdd:BiasAdd"BiasAdd"|ΓwΓlAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_57:AddN"AddN"ΡηΛη½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/dense_20/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"΄·®·£Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_6/Softmax_grad/mul:Mul"Mul"Π„
Κ„
ΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_58/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"Όϊ¶ϊ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/mean_grad/truediv:Mul"Mul"rm_model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/dense/BiasAdd:BiasAdd"BiasAdd"±›«›•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_66/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"TΪOΪ7Adam/Adam/update_61/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Όώ¶ώ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_grad/Mul_1:Mul"Mul"Sι	Nι	6mean_squared_error/SquaredDifference:SquaredDifference"SquaredDifference"”ϋϋƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_5/batchnorm/mul_2:Mul"Mul"·±¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_grad/Mul:Mul"Mul"Ή"΄"model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_37/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"­ΰ§ΰAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/dropout_6/dropout/Mul_1_grad/Mul:Mul"Mul"—ή‘ή…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/moments/variance:Mean"Mean"ΡςΛς½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/dense_16/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_6/batchnorm/sub:Sub"Sub"°|°qmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/dropout/dropout/Cast:Cast"Cast"»Q¶Q΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/dense_43/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΐΐΊΐ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/sequential/dense_6/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"U¦
P¦
8Adam/Adam/update_134/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΤφΞφ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_9/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"¥αα‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/dense_48/Tensordot/MatMul:MatMul"MatMul"’΄΄model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/layer_normalization_4/batchnorm/sub:Sub"Sub"Ι…Γ…²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/Rsqrt_grad/RsqrtGrad:RsqrtGrad"	RsqrtGrad"TΔOΔ7Adam/Adam/update_95/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΊΥ΄Υ©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/add_grad/Sum:Sum"Sum"ExecutorDoneCallback"²―¬―΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/Softmax_grad/mul:Mul"Mul"ε—ε†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/multi_head_attention_2/transpose_1:Transpose"	Transpose"»ΫµΫmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/moments/SquaredDifference:SquaredDifference"SquaredDifference"Η
Α
³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/sequential_1/dense_11/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"°•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/dropout_10/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"·±¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/mul_grad/Sum:Sum"Sum"£Μ£ΎAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/dense_58/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"Μ
Ζ
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/multi_head_attention_3/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ή½³½¨Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/sub_grad/Sum_1:Sum"Sum"•ίί„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_1:Mul"Mul"“¬	¬	model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/moments/mean:Mean"Mean"―ss•model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_56/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Όƒ¶ƒ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_61/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"§‚§xmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/sub:Sub"Sub"ΆΣ	Σ	‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_8/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΕζΏζ΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_5/moments/SquaredDifference_grad/Mul:Mul"Mul"™Ί“Ί†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_14/batchnorm/add_1:AddV2"AddV2"ΎΚΈΚ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Ό¶ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_62/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό™
¶™
«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_16/batchnorm/mul_grad/Sum_1:Sum"Sum"ΛΎΕΎ²Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/sequential_5/dense_46/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad" Ψ	Ψ	†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_15/dropout/GreaterEqual:GreaterEqual"GreaterEqual"™ω“ω†model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/layer_normalization_17/batchnorm/Rsqrt:Rsqrt"Rsqrt"½Ξ·Ξ¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_1_grad/Mul_1:Mul"Mul"»¶΅model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/dense_21/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"ΞΘµAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"H C )AssignAddVariableOp_3:AssignAddVariableOp"AssignAddVariableOp"†€umodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_6/dropout/Cast:Cast"Cast"”­­ƒmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_3/batchnorm/mul_2:Mul"Mul"άά‰model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_6/dense_38/BiasAdd:BiasAdd"BiasAdd"ΐπΊπ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/moments/variance_grad/truediv:Mul"Mul"“‡‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/sequential_3/dense_24/BiasAdd:BiasAdd"BiasAdd"µύ	―ύ	¤Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/multi_head_attention_10/Softmax_grad/sub:Sub"Sub"“€€‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/sub:Sub"Sub"Ό‘¶‘«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/batchnorm/mul_2_grad/Mul:Mul"Mul"›¤•¤„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/multi_head_attention_7/transpose:Transpose"	Transpose"TΧOΧ7Adam/Adam/update_55/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"ΡτΛτ½Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/multi_head_attention_1/dense_10/Tensordot/MatMul_grad/MatMul_1:MatMul"MatMul"¥ηη‘model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/multi_head_attention_9/dense_54/Tensordot/MatMul:MatMul"MatMul"£••model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer/multi_head_attention_5/dense_33/Tensordot/MatMul:MatMul"MatMul"Ε„Ώ„΄Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/layer_normalization_2/moments/SquaredDifference_grad/sub:Sub"Sub"ΌΛ¶Λ«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_grad/Mul_1:Mul"Mul"Uϊ	Pϊ	8Adam/Adam/update_169/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"Ί›΄›©Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_grad/Sum:Sum"Sum"±?¬?—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/ReadVariableOp:ReadVariableOp"ReadVariableOp"¦ώ ώ’model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_11/dense_61/Tensordot/MatMul:MatMul"MatMul"’’’…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/batchnorm/Rsqrt:Rsqrt"Rsqrt"®³¨³Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/sub_grad/Sum:Sum"Sum"ΥΆΟΆΌAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_54/BiasAdd_grad/BiasAddGrad:BiasAddGrad"BiasAddGrad"«ρ¥ρAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_1/dropout_3/dropout/Mul_grad/Mul:Mul"Mul"—ς‘ς„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/batchnorm/add:AddV2"AddV2"ΕΓ
ΏΓ
±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/sequential_3/dense_23/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"<7"Adam/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ύ–Έ–­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_1_grad/Mul_1:Mul"Mul"ΘΡΒΡ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/transpose_1_grad/transpose:Transpose"	Transpose"¦–¦model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/MatMul:BatchMatMulV2"BatchMatMulV2"»Έ
µΈ
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_3/layer_normalization_7/batchnorm/mul_1_grad/Mul:Mul"Mul"¬…¦…›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/dropout_18/dropout/Mul_grad/Mul:Mul"Mul"ΡΘΛΘΈAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/moments/mean_grad/BroadcastTo:BroadcastTo"BroadcastTo"°ΥΥAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/multi_head_attention/Softmax_grad/Sum:Sum"Sum"½ι·ι¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_2_grad/Mul_1:Mul"Mul"ΎΈ­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_13/batchnorm/mul_1_grad/Mul_1:Mul"Mul"¬Ό¦Ό›Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/dropout_14/dropout/Mul_grad/Mul:Mul"Mul"±L¬L—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/layer_normalization/batchnorm/mul/ReadVariableOp:ReadVariableOp"ReadVariableOp"›•„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/multi_head_attention_3/transpose:Transpose"	Transpose"±«–model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_6/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"»ν
µν
Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_grad/Sum_1:Sum"Sum"™£“£„model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer/multi_head_attention/dense_3/BiasAdd:BiasAdd"BiasAdd"Ά‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/multi_head_attention_1/MatMul_1:BatchMatMulV2"BatchMatMulV2"ΜσΖσ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/multi_head_attention_2/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"Ύ—Έ—­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_17/batchnorm/mul_2_grad/Mul_1:Mul"Mul"Ό§¶§«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_2_grad/Mul:Mul"Mul"Ύ΄Έ΄­Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/layer_normalization_12/batchnorm/mul_1_grad/Mul_1:Mul"Mul"»κµκAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer_2/layer_normalization_4/batchnorm/mul_1_grad/Mul:Mul"Mul"‚‹}‹smodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/dropout_7/dropout/Mul:Mul"Mul"ΚΊΔΊ―Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_4/MatMul_1_grad/MatMul:BatchMatMulV2"BatchMatMulV2"·Κ±Κ¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/encoder/encoder_layer/layer_normalization/batchnorm/mul_grad/Sum_1:Sum"Sum"TΫOΫ7Adam/Adam/update_53/ResourceApplyAdam:ResourceApplyAdam"ResourceApplyAdam"‘nnxmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/dense_26/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"Ό™¶™ model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/multi_head_attention_10/dense_57/BiasAdd/ReadVariableOp:ReadVariableOp"ReadVariableOp"‚Ύ}Ύsmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_2/dropout_5/dropout/Mul:Mul"Mul"—‘…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_3/layer_normalization_7/moments/variance:Mean"Mean"“Ξ	Ξ	‚model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/layer_normalization_11/batchnorm/mul:Mul"Mul"ΘΒ·Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/layer_normalization_15/moments/SquaredDifference_grad/mul_1:Mul"Mul"²±¬±΅Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/multi_head_attention_5/Softmax_grad/sub:Sub"Sub" 	’ 	…model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/encoder/encoder_layer_1/layer_normalization_2/batchnorm/add_1:AddV2"AddV2"»πµπmodel/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/layer_normalization_15/moments/SquaredDifference:SquaredDifference"SquaredDifference"›Α•Α‡model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_3/sequential_7/dense_66/Tensordot/MatMul:MatMul"MatMul"³‡­‡—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/sequential_6/dense_55/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp"½”·”¬Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_3/layer_normalization_19/moments/mean_grad/truediv:Mul"Mul"ΜζΖζ±Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_1/multi_head_attention_7/MatMul_grad/MatMul_1:BatchMatMulV2"BatchMatMulV2"·‘±‘¦Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_8/batchnorm/add_grad/Sum:Sum"Sum"ΟµΙµ»Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer_2/multi_head_attention_9/dense_52/Tensordot/MatMul_grad/MatMul:MatMul"MatMul"|¤w¤lAdam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/AddN_20:AddN"AddN"²Χ	¬Χ	—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_2/dropout_15/dropout/random_uniform/RandomUniform:RandomUniform"RandomUniform"ΌΕ¶Ε«Adam/gradients/PartitionedCall/gradients/StatefulPartitionedCall_grad/PartitionedCall/gradients/decoder/decoder_layer/layer_normalization_10/batchnorm/mul_2_grad/Mul_1:Mul"Mul"±¬—model/transformer/StatefulPartitionedCall/StatefulPartitionedCall/decoder/decoder_layer_1/sequential_5/dense_45/Tensordot/ReadVariableOp:ReadVariableOp"ReadVariableOp*	step_name*group_id*selected_group_ids*is_eager*
element_id*
_c*autotune*	parent_id*
id*_ct*iter_num*tf_function_call*
_p*tracing_count*notTraced-nonXla*		deterministic*true*_pt*

parallelism"DESKTOP-90FSP9O